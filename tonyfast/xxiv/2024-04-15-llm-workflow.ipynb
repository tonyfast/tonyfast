{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e3723d-ff31-426a-b07c-4e737fb58a79",
   "metadata": {},
   "source": [
    "## integrating llms into my notebook worflows.\n",
    "\n",
    "im finally giving in and integrating llms into my notebooks. i'll eventually need to experiment with their accessibility. i do believe that llms are assistive technology is a familiar way to interactive computing, one compute rich and the other compute poor. i'll learn to understand this technology as an accessibility tool. it feels like the sharper image selling accessibility to abled people at high markups.\n",
    "\n",
    "to use on the daily i want this feature to be lazy. \n",
    "i'll start using huggingfae and if i dig this life then ill edit this document accordingly.\n",
    "i want it as a magic and a code fence for `midgy`. i want caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9e5c96a-f6d6-40fa-9350-ddc35b8cbf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    MAIN = __name__ == \"__main__\"\n",
    "    FILE = \"__file__\" in locals()\n",
    "    I = MAIN and not FILE\n",
    "    import functools, asyncio, argparse, shlex, contextlib, io, IPython, textwrap\n",
    "    from pathlib import Path\n",
    "    shell = IPython.get_ipython()\n",
    "    if \"initialize\" not in locals():\n",
    "        @functools.lru_cache\n",
    "        def initialize():\n",
    "            global langchain, langchain_community\n",
    "            import langchain.cache, langchain.globals, langchain.chains.conversation.base, langchain.chains.conversation.memory, langchain.prompts.prompt\n",
    "            import langchain_community.chat_models.huggingface\n",
    "            langchain.globals.set_llm_cache(langchain.cache.SQLiteCache())\n",
    "            __import__(\"dotenv\").load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31293b-929c-4a6f-a695-178b24419c14",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/model_io/chat/chat_model_caching/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8b8c29-f09b-439d-9b62-7d9999cc5149",
   "metadata": {},
   "source": [
    "create our `llm` using the default `parser` values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a872b-6a46-4e0e-8db3-e19a0e0a9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @functools.lru_cache\n",
    "    def get_parser():\n",
    "        parser = argparse.ArgumentParser(prog=\"ðŸ—©\")\n",
    "        parser.add_argument(\"repo_id\",  type=str, nargs=\"?\", default=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "        parser.add_argument(\"-t\", \"--max_new_tokens\",  type=int, default=512*2)\n",
    "        parser.add_argument(\"-k\", \"--top_k\",  type=int, default=30)\n",
    "        parser.add_argument(\"-r\", \"--repetition_penalty\", type=float, default=1.03)\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4298d-58dd-4a58-bda0-f161561a9554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>create our <code>llm</code> using the default <code>parser</code> values</p>\n",
       "<pre><code>@functools.lru_cache\n",
       "def get_parser():\n",
       "    parser = argparse.ArgumentParser(prog=&quot;ðŸ—©&quot;)\n",
       "    parser.add_argument(&quot;repo_id&quot;,  type=str, nargs=&quot;?&quot;, default=&quot;mistralai/Mistral-7B-Instruct-v0.2&quot;)\n",
       "    parser.add_argument(&quot;-t&quot;, &quot;--max_new_tokens&quot;,  type=int, default=512*2)\n",
       "    parser.add_argument(&quot;-k&quot;, &quot;--top_k&quot;,  type=int, default=30)\n",
       "    parser.add_argument(&quot;-r&quot;, &quot;--repetition_penalty&quot;, type=float, default=1.03)\n",
       "    return parser\n",
       "</code></pre>\n",
       "<p>ensure that our creditials were accepted.</p>\n",
       "<pre><code>@functools.lru_cache\n",
       "def get_llm(**kwargs):\n",
       "    with contextlib.redirect_stdout(io.StringIO()):\n",
       "        return langchain_community.chat_models.huggingface.HuggingFaceEndpoint(**kwargs)</code></pre>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    @functools.lru_cache\n",
    "    def get_llm(**kwargs):\n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            return langchain_community.chat_models.huggingface.HuggingFaceEndpoint(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c9923-2f29-4a95-a20d-05abbc6e933e",
   "metadata": {},
   "source": [
    "%%\n",
    "> activate this to reset the prompt template\n",
    "\n",
    "i figure it makes sense to curate my own prompt template over time. i'll start with this template and modify the files on disk\n",
    "\n",
    "https://python.langchain.com/docs/modules/memory/conversational_customization/\n",
    "\n",
    "    >>> assert Path(\"prompt_template.md\").read_text().strip().endswith(\"them:\")\n",
    "\n",
    "    if I:\n",
    "```ipython\n",
    "%%file prompt_template.md\n",
    "The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "The AI always responds in well structured markdown with headings beginning at level 3.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "me: {input}\n",
    "them:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1efbe-bba9-4dca-839c-a28b39b7af5c",
   "metadata": {},
   "source": [
    "establish a conversation chain. we'll want to name these things and curate them over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6113e833-bba8-4db3-b9fd-b7feee0f4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @functools.lru_cache\n",
    "    def get_conversation(**kwargs):\n",
    "        return langchain.chains.conversation.base.ConversationChain(\n",
    "            prompt=langchain.prompts.prompt.PromptTemplate(input_variables=[\"history\", \"input\"], template=Path(\"prompt_template.md\").read_text()),\n",
    "            llm=get_llm(**kwargs),\n",
    "            memory=langchain.chains.conversation.memory.ConversationBufferMemory(ai_prefix=\"AI\", human_prefix=\"ME\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ebd7042-9bde-48c9-8906-5f1a5f0cb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "    async def invoke(input, **kwargs):\n",
    "        global result\n",
    "        try:\n",
    "            id = display(IPython.display.HTML(shell.tangle.parser.parser.render(F\"pending\\n: > {input}\")), display_id=True)\n",
    "            return id\n",
    "        finally:\n",
    "            result = await get_conversation(**kwargs).ainvoke(input)\n",
    "            left, sep, right = result[\"response\"].rpartition(\"AI:\")\n",
    "            id.update(IPython.display.HTML(shell.tangle.parser.parser.render(textwrap.dedent(right))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f21faed-aa6e-4359-9e45-a3f3ab36680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def chat(line, cell=None):\n",
    "        initialize()\n",
    "        if cell:\n",
    "            asyncio.create_task(invoke(cell, **vars(get_parser().parse_args(shlex.split(line)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bbdcb4-1f35-453f-8728-dcd18fd2eade",
   "metadata": {},
   "source": [
    "register the magic functions for `chat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c055f5fb-0ef5-4b24-8073-4b83a71db21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load_ipython_extension(shell):\n",
    "        shell.register_magic_function(chat, magic_kind=\"line_cell\")\n",
    "    def unload_ipython_extension(shell):\n",
    "        pass\n",
    "    I and load_ipython_extension(shell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d471c-49a1-4916-9dcc-762313e518a6",
   "metadata": {},
   "source": [
    "%%\n",
    "    if I:\n",
    "```ipython\n",
    "%%chat\n",
    "what are the standard ways of representing genomic data?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ec565-a6ff-4a8d-9105-027f6673cb50",
   "metadata": {},
   "source": [
    "## conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5b654-4f89-4618-a682-bbb5ba0acc0a",
   "metadata": {},
   "source": [
    "an overall theme is curating your llm over time. the feeling of control and consent is important in the interactive computing experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd114e-0367-41ed-a8bf-8fc6a44eda16",
   "metadata": {},
   "source": [
    "## some queries i made making this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad72c4-ea66-4914-af94-363b4c165fae",
   "metadata": {},
   "source": [
    "    if I:\n",
    "```ipython\n",
    "%%chat\n",
    "how do i understand css specificity?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac15b8-7d24-483e-ad77-b68c26ebd80f",
   "metadata": {},
   "source": [
    "I and await invoke(\"what is a tagged pdf and how does it relate to an html document? can tagged pdf be made from well formed html?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff669ca7-8a86-4bee-a23c-8dfc62e17688",
   "metadata": {},
   "source": [
    "I and  invoke(\"do web content accessibiity guidelines apply to pdf? is there a different standard? links are helpful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47488fc-5e63-4bf9-acb2-9b8b200ec7f2",
   "metadata": {},
   "source": [
    "I and  invoke(\"what are the major css apis or concepts like block contexts and 3d rendering?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e17d44-6701-40ea-ae2d-4ad25f5a44f0",
   "metadata": {},
   "source": [
    "I and  invoke(\"could you tell me some more core features of css so that i can better understand ALL of the capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc3c0b-dca1-48ab-9798-b0639149f2d3",
   "metadata": {},
   "source": [
    "I and  invoke(\"what are some of the most advanced css concepts a seasoned developer will encounter?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p311] *",
   "language": "python",
   "name": "conda-env-p311-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

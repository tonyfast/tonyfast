{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"\u2139\ufe0f about tonyfast \u00a4 tonyfast is a freelance developer, designer, and scientist with significant experience in open source and scientific software. he is a distinguished project jupyter contributor advocating for computational literacy, equity, and digital accessibility in scientific technologies. \ud83d\udcd3 notebooks \ud83d\udc80 deathbeds \ud83d\udcbb github \ud83d\udcbc linkedin \ud83d\udee0\ufe0f current projects \u00a4 notebooks for all collaboration with Space Telescope Science Institute improving the accessibility of static notebooks through collaboration with designers and compensated disabled testers. project jupyter accessibility working group , jupyter triage , and jupyter community calls writing experiments with literate programs and computational essays on my blog . pidgy , midgy , and importnb \u2795 more \ud83d\udcbf events and media \u00a4 writers workshop quirkshops open source directions jupyter accessibility workshops part 1 alt text events scipy alt text scavenger hunt atlanta jupyter user group pydata atlanta deathbeds blog jupyter day triangle jupyter days atlanta 2016 \ud83d\udcfd\ufe0f presentations \u00a4 ten pounds of \ud83d\udca9 reincarnation of the notebook powers often ten things 'bout jupyter notebookism the materials data scientist \ud83d\udc68\u200d\ud83c\udfed organizations \u00a4 Quansight, LLC PyData Atlanta Bastille Networks Anaconda Inc Georgia Tech University of California Santa Barbara \u2753 about this repository the tonyfast distribution \u00a4 this repository is one of github's special repositories for my personal profile. i wanted to do more with than just a readme so i'm using it as a place to package my computational essays or literate programs as a python distribution. currently this project features: blogs, essays, notebooks and markdown re-used as python source code a python project called tonyfast that uses hatch for most development tasks (see pyproject.toml ) pip install -e. # for development mode github actions to deploy my content on github pages. the documentation is made of: mkdocs documentation with my own notebook customizations. (see mkdocs.yml ) a no-install, in-the-browser jupyterlite demo so myself and others can try out the code themselves some things i'd like to do: add cron for some posts add tests for some posts build a solid binder to run heavier demos that might not work in jupyterlite","title":"Home"},{"location":"index.html#i-about-tonyfast","text":"tonyfast is a freelance developer, designer, and scientist with significant experience in open source and scientific software. he is a distinguished project jupyter contributor advocating for computational literacy, equity, and digital accessibility in scientific technologies. \ud83d\udcd3 notebooks \ud83d\udc80 deathbeds \ud83d\udcbb github \ud83d\udcbc linkedin","title":"\u2139\ufe0f about tonyfast"},{"location":"index.html#current-projects","text":"notebooks for all collaboration with Space Telescope Science Institute improving the accessibility of static notebooks through collaboration with designers and compensated disabled testers. project jupyter accessibility working group , jupyter triage , and jupyter community calls writing experiments with literate programs and computational essays on my blog . pidgy , midgy , and importnb \u2795 more","title":"\ud83d\udee0\ufe0f current projects"},{"location":"index.html#events-and-media","text":"writers workshop quirkshops open source directions jupyter accessibility workshops part 1 alt text events scipy alt text scavenger hunt atlanta jupyter user group pydata atlanta deathbeds blog jupyter day triangle jupyter days atlanta 2016","title":"\ud83d\udcbf events and media"},{"location":"index.html#presentations","text":"ten pounds of \ud83d\udca9 reincarnation of the notebook powers often ten things 'bout jupyter notebookism the materials data scientist","title":"\ud83d\udcfd\ufe0f presentations"},{"location":"index.html#organizations","text":"Quansight, LLC PyData Atlanta Bastille Networks Anaconda Inc Georgia Tech University of California Santa Barbara \u2753 about this repository","title":"\ud83d\udc68\u200d\ud83c\udfed organizations"},{"location":"index.html#the-tonyfast-distribution","text":"this repository is one of github's special repositories for my personal profile. i wanted to do more with than just a readme so i'm using it as a place to package my computational essays or literate programs as a python distribution. currently this project features: blogs, essays, notebooks and markdown re-used as python source code a python project called tonyfast that uses hatch for most development tasks (see pyproject.toml ) pip install -e. # for development mode github actions to deploy my content on github pages. the documentation is made of: mkdocs documentation with my own notebook customizations. (see mkdocs.yml ) a no-install, in-the-browser jupyterlite demo so myself and others can try out the code themselves some things i'd like to do: add cron for some posts add tests for some posts build a solid binder to run heavier demos that might not work in jupyterlite","title":"the tonyfast distribution"},{"location":"2023-02-05-pipings.html","text":"pipes are common sugar \u00a4 the | is a common operator in programming languages. in pidgy, we find the | operator in jijnja2 filters. here we adds pipes to markdown strings used in pidgy . the pipe classes \u00a4 make it possible chain markdown and python. class pipe ( functools . partial ): def __or__ ( self , object ): if isinstance ( object , str ): return super () . __call__ ( object ) if callable ( object ): return pipe ( lambda x : object ( super () . __call__ ( x ))) raise NotImplementedError ( F \"not implemented for { type ( object ) } \" ) def __ror__ ( self , object ): if isinstance ( object , str ): return super () . __call__ ( object ) if callable ( object ): return pipe ( lambda x : super () . __call__ ( object ( x ))) class do ( pipe ): def __call__ ( self , * args , ** kwargs ): super () . __call__ ( * args , ** kwargs ) object , * _ = args + ( None ,) return object pipe can used as a decorator @pipe def strip_fence ( body ): lines = body . splitlines ( 1 )[ 1 :] if lines [ - 1 ] . startswith (( \"```\" , \"~~~\" )): lines . pop () return \"\" . join ( lines ) maybe we want a pipeable version of print print = do ( print ) print | \"asdf\" x = ( ``` test crap ``` ) | strip_fence | do ( print ) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"pipes are common sugar"},{"location":"2023-02-05-pipings.html#pipes-are-common-sugar","text":"the | is a common operator in programming languages. in pidgy, we find the | operator in jijnja2 filters. here we adds pipes to markdown strings used in pidgy .","title":"pipes are common sugar"},{"location":"2023-02-05-pipings.html#the-pipe-classes","text":"make it possible chain markdown and python. class pipe ( functools . partial ): def __or__ ( self , object ): if isinstance ( object , str ): return super () . __call__ ( object ) if callable ( object ): return pipe ( lambda x : object ( super () . __call__ ( x ))) raise NotImplementedError ( F \"not implemented for { type ( object ) } \" ) def __ror__ ( self , object ): if isinstance ( object , str ): return super () . __call__ ( object ) if callable ( object ): return pipe ( lambda x : super () . __call__ ( object ( x ))) class do ( pipe ): def __call__ ( self , * args , ** kwargs ): super () . __call__ ( * args , ** kwargs ) object , * _ = args + ( None ,) return object pipe can used as a decorator @pipe def strip_fence ( body ): lines = body . splitlines ( 1 )[ 1 :] if lines [ - 1 ] . startswith (( \"```\" , \"~~~\" )): lines . pop () return \"\" . join ( lines ) maybe we want a pipeable version of print print = do ( print ) print | \"asdf\" x = ( ``` test crap ``` ) | strip_fence | do ( print ) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"the pipe classes"},{"location":"regexs.html","text":"regexes \u00a4 import re find blocks in jinja templates \u00a4 used in xxii/2022-12-31-markdownish-notebook.ipynb jinja_block = re . compile ( \" \\\\ s*\" . join ( \"(?P<block> \\\\ { \\\\ %-? block (?P<name> \\\\ S+)( \\\\ s+scoped)? -? \\\\ % \\\\ }(?P<inner>[ \\\\ S| \\r ? \\n ]*) \\\\ { \\\\ %-? endblock (?P=name) -? \\\\ % \\\\ })\" . split ()), re . MULTILINE )","title":"regexes"},{"location":"regexs.html#regexes","text":"import re","title":"regexes"},{"location":"regexs.html#find-blocks-in-jinja-templates","text":"used in xxii/2022-12-31-markdownish-notebook.ipynb jinja_block = re . compile ( \" \\\\ s*\" . join ( \"(?P<block> \\\\ { \\\\ %-? block (?P<name> \\\\ S+)( \\\\ s+scoped)? -? \\\\ % \\\\ }(?P<inner>[ \\\\ S| \\r ? \\n ]*) \\\\ { \\\\ %-? endblock (?P=name) -? \\\\ % \\\\ })\" . split ()), re . MULTILINE )","title":"find blocks in jinja templates"},{"location":"xxii/2022-11-12-async-import.html","text":"asynchronous imports in python \u00a4 lets say we have a python module with a top level await statement. this is valid in javascript and ipython, but not python due to the ambiguity of the top level await. we'll start digging into what it takes to have async imports in python https://gist.github.com/Rich-Harris/0b6f317657f5167663b493c722647221 dream is a our async function and catcher is a value we have access to async def dream (): await __import__ ( \"asyncio\" ) . sleep ( 1 ) return \"dream on\" print ( catcher := await dream ()) dream on by default, importing modules with top level await is a fail because await can't be outside a function. try : with __import__ ( \"importnb\" ) . Notebook (): import __ except SyntaxError as error : assert \"'await' outside function\" == error . args [ 0 ], error # boiler plate import importnb , ast ; from IPython import get_ipython shell = get_ipython (); we're about to do some nasty nested async business when we are working interactively. below we ask IPython to prefer to the trio thread when we execute code cells this way we can own the asyncio event loop. if \"__file__\" not in locals (): __import__ ( 'nest_asyncio' ) . apply () shell . loop_runner = __import__ ( \"IPython\" ) . core . async_helpers . _trio_runner luckily there is a flag for top level awaits thanks to matthias and his hard work on IPython . https://docs.python.org/3/library/ast.html#ast.PyCF_ALLOW_TOP_LEVEL_AWAIT we'll make an importer that includes the PyCF_ALLOW_TOP_LEVEL_AWAIT flag and executes the module through an asynchronous version of eval class ANotebook ( importnb . Notebook ): def exec_module ( self , module ): code = self . get_code ( self . name ) return __import__ ( \"asyncio\" ) . run ( eval ( code , vars ( module ))) def source_to_code ( self , nodes , path , * , _optimize =- 1 ): if not isinstance ( nodes , ast . Module ): nodes = self . parse ( nodes ) return compile ( self . visit ( nodes ), path , \"exec\" , optimize = _optimize , flags = ast . PyCF_ALLOW_TOP_LEVEL_AWAIT ) with ANotebook (): import __11_12_async_import as anb assert anb . catcher == \"dream on\" dream on we haven't done anything fancy with asyncing the reading the and decoding of the source. we'll get there though. this is just opening the can of worms. the jokes should follow.","title":"asynchronous imports in python"},{"location":"xxii/2022-11-12-async-import.html#asynchronous-imports-in-python","text":"lets say we have a python module with a top level await statement. this is valid in javascript and ipython, but not python due to the ambiguity of the top level await. we'll start digging into what it takes to have async imports in python https://gist.github.com/Rich-Harris/0b6f317657f5167663b493c722647221 dream is a our async function and catcher is a value we have access to async def dream (): await __import__ ( \"asyncio\" ) . sleep ( 1 ) return \"dream on\" print ( catcher := await dream ()) dream on by default, importing modules with top level await is a fail because await can't be outside a function. try : with __import__ ( \"importnb\" ) . Notebook (): import __ except SyntaxError as error : assert \"'await' outside function\" == error . args [ 0 ], error # boiler plate import importnb , ast ; from IPython import get_ipython shell = get_ipython (); we're about to do some nasty nested async business when we are working interactively. below we ask IPython to prefer to the trio thread when we execute code cells this way we can own the asyncio event loop. if \"__file__\" not in locals (): __import__ ( 'nest_asyncio' ) . apply () shell . loop_runner = __import__ ( \"IPython\" ) . core . async_helpers . _trio_runner luckily there is a flag for top level awaits thanks to matthias and his hard work on IPython . https://docs.python.org/3/library/ast.html#ast.PyCF_ALLOW_TOP_LEVEL_AWAIT we'll make an importer that includes the PyCF_ALLOW_TOP_LEVEL_AWAIT flag and executes the module through an asynchronous version of eval class ANotebook ( importnb . Notebook ): def exec_module ( self , module ): code = self . get_code ( self . name ) return __import__ ( \"asyncio\" ) . run ( eval ( code , vars ( module ))) def source_to_code ( self , nodes , path , * , _optimize =- 1 ): if not isinstance ( nodes , ast . Module ): nodes = self . parse ( nodes ) return compile ( self . visit ( nodes ), path , \"exec\" , optimize = _optimize , flags = ast . PyCF_ALLOW_TOP_LEVEL_AWAIT ) with ANotebook (): import __11_12_async_import as anb assert anb . catcher == \"dream on\" dream on we haven't done anything fancy with asyncing the reading the and decoding of the source. we'll get there though. this is just opening the can of worms. the jokes should follow.","title":"asynchronous imports in python"},{"location":"xxii/2022-11-12-pluggy-experiments.html","text":"fumbling around with pluggy \u00a4 https://pluggy.readthedocs.io/ register/unregister plugins define specs and implementations at the same time. import pluggy we're going to think about a sample PROG ram that has an interface. PROG = \"sample-program\" the specification decorates the functions and signatures of our interface. the implementation uses the specification for consistency when it actually defines computational work. implementation , specification = pluggy . HookimplMarker ( PROG ), pluggy . HookspecMarker ( PROG ) PROG s specification for my_plugin provides a default interface. @implementation @specification ( firstresult = False ) def my_plugin ( a , b , c ): return \"\" . join ( map ( str , ( a , b , c ))) we register the specifaction onto a plugin manager manager = pluggy . PluginManager ( PROG ) our specification lives in the import __main__ or MAIN namespace. manager . add_hookspecs ( __main__ := __import__ ( __name__ )) assert \"my_plugin\" in dir ( manager . hook ), \"the plugin is registered\" now we can add our implementation of my_plugin in the __main__ module manager . register ( __main__ ); now we can execute our manager.hook.my_plugin method manager . hook . my_plugin ( a = 10 , b = 20 , c = 30 ) let's add another implementation. we can't name it my_plugin otherwise we'll lose the scope of our previous method. here we name our method whatever we want and explicitly define the implementation:specname it refers to. @implementation ( specname = \"my_plugin\" ) def another_plugin ( a , b ): return ( a , b ) now seems like a good time test what async functions do @implementation ( specname = \"my_plugin\" , trylast = True ) async def async_my_plugin ( a , b ): return ( a , b ) reregistering the __main__ gives an error because pluggy doesn't allow someone register the module twice try : manager . register ( __main__ ); assert False , \"can't register the module twice\" except ValueError : assert True the proper registration requires we unregister the module first. manager . unregister ( __main__ ) manager . register ( __main__ ); now our invocation finds both implementations. ( results := manager . hook . my_plugin ( a = 10 , b = 20 , c = 30 )) async_my_plugin evaluates to a coroutine that we'd have to handle properly. assert __import__ ( \"inspect\" ) . iscoroutine ( results [ - 1 ])","title":"fumbling around with <pre>pluggy</pre>"},{"location":"xxii/2022-11-12-pluggy-experiments.html#fumbling-around-with-pluggy","text":"https://pluggy.readthedocs.io/ register/unregister plugins define specs and implementations at the same time. import pluggy we're going to think about a sample PROG ram that has an interface. PROG = \"sample-program\" the specification decorates the functions and signatures of our interface. the implementation uses the specification for consistency when it actually defines computational work. implementation , specification = pluggy . HookimplMarker ( PROG ), pluggy . HookspecMarker ( PROG ) PROG s specification for my_plugin provides a default interface. @implementation @specification ( firstresult = False ) def my_plugin ( a , b , c ): return \"\" . join ( map ( str , ( a , b , c ))) we register the specifaction onto a plugin manager manager = pluggy . PluginManager ( PROG ) our specification lives in the import __main__ or MAIN namespace. manager . add_hookspecs ( __main__ := __import__ ( __name__ )) assert \"my_plugin\" in dir ( manager . hook ), \"the plugin is registered\" now we can add our implementation of my_plugin in the __main__ module manager . register ( __main__ ); now we can execute our manager.hook.my_plugin method manager . hook . my_plugin ( a = 10 , b = 20 , c = 30 ) let's add another implementation. we can't name it my_plugin otherwise we'll lose the scope of our previous method. here we name our method whatever we want and explicitly define the implementation:specname it refers to. @implementation ( specname = \"my_plugin\" ) def another_plugin ( a , b ): return ( a , b ) now seems like a good time test what async functions do @implementation ( specname = \"my_plugin\" , trylast = True ) async def async_my_plugin ( a , b ): return ( a , b ) reregistering the __main__ gives an error because pluggy doesn't allow someone register the module twice try : manager . register ( __main__ ); assert False , \"can't register the module twice\" except ValueError : assert True the proper registration requires we unregister the module first. manager . unregister ( __main__ ) manager . register ( __main__ ); now our invocation finds both implementations. ( results := manager . hook . my_plugin ( a = 10 , b = 20 , c = 30 )) async_my_plugin evaluates to a coroutine that we'd have to handle properly. assert __import__ ( \"inspect\" ) . iscoroutine ( results [ - 1 ])","title":"fumbling around with pluggy"},{"location":"xxii/2022-11-17-assignment-expression-display.html","text":"using assignment expressions to display and assign in IPython \u00a4 not all notebok users are aware that there are different implicit display conditions that can be configured with IPython with the ast_node_interactivity option shell = get_ipython () by default, shell.ast_node_interactivity displays the last expressions shell . ast_node_interactivity the other 5 options follow the code below shell . traits ()[ \"ast_node_interactivity\" ] . values sometimes when i am debugging i want to store a variable and display it at the same. the IPython approach set shell.ast_node_interactivity = \"last_expr_or_assign\" . admittedly, i never choose this because i don't want it all the time, just while debugging. what i do instead is set the variable and append an implicit display at the end. my_variable = 42 ; my_variable i've shifted from this approach to using assignment expressions with make more sense. ( my_variable := 42 ) it feels lispy and i like it. ( button := __import__ ( \"ipywidgets\" ) . Button ( description = \"a butt\" ))","title":"using assignment expressions to display and assign in <pre>IPython</pre>"},{"location":"xxii/2022-11-17-assignment-expression-display.html#using-assignment-expressions-to-display-and-assign-in-ipython","text":"not all notebok users are aware that there are different implicit display conditions that can be configured with IPython with the ast_node_interactivity option shell = get_ipython () by default, shell.ast_node_interactivity displays the last expressions shell . ast_node_interactivity the other 5 options follow the code below shell . traits ()[ \"ast_node_interactivity\" ] . values sometimes when i am debugging i want to store a variable and display it at the same. the IPython approach set shell.ast_node_interactivity = \"last_expr_or_assign\" . admittedly, i never choose this because i don't want it all the time, just while debugging. what i do instead is set the variable and append an implicit display at the end. my_variable = 42 ; my_variable i've shifted from this approach to using assignment expressions with make more sense. ( my_variable := 42 ) it feels lispy and i like it. ( button := __import__ ( \"ipywidgets\" ) . Button ( description = \"a butt\" ))","title":"using assignment expressions to display and assign in IPython"},{"location":"xxii/2022-11-23-better-dask-shape.html","text":"unravel directories of notebooks with dask \u00a4 this is another pass at using dask to load notebooks with the ultimate intent to search them. in searching-notebooks , i first approach this task with some keen pandas skills that we not so kind in the dask land. this document takes another pass at using clearer expressions to ravel a bunch of notebooks to dask.dataframe taking care to load notebooks as dask.dataframe s offers the power to apply direct queries, export to parquet, export to sqlite, export to duckdb, arrow.. import pandas , json , jsonpointer , orjson , dask.dataframe ; from pathlib import Path from toolz.curried import * XXX = __name__ == \"__main__\" and \"__file__\" not in locals () if you know the shape then define it \u00a4 dask truly prefers explicit dtypes while pandas is more flexible. meta holds our shape information for the cells, outputs, and displays class meta : O = \"object\" ANY = None , O NB = [( \"cells\" , O ), ( \"metadata\" , O ), ( \"nbformat\" , int ), ( \"nbformat_minor\" , int )] CELL = [ ( \"cell_type\" , str ), ( \"execution_count\" , int ), ( \"id\" , str ), ( \"metadata\" , O ), ( \"outputs\" , O ), ( \"source\" , str ), ( \"cell_ct\" , int ),] OUTPUT = [ ( \"data\" , O ), ( \"metadata\" , O ), ( \"ename\" , str ), ( \"evalue\" , str ), ( \"text\" , str ), ( \"execution_count\" , int ), ( \"output_type\" , str ), ( \"output_ct\" , int )] DISPLAY = [( \"type\" , str ), ( \"value\" , str )] new_nb = pandas . Series ( index = map ( first , NB ), dtype = \"O\" ) new_cell = pandas . Series ( index = map ( first , CELL ), dtype = \"O\" ) new_output = pandas . Series ( index = map ( first , OUTPUT ), dtype = \"O\" ) new_display = pandas . Series ( index = map ( first , DISPLAY ), dtype = \"O\" ) def enumerate_list ( x , key = \"cell_ct\" ): return [{ key : i , ** y } for i , y in enumerate ( x )] def get_series ( data , key = \"text\" , new = meta . new_output ): if key in data : data [ key ] = \"\" . join ( data [ key ]) s = new . copy () return s . update ( data ) or s off to the races as we load some data from our local files. WHERE = Path ( \"oct\" ) the files we include start and remain our index. in prior iterations, there were a few set index operations, but we don't want to be opening files to do this cause that is costly. we'll store other metadata on the dataframe as we unpack the notebook shapes. def get_files ( WHERE = WHERE ): return dask . bag . from_sequence ( dict ( file = str ( x )) for x in WHERE . glob ( \"*.ipynb\" ) ) . to_dataframe () . set_index ( \"file\" ) XXX and ( files := get_files ()) Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npartitions=20 oct/2022-10-05-dask-search.ipynb oct/2022-10-06-github-open-source-stats.ipynb ... oct/2022-11-21-1.ipynb oct/test_nbconvert_html5.ipynb Dask Name: sort_index, 8 graph layers contents loads our files in to a dataframe containg real cell contents. each row is a file. def get_contents_from_files ( files ): return files . index . to_series () . apply ( compose_left ( Path , Path . read_text , orjson . loads , partial ( get_series , new = meta . new_nb )), meta = meta . NB ) XXX and ( contents := get_contents_from_files ( files )) Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cells metadata nbformat nbformat_minor npartitions=20 oct/2022-10-05-dask-search.ipynb object object int64 int64 oct/2022-10-06-github-open-source-stats.ipynb ... ... ... ... ... ... ... ... ... oct/2022-11-21-1.ipynb ... ... ... ... oct/test_nbconvert_html5.ipynb ... ... ... ... Dask Name: apply, 11 graph layers the cells are built by exploding the rows of the contents def get_cells_from_contents ( contents ): cells = contents . cells cells = cells . apply ( enumerate_list , meta = meta . ANY ) return cells . explode () . apply ( get_series , key = \"source\" , new = meta . new_cell , meta = meta . CELL ) if XXX : cells = get_cells_from_contents ( contents ) meta_cells = cells [ \"metadata cell_ct\" . split ()]; cells . pop ( \"metadata\" ); display ( cells ) Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cell_type execution_count id outputs source cell_ct npartitions=20 oct/2022-10-05-dask-search.ipynb object int64 object object object int64 oct/2022-10-06-github-open-source-stats.ipynb ... ... ... ... ... ... ... ... ... ... ... ... ... oct/2022-11-21-1.ipynb ... ... ... ... ... ... oct/test_nbconvert_html5.ipynb ... ... ... ... ... ... Dask Name: drop_by_shallow_copy, 16 graph layers new we deal will outputs that include display_data, stdout, and stderr. def get_outputs_from_cells ( cells ): outputs = cells [ \"outputs cell_ct\" . split ()] . dropna ( subset = \"outputs\" ) outputs . outputs = outputs . outputs . apply ( enumerate_list , key = \"output_ct\" , meta = meta . ANY ) outputs = outputs . explode ( \"outputs\" ) . dropna ( subset = \"outputs\" ) return dask . dataframe . concat ([ outputs . pop ( \"outputs\" ) . apply ( get_series , key = \"text\" , new = meta . new_output , meta = meta . OUTPUT ), outputs ], axis = 1 ) if XXX : outputs = get_outputs_from_cells ( cells ) meta_display = outputs [ \"metadata cell_ct output_ct\" . split ()]; outputs . pop ( \"metadata\" ); display ( outputs ) Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data ename evalue text execution_count output_type output_ct cell_ct npartitions=20 oct/2022-10-05-dask-search.ipynb object object object object int64 object int64 int64 oct/2022-10-06-github-open-source-stats.ipynb ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... oct/2022-11-21-1.ipynb ... ... ... ... ... ... ... ... oct/test_nbconvert_html5.ipynb ... ... ... ... ... ... ... ... Dask Name: drop_by_shallow_copy, 2 graph layers separating the different standard out/error displays from the rich display data. there is probably more to for managing the different types of outputs from the different reprs. def get_display_data_from_outputs ( outputs ): display_data = outputs [ \"data execution_count output_type cell_ct output_ct\" . split ()] . dropna ( subset = \"data\" ) display_data [ \"data\" ] = display_data [ \"data\" ] . apply ( compose_left ( dict . items , list ), meta = meta . ANY ) display_data = display_data . explode ( \"data\" ) . dropna ( subset = \"data\" ) return dask . dataframe . concat ([ display_data . pop ( \"data\" ) . apply ( compose_left ( partial ( zip , meta . new_display . index ), dict , partial ( get_series , key = None , new = meta . new_display ) ), meta = meta . DISPLAY ), display_data ], axis = 1 ) XXX and ( display_data := get_display_data_from_outputs ( outputs )) . compute () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type value execution_count output_type cell_ct output_ct file oct/2022-10-05-dask-search.ipynb text/plain [\"the expected cell keys are ['attachments', '... 3.0 execute_result 7 0 oct/2022-10-05-dask-search.ipynb text/html [<div><strong>Dask DataFrame Structure:</stron... NaN display_data 15 1 oct/2022-10-05-dask-search.ipynb text/plain [Dask DataFrame Structure:\\n, at... NaN display_data 15 1 oct/2022-10-05-dask-search.ipynb text/html [<div>\\n, <style scoped>\\n, .dataframe tbo... NaN display_data 17 0 oct/2022-10-05-dask-search.ipynb text/plain [path ..... NaN display_data 17 0 ... ... ... ... ... ... ... oct/2022-11-21-1.ipynb application/vnd.jupyter.widget-view+json {'model_id': 'cd4f96b3078c4adf851d419b9c8e7885... NaN display_data 4 1 oct/2022-11-21-1.ipynb text/plain [HTML(value='<pre><code>importlib._bootstrap_e... NaN display_data 4 1 oct/test_nbconvert_html5.ipynb text/plain [([], 1)] 4.0 execute_result 5 0 oct/test_nbconvert_html5.ipynb text/plain [([], 16)] 5.0 execute_result 7 0 oct/test_nbconvert_html5.ipynb text/plain [([], 5)] 6.0 execute_result 9 0 173 rows \u00d7 6 columns where to go from \u00a4 extend to other files. the notebook format is a hypermedia document format. save to different formats. initially we think about parquet, while in theory from this dataframe we could go further an imagine it being the seed for documentation. XXX and display ( * ( x . sample ( frac = .1 ) . compute () . sample ( 5 ) for x in ( cells , outputs , display_data ))) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cell_type execution_count id outputs source cell_ct file oct/2022-10-29-metadata-formatter.ipynb markdown NaN 50d9dcf9-18f4-4eb7-9a9a-cc3e14b3b2e2 NaN #### dataframes 16 oct/2022-10-06-github-open-source-stats.ipynb code 197.0 af8301f0-8a28-4350-87fa-a728867ccf67 [] %reload_ext doit 6 oct/2022-10-21-markdown-future.ipynb code 5.0 21fa2ad9-23f9-4868-9cb7-3bb87b69c542 [{'data': {'text/markdown': ['# tangle (code) ... # tangle (code) and weave (display)\\n\\n kni... 7 oct/2022-10-19-mobius-text.ipynb code NaN 55228766-71c2-48f0-819f-afab637a4867 [] 18 oct/2022-10-27-axe-core-playwright-python.ipynb code 17.0 6e7cb333-6cc3-4c73-b256-382bcb3e9c2c [] async def injectAxe(page): \\n await page.ev... 9 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data ename evalue text execution_count output_type output_ct cell_ct file oct/2022-10-21-markdown-future.ipynb {'text/markdown': ['i'm not saying importing m... NaN NaN NaN NaN display_data 0 17 oct/2022-11-17--Copy1.ipynb {'application/vnd.jupyter.widget-view+json': {... NaN NaN NaN NaN display_data 0 6 oct/2022-10-21-markdown-future.ipynb {'text/markdown': ['```mermaid ', 'flowchart L... NaN NaN NaN NaN display_data 1 16 oct/2022-10-29-.ipynb NaN AttributeError 'dict' object has no attribute 'breaks' NaN NaN error 1 12 oct/2022-10-21-pidgy-displays.ipynb {'application/vnd.jupyter.widget-view+json': {... NaN NaN NaN NaN display_data 0 17 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type value execution_count output_type cell_ct output_ct file oct/2022-10-05-dask-search.ipynb text/html [<div>\\n, <style scoped>\\n, .dataframe tbo... NaN display_data 19 0 oct/2022-11-21-1.ipynb text/plain [HTML(value='<pre><code>__</code></pre>\\n')] NaN display_data 2 1 oct/2022-10-29-.ipynb application/vnd.jupyter.widget-view+json {'model_id': 'fa05bd5e43cc4785834b4b87ec154fbe... NaN display_data 11 0 oct/2022-11-17--Copy1.ipynb text/plain [HTML(value='<pre><code>notebooks = pandas.con... NaN display_data 5 0 oct/2022-10-21-markdown-future.ipynb text/markdown [## literate computing with literary machines ... NaN display_data 5 0 from dataclasses import dataclass , field @dataclass class Contents : dir : Path = field ( default_factory = Path . cwd ) contents : dask . dataframe . DataFrame = None def __post_init__ ( self ): self . contents = get_contents_from_files ( get_files ( self . dir )) self . cells = get_cells_from_contents ( self . contents ) self . outputs = get_outputs_from_cells ( self . cells ) self . display_data = self . get_display_data_from_outputs ( self . outputs )","title":"unravel directories of notebooks with <pre>dask</pre>"},{"location":"xxii/2022-11-23-better-dask-shape.html#unravel-directories-of-notebooks-with-dask","text":"this is another pass at using dask to load notebooks with the ultimate intent to search them. in searching-notebooks , i first approach this task with some keen pandas skills that we not so kind in the dask land. this document takes another pass at using clearer expressions to ravel a bunch of notebooks to dask.dataframe taking care to load notebooks as dask.dataframe s offers the power to apply direct queries, export to parquet, export to sqlite, export to duckdb, arrow.. import pandas , json , jsonpointer , orjson , dask.dataframe ; from pathlib import Path from toolz.curried import * XXX = __name__ == \"__main__\" and \"__file__\" not in locals ()","title":"unravel directories of notebooks with dask"},{"location":"xxii/2022-11-23-better-dask-shape.html#if-you-know-the-shape-then-define-it","text":"dask truly prefers explicit dtypes while pandas is more flexible. meta holds our shape information for the cells, outputs, and displays class meta : O = \"object\" ANY = None , O NB = [( \"cells\" , O ), ( \"metadata\" , O ), ( \"nbformat\" , int ), ( \"nbformat_minor\" , int )] CELL = [ ( \"cell_type\" , str ), ( \"execution_count\" , int ), ( \"id\" , str ), ( \"metadata\" , O ), ( \"outputs\" , O ), ( \"source\" , str ), ( \"cell_ct\" , int ),] OUTPUT = [ ( \"data\" , O ), ( \"metadata\" , O ), ( \"ename\" , str ), ( \"evalue\" , str ), ( \"text\" , str ), ( \"execution_count\" , int ), ( \"output_type\" , str ), ( \"output_ct\" , int )] DISPLAY = [( \"type\" , str ), ( \"value\" , str )] new_nb = pandas . Series ( index = map ( first , NB ), dtype = \"O\" ) new_cell = pandas . Series ( index = map ( first , CELL ), dtype = \"O\" ) new_output = pandas . Series ( index = map ( first , OUTPUT ), dtype = \"O\" ) new_display = pandas . Series ( index = map ( first , DISPLAY ), dtype = \"O\" ) def enumerate_list ( x , key = \"cell_ct\" ): return [{ key : i , ** y } for i , y in enumerate ( x )] def get_series ( data , key = \"text\" , new = meta . new_output ): if key in data : data [ key ] = \"\" . join ( data [ key ]) s = new . copy () return s . update ( data ) or s off to the races as we load some data from our local files. WHERE = Path ( \"oct\" ) the files we include start and remain our index. in prior iterations, there were a few set index operations, but we don't want to be opening files to do this cause that is costly. we'll store other metadata on the dataframe as we unpack the notebook shapes. def get_files ( WHERE = WHERE ): return dask . bag . from_sequence ( dict ( file = str ( x )) for x in WHERE . glob ( \"*.ipynb\" ) ) . to_dataframe () . set_index ( \"file\" ) XXX and ( files := get_files ()) Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npartitions=20 oct/2022-10-05-dask-search.ipynb oct/2022-10-06-github-open-source-stats.ipynb ... oct/2022-11-21-1.ipynb oct/test_nbconvert_html5.ipynb Dask Name: sort_index, 8 graph layers contents loads our files in to a dataframe containg real cell contents. each row is a file. def get_contents_from_files ( files ): return files . index . to_series () . apply ( compose_left ( Path , Path . read_text , orjson . loads , partial ( get_series , new = meta . new_nb )), meta = meta . NB ) XXX and ( contents := get_contents_from_files ( files )) Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cells metadata nbformat nbformat_minor npartitions=20 oct/2022-10-05-dask-search.ipynb object object int64 int64 oct/2022-10-06-github-open-source-stats.ipynb ... ... ... ... ... ... ... ... ... oct/2022-11-21-1.ipynb ... ... ... ... oct/test_nbconvert_html5.ipynb ... ... ... ... Dask Name: apply, 11 graph layers the cells are built by exploding the rows of the contents def get_cells_from_contents ( contents ): cells = contents . cells cells = cells . apply ( enumerate_list , meta = meta . ANY ) return cells . explode () . apply ( get_series , key = \"source\" , new = meta . new_cell , meta = meta . CELL ) if XXX : cells = get_cells_from_contents ( contents ) meta_cells = cells [ \"metadata cell_ct\" . split ()]; cells . pop ( \"metadata\" ); display ( cells ) Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cell_type execution_count id outputs source cell_ct npartitions=20 oct/2022-10-05-dask-search.ipynb object int64 object object object int64 oct/2022-10-06-github-open-source-stats.ipynb ... ... ... ... ... ... ... ... ... ... ... ... ... oct/2022-11-21-1.ipynb ... ... ... ... ... ... oct/test_nbconvert_html5.ipynb ... ... ... ... ... ... Dask Name: drop_by_shallow_copy, 16 graph layers new we deal will outputs that include display_data, stdout, and stderr. def get_outputs_from_cells ( cells ): outputs = cells [ \"outputs cell_ct\" . split ()] . dropna ( subset = \"outputs\" ) outputs . outputs = outputs . outputs . apply ( enumerate_list , key = \"output_ct\" , meta = meta . ANY ) outputs = outputs . explode ( \"outputs\" ) . dropna ( subset = \"outputs\" ) return dask . dataframe . concat ([ outputs . pop ( \"outputs\" ) . apply ( get_series , key = \"text\" , new = meta . new_output , meta = meta . OUTPUT ), outputs ], axis = 1 ) if XXX : outputs = get_outputs_from_cells ( cells ) meta_display = outputs [ \"metadata cell_ct output_ct\" . split ()]; outputs . pop ( \"metadata\" ); display ( outputs ) Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data ename evalue text execution_count output_type output_ct cell_ct npartitions=20 oct/2022-10-05-dask-search.ipynb object object object object int64 object int64 int64 oct/2022-10-06-github-open-source-stats.ipynb ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... oct/2022-11-21-1.ipynb ... ... ... ... ... ... ... ... oct/test_nbconvert_html5.ipynb ... ... ... ... ... ... ... ... Dask Name: drop_by_shallow_copy, 2 graph layers separating the different standard out/error displays from the rich display data. there is probably more to for managing the different types of outputs from the different reprs. def get_display_data_from_outputs ( outputs ): display_data = outputs [ \"data execution_count output_type cell_ct output_ct\" . split ()] . dropna ( subset = \"data\" ) display_data [ \"data\" ] = display_data [ \"data\" ] . apply ( compose_left ( dict . items , list ), meta = meta . ANY ) display_data = display_data . explode ( \"data\" ) . dropna ( subset = \"data\" ) return dask . dataframe . concat ([ display_data . pop ( \"data\" ) . apply ( compose_left ( partial ( zip , meta . new_display . index ), dict , partial ( get_series , key = None , new = meta . new_display ) ), meta = meta . DISPLAY ), display_data ], axis = 1 ) XXX and ( display_data := get_display_data_from_outputs ( outputs )) . compute () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type value execution_count output_type cell_ct output_ct file oct/2022-10-05-dask-search.ipynb text/plain [\"the expected cell keys are ['attachments', '... 3.0 execute_result 7 0 oct/2022-10-05-dask-search.ipynb text/html [<div><strong>Dask DataFrame Structure:</stron... NaN display_data 15 1 oct/2022-10-05-dask-search.ipynb text/plain [Dask DataFrame Structure:\\n, at... NaN display_data 15 1 oct/2022-10-05-dask-search.ipynb text/html [<div>\\n, <style scoped>\\n, .dataframe tbo... NaN display_data 17 0 oct/2022-10-05-dask-search.ipynb text/plain [path ..... NaN display_data 17 0 ... ... ... ... ... ... ... oct/2022-11-21-1.ipynb application/vnd.jupyter.widget-view+json {'model_id': 'cd4f96b3078c4adf851d419b9c8e7885... NaN display_data 4 1 oct/2022-11-21-1.ipynb text/plain [HTML(value='<pre><code>importlib._bootstrap_e... NaN display_data 4 1 oct/test_nbconvert_html5.ipynb text/plain [([], 1)] 4.0 execute_result 5 0 oct/test_nbconvert_html5.ipynb text/plain [([], 16)] 5.0 execute_result 7 0 oct/test_nbconvert_html5.ipynb text/plain [([], 5)] 6.0 execute_result 9 0 173 rows \u00d7 6 columns","title":"if you know the shape then define it"},{"location":"xxii/2022-11-23-better-dask-shape.html#where-to-go-from","text":"extend to other files. the notebook format is a hypermedia document format. save to different formats. initially we think about parquet, while in theory from this dataframe we could go further an imagine it being the seed for documentation. XXX and display ( * ( x . sample ( frac = .1 ) . compute () . sample ( 5 ) for x in ( cells , outputs , display_data ))) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cell_type execution_count id outputs source cell_ct file oct/2022-10-29-metadata-formatter.ipynb markdown NaN 50d9dcf9-18f4-4eb7-9a9a-cc3e14b3b2e2 NaN #### dataframes 16 oct/2022-10-06-github-open-source-stats.ipynb code 197.0 af8301f0-8a28-4350-87fa-a728867ccf67 [] %reload_ext doit 6 oct/2022-10-21-markdown-future.ipynb code 5.0 21fa2ad9-23f9-4868-9cb7-3bb87b69c542 [{'data': {'text/markdown': ['# tangle (code) ... # tangle (code) and weave (display)\\n\\n kni... 7 oct/2022-10-19-mobius-text.ipynb code NaN 55228766-71c2-48f0-819f-afab637a4867 [] 18 oct/2022-10-27-axe-core-playwright-python.ipynb code 17.0 6e7cb333-6cc3-4c73-b256-382bcb3e9c2c [] async def injectAxe(page): \\n await page.ev... 9 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data ename evalue text execution_count output_type output_ct cell_ct file oct/2022-10-21-markdown-future.ipynb {'text/markdown': ['i'm not saying importing m... NaN NaN NaN NaN display_data 0 17 oct/2022-11-17--Copy1.ipynb {'application/vnd.jupyter.widget-view+json': {... NaN NaN NaN NaN display_data 0 6 oct/2022-10-21-markdown-future.ipynb {'text/markdown': ['```mermaid ', 'flowchart L... NaN NaN NaN NaN display_data 1 16 oct/2022-10-29-.ipynb NaN AttributeError 'dict' object has no attribute 'breaks' NaN NaN error 1 12 oct/2022-10-21-pidgy-displays.ipynb {'application/vnd.jupyter.widget-view+json': {... NaN NaN NaN NaN display_data 0 17 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type value execution_count output_type cell_ct output_ct file oct/2022-10-05-dask-search.ipynb text/html [<div>\\n, <style scoped>\\n, .dataframe tbo... NaN display_data 19 0 oct/2022-11-21-1.ipynb text/plain [HTML(value='<pre><code>__</code></pre>\\n')] NaN display_data 2 1 oct/2022-10-29-.ipynb application/vnd.jupyter.widget-view+json {'model_id': 'fa05bd5e43cc4785834b4b87ec154fbe... NaN display_data 11 0 oct/2022-11-17--Copy1.ipynb text/plain [HTML(value='<pre><code>notebooks = pandas.con... NaN display_data 5 0 oct/2022-10-21-markdown-future.ipynb text/markdown [## literate computing with literary machines ... NaN display_data 5 0 from dataclasses import dataclass , field @dataclass class Contents : dir : Path = field ( default_factory = Path . cwd ) contents : dask . dataframe . DataFrame = None def __post_init__ ( self ): self . contents = get_contents_from_files ( get_files ( self . dir )) self . cells = get_cells_from_contents ( self . contents ) self . outputs = get_outputs_from_cells ( self . cells ) self . display_data = self . get_display_data_from_outputs ( self . outputs )","title":"where to go from"},{"location":"xxii/2022-11-26-package-mimic-and-rel-imports.html","text":"making __main__ a __package__ \u00a4 to make an interactive computing session, or a python module generally, appear like a package we need to set two varaiables. import pytest ; from pathlib import Path there is a python file one directory up called mkdocs.py . we should be able to get at this file, and we will. our first transgression is not being a package. with pytest . raises ( ImportError , match = \"attempted relative import with no known parent package\" ): from . import mkdocs to be a package we need __package__ and __path__ __package__ , __path__ = __name__ , [ \"\" ] the __path__ contains the lookup directories for parts of the package; this is a feature of namespace packages AND where shit gets weird. now that we quack like a package we can attempt relative imports, but mkdocs.py is still out of reach. with pytest . raises ( ImportError , match = \"cannot import name 'mkdocs' from '__main__' \\(unknown location\\)\" ): from . import mkdocs append the parent to the __path__ __path__ += [ \"..\" ] and boom! from . import mkdocs #! how is this different from sys.paths \u00a4 it is probably not much different, but more a cheat code you can abuse at your own \u26a0","title":"making <pre>__main__</pre> a <pre>__package__</pre>"},{"location":"xxii/2022-11-26-package-mimic-and-rel-imports.html#making-__main__-a-__package__","text":"to make an interactive computing session, or a python module generally, appear like a package we need to set two varaiables. import pytest ; from pathlib import Path there is a python file one directory up called mkdocs.py . we should be able to get at this file, and we will. our first transgression is not being a package. with pytest . raises ( ImportError , match = \"attempted relative import with no known parent package\" ): from . import mkdocs to be a package we need __package__ and __path__ __package__ , __path__ = __name__ , [ \"\" ] the __path__ contains the lookup directories for parts of the package; this is a feature of namespace packages AND where shit gets weird. now that we quack like a package we can attempt relative imports, but mkdocs.py is still out of reach. with pytest . raises ( ImportError , match = \"cannot import name 'mkdocs' from '__main__' \\(unknown location\\)\" ): from . import mkdocs append the parent to the __path__ __path__ += [ \"..\" ] and boom! from . import mkdocs #!","title":"making __main__ a __package__"},{"location":"xxii/2022-11-26-package-mimic-and-rel-imports.html#how-is-this-different-from-syspaths","text":"it is probably not much different, but more a cheat code you can abuse at your own \u26a0","title":"how is this different from sys.paths"},{"location":"xxii/2022-12-05-markdown-to-doctest.html","text":"formatting markdown it tokens as python doctests \u00a4 midgy is a tool i've been crafting that translates markdown to valid python. this concept might sound perculiar from a programming perspective, but it was designed as a [literate programming] tool. to avoid feature creep, midgy tries to stick fairly close to the commonspec when tokenizing markdown. midgy adds a doctest token to the parser. we make this addition because doctest is a [literate programming] considered in the core python language. in this document, we convert the markdown_it tokens into valid doctest.DocTest runners. import midgy , doctest , unittest , typing from textwrap import dedent write some sample doctests to parse def a_testable_function (): \"\"\" >>> range(1) range(0, 1) >>> assert False Traceback (most recent call last): ... AssertionError \"\"\" verify that these tests pass. doctest.testmod runs doctest on the __main__ module. doctest . testmod ( optionflags = doctest . ELLIPSIS ) make some markdown_it tokens from the doctests. ( tokens := ( parser := midgy . Python ()) . parse ( a_testable_function . __doc__ )) get_example_from_token translates a markdown token to a doctest def get_example_from_token ( token ) -> typing . Iterable [ doctest . Example ]: m = doctest . DocTestParser . _EXAMPLE_RE . match ( token . content ) want = dedent ( m . group ( \"want\" )) exc = doctest . DocTestParser . _EXCEPTION_RE . match ( want ) source = \"\" . join ( x . lstrip ()[ 4 :] for x in m . group ( \"source\" ) . splitlines ( 1 )) yield doctest . Example ( source = source , want = want , lineno = token . map [ 0 ], exc_msg = exc . group ( \"msg\" ) if exc else None , indent = len ( m . group ( \"indent\" )) ) get_examples_from_tokens aggregates the doctest.Example s def get_examples_from_tokens ( tokens ) -> typing . Iterable [ doctest . Example ]: for token in tokens : if token . meta . get ( \"is_doctest\" ): yield from get_example_from_token ( token ) finally we generate a unittest.TestSuite def get_suite_from_tokens ( tokens ) -> unittest . TestSuite : suite = unittest . TestSuite () for example in get_examples_from_tokens ( tokens ): suite . addTest ( doctest . DocTestCase ( doctest . DocTest ([ example ], globals (), __name__ , None , example . lineno , None ), optionflags = doctest . ELLIPSIS )) return suite def run_suite ( suite = ( suite := get_suite_from_tokens ( tokens ))) -> unittest . TestResult : suite . run ( result := unittest . TestResult ()) return result run our generated doctest suite to verify that it doesn't fail. run_suite ( suite )","title":"formatting markdown it tokens as python <pre>doctests</pre>"},{"location":"xxii/2022-12-05-markdown-to-doctest.html#formatting-markdown-it-tokens-as-python-doctests","text":"midgy is a tool i've been crafting that translates markdown to valid python. this concept might sound perculiar from a programming perspective, but it was designed as a [literate programming] tool. to avoid feature creep, midgy tries to stick fairly close to the commonspec when tokenizing markdown. midgy adds a doctest token to the parser. we make this addition because doctest is a [literate programming] considered in the core python language. in this document, we convert the markdown_it tokens into valid doctest.DocTest runners. import midgy , doctest , unittest , typing from textwrap import dedent write some sample doctests to parse def a_testable_function (): \"\"\" >>> range(1) range(0, 1) >>> assert False Traceback (most recent call last): ... AssertionError \"\"\" verify that these tests pass. doctest.testmod runs doctest on the __main__ module. doctest . testmod ( optionflags = doctest . ELLIPSIS ) make some markdown_it tokens from the doctests. ( tokens := ( parser := midgy . Python ()) . parse ( a_testable_function . __doc__ )) get_example_from_token translates a markdown token to a doctest def get_example_from_token ( token ) -> typing . Iterable [ doctest . Example ]: m = doctest . DocTestParser . _EXAMPLE_RE . match ( token . content ) want = dedent ( m . group ( \"want\" )) exc = doctest . DocTestParser . _EXCEPTION_RE . match ( want ) source = \"\" . join ( x . lstrip ()[ 4 :] for x in m . group ( \"source\" ) . splitlines ( 1 )) yield doctest . Example ( source = source , want = want , lineno = token . map [ 0 ], exc_msg = exc . group ( \"msg\" ) if exc else None , indent = len ( m . group ( \"indent\" )) ) get_examples_from_tokens aggregates the doctest.Example s def get_examples_from_tokens ( tokens ) -> typing . Iterable [ doctest . Example ]: for token in tokens : if token . meta . get ( \"is_doctest\" ): yield from get_example_from_token ( token ) finally we generate a unittest.TestSuite def get_suite_from_tokens ( tokens ) -> unittest . TestSuite : suite = unittest . TestSuite () for example in get_examples_from_tokens ( tokens ): suite . addTest ( doctest . DocTestCase ( doctest . DocTest ([ example ], globals (), __name__ , None , example . lineno , None ), optionflags = doctest . ELLIPSIS )) return suite def run_suite ( suite = ( suite := get_suite_from_tokens ( tokens ))) -> unittest . TestResult : suite . run ( result := unittest . TestResult ()) return result run our generated doctest suite to verify that it doesn't fail. run_suite ( suite )","title":"formatting markdown it tokens as python doctests"},{"location":"xxii/2022-12-09-pyproject-analysis.html","text":"exploring many pyproject.toml configs \u00a4 i composed the following query using github's graphql explorer because it has completion which helps in the composition. i also refered to GitHub GraphQL - Get files in a repository for some ideas about how to compose my query. this work is my first time interacting with graphql for data analysis. i really preferred the iGraphQl experiences that provides completion otherwise i would have been totally lost. it uses requests to retreieve the results, and requsts_cache for caching. at the end, we start looking at some dataframes for our requests. from typing import * ; from toolz.curried import * ; import pandas , requests , tomli pyproject_query = \"\"\" { search(type: REPOSITORY, query: \"install in:readme language:python stars:>500\", first:100 %s ) { pageInfo { hasNextPage endCursor } edges { node { ... on Repository { url stargazerCount object(expression:\"HEAD:pyproject.toml\") { ... on Blob { text } } } } } } }\"\"\" the graqhql query i wanted retrieves the pyproject.toml from a bunch of python projects. the initial goal of this query is to discover python projects and retrieve their pyproject.toml for comparison. we are looking for pyproject.toml which outlines strict metadata specifications . it would be cool to get a high level view of the python conventions popular projects are using. i'd love suggestions on a better query that finds more repositories with pyproject.toml files. paginating the requests to get a bunch of data \u00a4 get_one_page makes a POST the github graphql endpoint - https://api.github.com/graphql def get_one_page ( query : str , prior : requests . Response = None , fill : str = \"\" ) -> requests . Response : if prior and prior . json ()[ \"data\" ][ \"search\" ][ \"pageInfo\" ][ \"hasNextPage\" ]: fill = \"\"\", after: \" %s \" \"\"\" % prior . json ()[ \"data\" ][ \"search\" ][ \"pageInfo\" ][ \"endCursor\" ] return requests . post ( \"https://api.github.com/graphql\" , json = dict ( query = query % fill ), ** header ) get_pages yields multiple requests if there is pagination the nodes exported. def get_pages ( query : str , prior = None , max = 15 ): for i in range ( max ): prior = get_one_page ( query , prior = prior ) yield prior if prior . status_code != 200 : break if not prior . json ()[ \"data\" ][ \"search\" ][ \"pageInfo\" ][ \"hasNextPage\" ]: break gather a few pages into a list of responses def gather ( query : str , max : int = 2 ): return list ( get_pages ( query , max = max )) analyze some actual data \u00a4 boilerplate to begin the analysis __import__ ( \"requests_cache\" ) . install_cache ( allowable_methods = [ 'GET' , 'POST' ]) from info import header # this has some api info pandas . options . display . max_colwidth = None \u00d8 = __name__ == \"__main__\" and \"__file__\" not in locals () transform the responses in a big pandas dataframe of configs tidy_responses transforms our query responses into a single dataframe. def tidy_responses ( responses : list [ requests . Response ]) -> pandas . DataFrame : return pipe ( responses , map ( compose_left ( operator . methodcaller ( \"json\" ), get ( \"data\" ), get ( \"search\" ), get ( \"edges\" ), pandas . DataFrame ) ), partial ( pandas . concat , axis = 1 )) . stack () tidy_configs further shapes the data down to the pyproject.toml data def tidy_configs ( df : pandas . DataFrame ) -> pandas . DataFrame : return df . apply ( pandas . Series ) . dropna ( subset = \"object\" ) \\ . set_index ( \"url\" )[ \"object\" ] . apply ( pandas . Series )[ \"text\" ] . apply ( tomli . loads ) . apply ( pandas . Series ) if \u00d8 : configs = tidy_configs ( df := tidy_responses ( responses := gather ( pyproject_query , max = 15 ))) print ( F \"\"\"we made { len ( responses ) } requests returning information about a { len ( df ) } repositories. we retrieved { len ( configs ) } pyproject configs from this scrape.\"\"\" ) we made 10 requests returning information about a 1000 repositories. we retrieved 234 pyproject configs from this scrape. inspecting the build backend \u00a4 if \u00d8 : builds = configs [ \"build-system\" ] . dropna () . apply ( pandas . Series ) print ( F \"\"\" { len ( builds ) } projects define a build backends, their specific frequencies are:\"\"\" ) display ( builds [ \"build-backend\" ] . dropna () . value_counts () . to_frame ( \"build-backend\" ) . T ) 173 projects define a build backends, their specific frequencies are: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } setuptools.build_meta poetry.core.masonry.api hatchling.build flit_core.buildapi poetry.masonry.api pdm.pep517.api mesonpy poetry_dynamic_versioning.backend build-backend 88 25 15 8 4 2 1 1 inspecting the tools \u00a4 the different tool frequencies if \u00d8 : ranks = configs [ \"tool\" ] . dropna () . apply ( list ) . apply ( pandas . Series ) . stack () . value_counts () display ( ranks [( top := ranks > 4 )] . to_frame ( \"top\" ) . T , ranks [ ~ top ] . to_frame ( \"rest\" ) . T ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } black isort pytest mypy coverage poetry setuptools_scm hatch setuptools pylint towncrier pyright cibuildwheel usort flit top 123 85 67 42 34 32 21 15 14 14 11 10 6 5 5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } nbqa flake8 tox ruff pycln pydocstyle autoflake interrogate tbump codespell ... versioningit poe distutils setuptools-git-versioning ufmt hooky bandit versioneer mutmut typeshed rest 4 3 3 3 3 3 2 2 2 2 ... 1 1 1 1 1 1 1 1 1 1 1 rows \u00d7 39 columns fin \u00a4 i think having knowledge at this scope of projects helps making decisions about what to do with your own. if black is the zeigeist why are you stalling?","title":"exploring many <pre>pyproject.toml configs</pre>"},{"location":"xxii/2022-12-09-pyproject-analysis.html#exploring-many-pyprojecttoml-configs","text":"i composed the following query using github's graphql explorer because it has completion which helps in the composition. i also refered to GitHub GraphQL - Get files in a repository for some ideas about how to compose my query. this work is my first time interacting with graphql for data analysis. i really preferred the iGraphQl experiences that provides completion otherwise i would have been totally lost. it uses requests to retreieve the results, and requsts_cache for caching. at the end, we start looking at some dataframes for our requests. from typing import * ; from toolz.curried import * ; import pandas , requests , tomli pyproject_query = \"\"\" { search(type: REPOSITORY, query: \"install in:readme language:python stars:>500\", first:100 %s ) { pageInfo { hasNextPage endCursor } edges { node { ... on Repository { url stargazerCount object(expression:\"HEAD:pyproject.toml\") { ... on Blob { text } } } } } } }\"\"\" the graqhql query i wanted retrieves the pyproject.toml from a bunch of python projects. the initial goal of this query is to discover python projects and retrieve their pyproject.toml for comparison. we are looking for pyproject.toml which outlines strict metadata specifications . it would be cool to get a high level view of the python conventions popular projects are using. i'd love suggestions on a better query that finds more repositories with pyproject.toml files.","title":"exploring many pyproject.toml configs"},{"location":"xxii/2022-12-09-pyproject-analysis.html#paginating-the-requests-to-get-a-bunch-of-data","text":"get_one_page makes a POST the github graphql endpoint - https://api.github.com/graphql def get_one_page ( query : str , prior : requests . Response = None , fill : str = \"\" ) -> requests . Response : if prior and prior . json ()[ \"data\" ][ \"search\" ][ \"pageInfo\" ][ \"hasNextPage\" ]: fill = \"\"\", after: \" %s \" \"\"\" % prior . json ()[ \"data\" ][ \"search\" ][ \"pageInfo\" ][ \"endCursor\" ] return requests . post ( \"https://api.github.com/graphql\" , json = dict ( query = query % fill ), ** header ) get_pages yields multiple requests if there is pagination the nodes exported. def get_pages ( query : str , prior = None , max = 15 ): for i in range ( max ): prior = get_one_page ( query , prior = prior ) yield prior if prior . status_code != 200 : break if not prior . json ()[ \"data\" ][ \"search\" ][ \"pageInfo\" ][ \"hasNextPage\" ]: break gather a few pages into a list of responses def gather ( query : str , max : int = 2 ): return list ( get_pages ( query , max = max ))","title":"paginating the requests to get a bunch of data"},{"location":"xxii/2022-12-09-pyproject-analysis.html#analyze-some-actual-data","text":"boilerplate to begin the analysis __import__ ( \"requests_cache\" ) . install_cache ( allowable_methods = [ 'GET' , 'POST' ]) from info import header # this has some api info pandas . options . display . max_colwidth = None \u00d8 = __name__ == \"__main__\" and \"__file__\" not in locals () transform the responses in a big pandas dataframe of configs tidy_responses transforms our query responses into a single dataframe. def tidy_responses ( responses : list [ requests . Response ]) -> pandas . DataFrame : return pipe ( responses , map ( compose_left ( operator . methodcaller ( \"json\" ), get ( \"data\" ), get ( \"search\" ), get ( \"edges\" ), pandas . DataFrame ) ), partial ( pandas . concat , axis = 1 )) . stack () tidy_configs further shapes the data down to the pyproject.toml data def tidy_configs ( df : pandas . DataFrame ) -> pandas . DataFrame : return df . apply ( pandas . Series ) . dropna ( subset = \"object\" ) \\ . set_index ( \"url\" )[ \"object\" ] . apply ( pandas . Series )[ \"text\" ] . apply ( tomli . loads ) . apply ( pandas . Series ) if \u00d8 : configs = tidy_configs ( df := tidy_responses ( responses := gather ( pyproject_query , max = 15 ))) print ( F \"\"\"we made { len ( responses ) } requests returning information about a { len ( df ) } repositories. we retrieved { len ( configs ) } pyproject configs from this scrape.\"\"\" ) we made 10 requests returning information about a 1000 repositories. we retrieved 234 pyproject configs from this scrape.","title":"analyze some actual data"},{"location":"xxii/2022-12-09-pyproject-analysis.html#inspecting-the-build-backend","text":"if \u00d8 : builds = configs [ \"build-system\" ] . dropna () . apply ( pandas . Series ) print ( F \"\"\" { len ( builds ) } projects define a build backends, their specific frequencies are:\"\"\" ) display ( builds [ \"build-backend\" ] . dropna () . value_counts () . to_frame ( \"build-backend\" ) . T ) 173 projects define a build backends, their specific frequencies are: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } setuptools.build_meta poetry.core.masonry.api hatchling.build flit_core.buildapi poetry.masonry.api pdm.pep517.api mesonpy poetry_dynamic_versioning.backend build-backend 88 25 15 8 4 2 1 1","title":"inspecting the build backend"},{"location":"xxii/2022-12-09-pyproject-analysis.html#inspecting-the-tools","text":"the different tool frequencies if \u00d8 : ranks = configs [ \"tool\" ] . dropna () . apply ( list ) . apply ( pandas . Series ) . stack () . value_counts () display ( ranks [( top := ranks > 4 )] . to_frame ( \"top\" ) . T , ranks [ ~ top ] . to_frame ( \"rest\" ) . T ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } black isort pytest mypy coverage poetry setuptools_scm hatch setuptools pylint towncrier pyright cibuildwheel usort flit top 123 85 67 42 34 32 21 15 14 14 11 10 6 5 5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } nbqa flake8 tox ruff pycln pydocstyle autoflake interrogate tbump codespell ... versioningit poe distutils setuptools-git-versioning ufmt hooky bandit versioneer mutmut typeshed rest 4 3 3 3 3 3 2 2 2 2 ... 1 1 1 1 1 1 1 1 1 1 1 rows \u00d7 39 columns","title":"inspecting the tools"},{"location":"xxii/2022-12-09-pyproject-analysis.html#fin","text":"i think having knowledge at this scope of projects helps making decisions about what to do with your own. if black is the zeigeist why are you stalling?","title":"fin"},{"location":"xxii/2022-12-13-github-api-stats.html","text":"extracting info from the github statistics api \u00a4 i've wanted to take looks at github repositories over time. what kind of trends are there revealed over the years of projects? what can we infer from past actions that might predicate success? this work is an extension of the pyproject.toml analysis . we use the same list of respositories to generate their statistics. doit is likely a suboptimal choice for this, so we'll probably want to try dask later. with __import__ ( \"importnb\" ) . Notebook (): import __12_09_pyproject_analysis as prior import pandas df = prior . tidy_responses ( responses := prior . gather ( prior . pyproject_query )) . apply ( pandas . Series ) projects = df . url . str . rsplit ( \"/\" , 2 , expand = True )[[ 1 , 2 ]] . apply ( \"/\" . join , axis = 1 ) from info import header import requests , requests_cache , uritemplate , toolz , operator , doit requests_cache . install_cache () from info import header the endpoints we retrieve the stats from GH = \"https://api.github.com\" STATS = \"code_frequency commit_activity participation\" . split () URI = uritemplate . URITemplate ( GH + \"/repos/ {owner} / {repo} /stats/ {stat} \" ) RELEASES = uritemplate . URITemplate ( GH + \"/repos/ {owner} / {repo} /releases\" ) get_stats is a function that tries again when a 202 status is returned. def get_stats ( url ): response = requests . get ( url , ** header ) if response . status_code == 202 : __import__ ( \"time\" ) . sleep ( 1 ) return get_stats ( url ) return response . json () @doit . task_params ([ dict ( name = \"project\" , type = list , long = \"project\" , default = [])]) def task_collect_stats ( project = projects ): for url , owner , repo in get_iterables ( project ): * _ , stat = url . rpartition ( \"/\" ) yield dict ( name = \"-\" . join (( owner , repo , stat )), actions = [( toolz . curried . do ( get_stats ), [ url ])]) def get_iterables ( projects = projects ): for owner , repo in ( x . split ( \"/\" ) for x in projects ): for stat in STATS : yield URI . expand ( locals ()), owner , repo yield RELEASES . expand ( locals ()), owner , repo retrieve the data with doit \u00a4 % reload_ext doit projects_args = \" \" . join ( projects . apply ( \"--project %s \" . __mod__ )) % doit run collect_stats $ projects_args . collect_stats:tensorflow-models-code_frequency . collect_stats:tensorflow-models-commit_activity . collect_stats:tensorflow-models-participation . collect_stats:tensorflow-models-releases . collect_stats:mongodb-mongo-python-driver-code_frequency . collect_stats:mongodb-mongo-python-driver-commit_activity . collect_stats:mongodb-mongo-python-driver-participation . collect_stats:mongodb-mongo-python-driver-releases . collect_stats:3b1b-manim-code_frequency . collect_stats:3b1b-manim-commit_activity . collect_stats:3b1b-manim-participation . collect_stats:3b1b-manim-releases . collect_stats:alievk-avatarify-python-code_frequency . collect_stats:alievk-avatarify-python-commit_activity . collect_stats:alievk-avatarify-python-participation . collect_stats:alievk-avatarify-python-releases . collect_stats:python-cpython-code_frequency . collect_stats:python-cpython-commit_activity . collect_stats:python-cpython-participation . collect_stats:python-cpython-releases . collect_stats:ehmatthes-pcc-code_frequency . collect_stats:ehmatthes-pcc-commit_activity . collect_stats:ehmatthes-pcc-participation . collect_stats:ehmatthes-pcc-releases . collect_stats:openai-gym-code_frequency . collect_stats:openai-gym-commit_activity . collect_stats:openai-gym-participation . collect_stats:openai-gym-releases . collect_stats:openai-DALL-E-code_frequency . collect_stats:openai-DALL-E-commit_activity . collect_stats:openai-DALL-E-participation . collect_stats:openai-DALL-E-releases . collect_stats:tweepy-tweepy-code_frequency . collect_stats:tweepy-tweepy-commit_activity . collect_stats:tweepy-tweepy-participation . collect_stats:tweepy-tweepy-releases . collect_stats:google-research-football-code_frequency . collect_stats:google-research-football-commit_activity . collect_stats:google-research-football-participation . collect_stats:google-research-football-releases . collect_stats:pandas-dev-pandas-code_frequency . collect_stats:pandas-dev-pandas-commit_activity . collect_stats:pandas-dev-pandas-participation . collect_stats:pandas-dev-pandas-releases . collect_stats:cupy-cupy-code_frequency . collect_stats:cupy-cupy-commit_activity . collect_stats:cupy-cupy-participation . collect_stats:cupy-cupy-releases . collect_stats:explosion-spaCy-code_frequency . collect_stats:explosion-spaCy-commit_activity . collect_stats:explosion-spaCy-participation . collect_stats:explosion-spaCy-releases . collect_stats:biopython-biopython-code_frequency . collect_stats:biopython-biopython-commit_activity . collect_stats:biopython-biopython-participation . collect_stats:biopython-biopython-releases . collect_stats:shengqiangzhang-examples-of-web-crawlers-code_frequency . collect_stats:shengqiangzhang-examples-of-web-crawlers-commit_activity . collect_stats:shengqiangzhang-examples-of-web-crawlers-participation . collect_stats:shengqiangzhang-examples-of-web-crawlers-releases . collect_stats:sshwsfc-xadmin-code_frequency . collect_stats:sshwsfc-xadmin-commit_activity . collect_stats:sshwsfc-xadmin-participation . collect_stats:sshwsfc-xadmin-releases . collect_stats:facebook-prophet-code_frequency . collect_stats:facebook-prophet-commit_activity . collect_stats:facebook-prophet-participation . collect_stats:facebook-prophet-releases . collect_stats:fogleman-Minecraft-code_frequency . collect_stats:fogleman-Minecraft-commit_activity . collect_stats:fogleman-Minecraft-participation . collect_stats:fogleman-Minecraft-releases . collect_stats:magenta-magenta-code_frequency . collect_stats:magenta-magenta-commit_activity . collect_stats:magenta-magenta-participation . collect_stats:magenta-magenta-releases . collect_stats:AUTOMATIC1111-stable-diffusion-webui-code_frequency . collect_stats:AUTOMATIC1111-stable-diffusion-webui-commit_activity . collect_stats:AUTOMATIC1111-stable-diffusion-webui-participation . collect_stats:AUTOMATIC1111-stable-diffusion-webui-releases . collect_stats:python-telegram-bot-python-telegram-bot-code_frequency . collect_stats:python-telegram-bot-python-telegram-bot-commit_activity . collect_stats:python-telegram-bot-python-telegram-bot-participation . collect_stats:python-telegram-bot-python-telegram-bot-releases . collect_stats:dbcli-mycli-code_frequency . collect_stats:dbcli-mycli-commit_activity . collect_stats:dbcli-mycli-participation . collect_stats:dbcli-mycli-releases . collect_stats:ageitgey-face_recognition-code_frequency . collect_stats:ageitgey-face_recognition-commit_activity . collect_stats:ageitgey-face_recognition-participation . collect_stats:ageitgey-face_recognition-releases . collect_stats:pydantic-pydantic-code_frequency . collect_stats:pydantic-pydantic-commit_activity . collect_stats:pydantic-pydantic-participation . collect_stats:pydantic-pydantic-releases . collect_stats:plotly-plotly.py-code_frequency . collect_stats:plotly-plotly.py-commit_activity . collect_stats:plotly-plotly.py-participation . collect_stats:plotly-plotly.py-releases . collect_stats:p2pool-p2pool-code_frequency . collect_stats:p2pool-p2pool-commit_activity . collect_stats:p2pool-p2pool-participation . collect_stats:p2pool-p2pool-releases . collect_stats:pypa-pipenv-code_frequency . collect_stats:pypa-pipenv-commit_activity . collect_stats:pypa-pipenv-participation . collect_stats:pypa-pipenv-releases . collect_stats:psf-requests-code_frequency . collect_stats:psf-requests-commit_activity . collect_stats:psf-requests-participation . collect_stats:psf-requests-releases . collect_stats:ipython-ipython-code_frequency . collect_stats:ipython-ipython-commit_activity . collect_stats:ipython-ipython-participation . collect_stats:ipython-ipython-releases . collect_stats:amdegroot-ssd.pytorch-code_frequency . collect_stats:amdegroot-ssd.pytorch-commit_activity . collect_stats:amdegroot-ssd.pytorch-participation . collect_stats:amdegroot-ssd.pytorch-releases . collect_stats:apachecn-ailearning-code_frequency . collect_stats:apachecn-ailearning-commit_activity . collect_stats:apachecn-ailearning-participation . collect_stats:apachecn-ailearning-releases . collect_stats:scikit-learn-scikit-learn-code_frequency . collect_stats:scikit-learn-scikit-learn-commit_activity . collect_stats:scikit-learn-scikit-learn-participation . collect_stats:scikit-learn-scikit-learn-releases . collect_stats:pytorch-vision-code_frequency . collect_stats:pytorch-vision-commit_activity . collect_stats:pytorch-vision-participation . collect_stats:pytorch-vision-releases . collect_stats:sympy-sympy-code_frequency . collect_stats:sympy-sympy-commit_activity . collect_stats:sympy-sympy-participation . collect_stats:sympy-sympy-releases . collect_stats:onnx-onnx-code_frequency . collect_stats:onnx-onnx-commit_activity . collect_stats:onnx-onnx-participation . collect_stats:onnx-onnx-releases . collect_stats:Netflix-metaflow-code_frequency . collect_stats:Netflix-metaflow-commit_activity . collect_stats:Netflix-metaflow-participation . collect_stats:Netflix-metaflow-releases . collect_stats:heartexlabs-labelImg-code_frequency . collect_stats:heartexlabs-labelImg-commit_activity . collect_stats:heartexlabs-labelImg-participation . collect_stats:heartexlabs-labelImg-releases . collect_stats:OctoPrint-OctoPrint-code_frequency . collect_stats:OctoPrint-OctoPrint-commit_activity . collect_stats:OctoPrint-OctoPrint-participation . collect_stats:OctoPrint-OctoPrint-releases . collect_stats:wkentaro-labelme-code_frequency . collect_stats:wkentaro-labelme-commit_activity . collect_stats:wkentaro-labelme-participation . collect_stats:wkentaro-labelme-releases . collect_stats:fail2ban-fail2ban-code_frequency . collect_stats:fail2ban-fail2ban-commit_activity . collect_stats:fail2ban-fail2ban-participation . collect_stats:fail2ban-fail2ban-releases . collect_stats:Rapptz-discord.py-code_frequency . collect_stats:Rapptz-discord.py-commit_activity . collect_stats:Rapptz-discord.py-participation . collect_stats:Rapptz-discord.py-releases . collect_stats:guohongze-adminset-code_frequency . collect_stats:guohongze-adminset-commit_activity . collect_stats:guohongze-adminset-participation . collect_stats:guohongze-adminset-releases . collect_stats:threat9-routersploit-code_frequency . collect_stats:threat9-routersploit-commit_activity . collect_stats:threat9-routersploit-participation . collect_stats:threat9-routersploit-releases . collect_stats:ddbourgin-numpy-ml-code_frequency . collect_stats:ddbourgin-numpy-ml-commit_activity . collect_stats:ddbourgin-numpy-ml-participation . collect_stats:ddbourgin-numpy-ml-releases . collect_stats:pallets-flask-code_frequency . collect_stats:pallets-flask-commit_activity . collect_stats:pallets-flask-participation . collect_stats:pallets-flask-releases . collect_stats:modin-project-modin-code_frequency . collect_stats:modin-project-modin-commit_activity . collect_stats:modin-project-modin-participation . collect_stats:modin-project-modin-releases . collect_stats:spesmilo-electrum-code_frequency . collect_stats:spesmilo-electrum-commit_activity . collect_stats:spesmilo-electrum-participation . collect_stats:spesmilo-electrum-releases . collect_stats:iterative-dvc-code_frequency . collect_stats:iterative-dvc-commit_activity . collect_stats:iterative-dvc-participation . collect_stats:iterative-dvc-releases . collect_stats:tgalal-yowsup-code_frequency . collect_stats:tgalal-yowsup-commit_activity . collect_stats:tgalal-yowsup-participation . collect_stats:tgalal-yowsup-releases . collect_stats:Azure-azure-cli-code_frequency . collect_stats:Azure-azure-cli-commit_activity . collect_stats:Azure-azure-cli-participation . collect_stats:Azure-azure-cli-releases . collect_stats:shadowsocksr-backup-shadowsocksr-code_frequency . collect_stats:shadowsocksr-backup-shadowsocksr-commit_activity . collect_stats:shadowsocksr-backup-shadowsocksr-participation . collect_stats:shadowsocksr-backup-shadowsocksr-releases . collect_stats:ranger-ranger-code_frequency . collect_stats:ranger-ranger-commit_activity . collect_stats:ranger-ranger-participation . collect_stats:ranger-ranger-releases . collect_stats:localstack-localstack-code_frequency . collect_stats:localstack-localstack-commit_activity . collect_stats:localstack-localstack-participation . collect_stats:localstack-localstack-releases . collect_stats:MatrixTM-MHDDoS-code_frequency . collect_stats:MatrixTM-MHDDoS-commit_activity . collect_stats:MatrixTM-MHDDoS-participation . collect_stats:MatrixTM-MHDDoS-releases . collect_stats:soimort-you-get-code_frequency . collect_stats:soimort-you-get-commit_activity . collect_stats:soimort-you-get-participation . collect_stats:soimort-you-get-releases . collect_stats:Gameye98-Lazymux-code_frequency . collect_stats:Gameye98-Lazymux-commit_activity . collect_stats:Gameye98-Lazymux-participation . collect_stats:Gameye98-Lazymux-releases . collect_stats:allenai-allennlp-code_frequency . collect_stats:allenai-allennlp-commit_activity . collect_stats:allenai-allennlp-participation . collect_stats:allenai-allennlp-releases . collect_stats:CorentinJ-Real-Time-Voice-Cloning-code_frequency . collect_stats:CorentinJ-Real-Time-Voice-Cloning-commit_activity . collect_stats:CorentinJ-Real-Time-Voice-Cloning-participation . collect_stats:CorentinJ-Real-Time-Voice-Cloning-releases . collect_stats:RasaHQ-rasa-code_frequency . collect_stats:RasaHQ-rasa-commit_activity . collect_stats:RasaHQ-rasa-participation . collect_stats:RasaHQ-rasa-releases . collect_stats:websocket-client-websocket-client-code_frequency . collect_stats:websocket-client-websocket-client-commit_activity . collect_stats:websocket-client-websocket-client-participation . collect_stats:websocket-client-websocket-client-releases . collect_stats:scrapy-scrapy-code_frequency . collect_stats:scrapy-scrapy-commit_activity . collect_stats:scrapy-scrapy-participation . collect_stats:scrapy-scrapy-releases . collect_stats:eriklindernoren-PyTorch-YOLOv3-code_frequency . collect_stats:eriklindernoren-PyTorch-YOLOv3-commit_activity . collect_stats:eriklindernoren-PyTorch-YOLOv3-participation . collect_stats:eriklindernoren-PyTorch-YOLOv3-releases . collect_stats:trustedsec-social-engineer-toolkit-code_frequency . collect_stats:trustedsec-social-engineer-toolkit-commit_activity . collect_stats:trustedsec-social-engineer-toolkit-participation . collect_stats:trustedsec-social-engineer-toolkit-releases . collect_stats:zalando-patroni-code_frequency . collect_stats:zalando-patroni-commit_activity . collect_stats:zalando-patroni-participation . collect_stats:zalando-patroni-releases . collect_stats:aboul3la-Sublist3r-code_frequency . collect_stats:aboul3la-Sublist3r-commit_activity . collect_stats:aboul3la-Sublist3r-participation . collect_stats:aboul3la-Sublist3r-releases . collect_stats:lra-mackup-code_frequency . collect_stats:lra-mackup-commit_activity . collect_stats:lra-mackup-participation . collect_stats:lra-mackup-releases . collect_stats:MrS0m30n3-youtube-dl-gui-code_frequency . collect_stats:MrS0m30n3-youtube-dl-gui-commit_activity . collect_stats:MrS0m30n3-youtube-dl-gui-participation . collect_stats:MrS0m30n3-youtube-dl-gui-releases . collect_stats:Bitwise-01-Instagram--code_frequency . collect_stats:Bitwise-01-Instagram--commit_activity . collect_stats:Bitwise-01-Instagram--participation . collect_stats:Bitwise-01-Instagram--releases . collect_stats:bokeh-bokeh-code_frequency . collect_stats:bokeh-bokeh-commit_activity . collect_stats:bokeh-bokeh-participation . collect_stats:bokeh-bokeh-releases . collect_stats:quantopian-zipline-code_frequency . collect_stats:quantopian-zipline-commit_activity . collect_stats:quantopian-zipline-participation . collect_stats:quantopian-zipline-releases . collect_stats:jupyter-jupyter-code_frequency . collect_stats:jupyter-jupyter-commit_activity . collect_stats:jupyter-jupyter-participation . collect_stats:jupyter-jupyter-releases . collect_stats:dbcli-pgcli-code_frequency . collect_stats:dbcli-pgcli-commit_activity . collect_stats:dbcli-pgcli-participation . collect_stats:dbcli-pgcli-releases . collect_stats:aws-aws-cli-code_frequency . collect_stats:aws-aws-cli-commit_activity . collect_stats:aws-aws-cli-participation . collect_stats:aws-aws-cli-releases . collect_stats:ocrmypdf-OCRmyPDF-code_frequency . collect_stats:ocrmypdf-OCRmyPDF-commit_activity . collect_stats:ocrmypdf-OCRmyPDF-participation . collect_stats:ocrmypdf-OCRmyPDF-releases . collect_stats:apache-airflow-code_frequency . collect_stats:apache-airflow-commit_activity . collect_stats:apache-airflow-participation . collect_stats:apache-airflow-releases . collect_stats:encode-uvicorn-code_frequency . collect_stats:encode-uvicorn-commit_activity . collect_stats:encode-uvicorn-participation . collect_stats:encode-uvicorn-releases . collect_stats:mwaskom-seaborn-code_frequency . collect_stats:mwaskom-seaborn-commit_activity . collect_stats:mwaskom-seaborn-participation . collect_stats:mwaskom-seaborn-releases . collect_stats:NVIDIA-apex-code_frequency . collect_stats:NVIDIA-apex-commit_activity . collect_stats:NVIDIA-apex-participation . collect_stats:NVIDIA-apex-releases . collect_stats:numenta-nupic-code_frequency . collect_stats:numenta-nupic-commit_activity . collect_stats:numenta-nupic-participation . collect_stats:numenta-nupic-releases . collect_stats:wiseodd-generative-models-code_frequency . collect_stats:wiseodd-generative-models-commit_activity . collect_stats:wiseodd-generative-models-participation . collect_stats:wiseodd-generative-models-releases . collect_stats:TeamUltroid-Ultroid-code_frequency . collect_stats:TeamUltroid-Ultroid-commit_activity . collect_stats:TeamUltroid-Ultroid-participation . collect_stats:TeamUltroid-Ultroid-releases . collect_stats:deeppavlov-DeepPavlov-code_frequency . collect_stats:deeppavlov-DeepPavlov-commit_activity . collect_stats:deeppavlov-DeepPavlov-participation . collect_stats:deeppavlov-DeepPavlov-releases . collect_stats:openai-baselines-code_frequency . collect_stats:openai-baselines-commit_activity . collect_stats:openai-baselines-participation . collect_stats:openai-baselines-releases . collect_stats:lyst-lightfm-code_frequency . collect_stats:lyst-lightfm-commit_activity . collect_stats:lyst-lightfm-participation . collect_stats:lyst-lightfm-releases . collect_stats:deezer-spleeter-code_frequency . collect_stats:deezer-spleeter-commit_activity . collect_stats:deezer-spleeter-participation . collect_stats:deezer-spleeter-releases . collect_stats:babysor-MockingBird-code_frequency . collect_stats:babysor-MockingBird-commit_activity . collect_stats:babysor-MockingBird-participation . collect_stats:babysor-MockingBird-releases . collect_stats:tensorlayer-TensorLayer-code_frequency . collect_stats:tensorlayer-TensorLayer-commit_activity . collect_stats:tensorlayer-TensorLayer-participation . collect_stats:tensorlayer-TensorLayer-releases . collect_stats:flask-admin-flask-admin-code_frequency . collect_stats:flask-admin-flask-admin-commit_activity . collect_stats:flask-admin-flask-admin-participation . collect_stats:flask-admin-flask-admin-releases . collect_stats:codelucas-newspaper-code_frequency . collect_stats:codelucas-newspaper-commit_activity . collect_stats:codelucas-newspaper-participation . collect_stats:codelucas-newspaper-releases . collect_stats:dmlc-gluon-nlp-code_frequency . collect_stats:dmlc-gluon-nlp-commit_activity . collect_stats:dmlc-gluon-nlp-participation . collect_stats:dmlc-gluon-nlp-releases . collect_stats:python-poetry-poetry-code_frequency . collect_stats:python-poetry-poetry-commit_activity . collect_stats:python-poetry-poetry-participation . collect_stats:python-poetry-poetry-releases . collect_stats:darknessomi-musicbox-code_frequency . collect_stats:darknessomi-musicbox-commit_activity . collect_stats:darknessomi-musicbox-participation . collect_stats:darknessomi-musicbox-releases . collect_stats:pyro-ppl-pyro-code_frequency . collect_stats:pyro-ppl-pyro-commit_activity . collect_stats:pyro-ppl-pyro-participation . collect_stats:pyro-ppl-pyro-releases . collect_stats:alexjc-neural-enhance-code_frequency . collect_stats:alexjc-neural-enhance-commit_activity . collect_stats:alexjc-neural-enhance-participation . collect_stats:alexjc-neural-enhance-releases . collect_stats:QuantEcon-QuantEcon.py-code_frequency . collect_stats:QuantEcon-QuantEcon.py-commit_activity . collect_stats:QuantEcon-QuantEcon.py-participation . collect_stats:QuantEcon-QuantEcon.py-releases . collect_stats:Gallopsled-pwntools-code_frequency . collect_stats:Gallopsled-pwntools-commit_activity . collect_stats:Gallopsled-pwntools-participation . collect_stats:Gallopsled-pwntools-releases . collect_stats:amueller-word_cloud-code_frequency . collect_stats:amueller-word_cloud-commit_activity . collect_stats:amueller-word_cloud-participation . collect_stats:amueller-word_cloud-releases . collect_stats:rasbt-mlxtend-code_frequency . collect_stats:rasbt-mlxtend-commit_activity . collect_stats:rasbt-mlxtend-participation . collect_stats:rasbt-mlxtend-releases . collect_stats:google-python-fire-code_frequency . collect_stats:google-python-fire-commit_activity . collect_stats:google-python-fire-participation . collect_stats:google-python-fire-releases . collect_stats:1adrianb-face-alignment-code_frequency . collect_stats:1adrianb-face-alignment-commit_activity . collect_stats:1adrianb-face-alignment-participation . collect_stats:1adrianb-face-alignment-releases . collect_stats:django-django-code_frequency . collect_stats:django-django-commit_activity . collect_stats:django-django-participation . collect_stats:django-django-releases . collect_stats:elebumm-RedditVideoMakerBot-code_frequency . collect_stats:elebumm-RedditVideoMakerBot-commit_activity . collect_stats:elebumm-RedditVideoMakerBot-participation . collect_stats:elebumm-RedditVideoMakerBot-releases . collect_stats:Jack-Cherish-python-spider-code_frequency . collect_stats:Jack-Cherish-python-spider-commit_activity . collect_stats:Jack-Cherish-python-spider-participation . collect_stats:Jack-Cherish-python-spider-releases . collect_stats:OpenBB-finance-OpenBBTerminal-code_frequency . collect_stats:OpenBB-finance-OpenBBTerminal-commit_activity . collect_stats:OpenBB-finance-OpenBBTerminal-participation . collect_stats:OpenBB-finance-OpenBBTerminal-releases . collect_stats:networkx-networkx-code_frequency . collect_stats:networkx-networkx-commit_activity . collect_stats:networkx-networkx-participation . collect_stats:networkx-networkx-releases . collect_stats:openshift-openshift-ansible-code_frequency . collect_stats:openshift-openshift-ansible-commit_activity . collect_stats:openshift-openshift-ansible-participation . collect_stats:openshift-openshift-ansible-releases . collect_stats:heartexlabs-label-studio-code_frequency . collect_stats:heartexlabs-label-studio-commit_activity . collect_stats:heartexlabs-label-studio-participation . collect_stats:heartexlabs-label-studio-releases . collect_stats:CTFd-CTFd-code_frequency . collect_stats:CTFd-CTFd-commit_activity . collect_stats:CTFd-CTFd-participation . collect_stats:CTFd-CTFd-releases . collect_stats:nvbn-thefuck-code_frequency . collect_stats:nvbn-thefuck-commit_activity . collect_stats:nvbn-thefuck-participation . collect_stats:nvbn-thefuck-releases . collect_stats:encode-django-rest-framework-code_frequency . collect_stats:encode-django-rest-framework-commit_activity . collect_stats:encode-django-rest-framework-participation . collect_stats:encode-django-rest-framework-releases . collect_stats:tflearn-tflearn-code_frequency . collect_stats:tflearn-tflearn-commit_activity . collect_stats:tflearn-tflearn-participation . collect_stats:tflearn-tflearn-releases . collect_stats:Theano-Theano-code_frequency . collect_stats:Theano-Theano-commit_activity . collect_stats:Theano-Theano-participation . collect_stats:Theano-Theano-releases . collect_stats:TheSpeedX-TBomb-code_frequency . collect_stats:TheSpeedX-TBomb-commit_activity . collect_stats:TheSpeedX-TBomb-participation . collect_stats:TheSpeedX-TBomb-releases . collect_stats:hyperopt-hyperopt-code_frequency . collect_stats:hyperopt-hyperopt-commit_activity . collect_stats:hyperopt-hyperopt-participation . collect_stats:hyperopt-hyperopt-releases . collect_stats:donnemartin-dev-setup-code_frequency . collect_stats:donnemartin-dev-setup-commit_activity . collect_stats:donnemartin-dev-setup-participation . collect_stats:donnemartin-dev-setup-releases . collect_stats:microsoft-qlib-code_frequency . collect_stats:microsoft-qlib-commit_activity . collect_stats:microsoft-qlib-participation . collect_stats:microsoft-qlib-releases . collect_stats:googleapis-google-api-python-client-code_frequency . collect_stats:googleapis-google-api-python-client-commit_activity . collect_stats:googleapis-google-api-python-client-participation . collect_stats:googleapis-google-api-python-client-releases . collect_stats:horovod-horovod-code_frequency . collect_stats:horovod-horovod-commit_activity . collect_stats:horovod-horovod-participation . collect_stats:horovod-horovod-releases . collect_stats:scikit-image-scikit-image-code_frequency . collect_stats:scikit-image-scikit-image-commit_activity . collect_stats:scikit-image-scikit-image-participation . collect_stats:scikit-image-scikit-image-releases . collect_stats:git-cola-git-cola-code_frequency . collect_stats:git-cola-git-cola-commit_activity . collect_stats:git-cola-git-cola-participation . collect_stats:git-cola-git-cola-releases . collect_stats:nicolargo-glances-code_frequency . collect_stats:nicolargo-glances-commit_activity . collect_stats:nicolargo-glances-participation . collect_stats:nicolargo-glances-releases . collect_stats:deepmind-pysc2-code_frequency . collect_stats:deepmind-pysc2-commit_activity . collect_stats:deepmind-pysc2-participation . collect_stats:deepmind-pysc2-releases . collect_stats:librosa-librosa-code_frequency . collect_stats:librosa-librosa-commit_activity . collect_stats:librosa-librosa-participation . collect_stats:librosa-librosa-releases . collect_stats:PyMySQL-mysqlclient-code_frequency . collect_stats:PyMySQL-mysqlclient-commit_activity . collect_stats:PyMySQL-mysqlclient-participation . collect_stats:PyMySQL-mysqlclient-releases . collect_stats:trustedsec-ptf-code_frequency . collect_stats:trustedsec-ptf-commit_activity . collect_stats:trustedsec-ptf-participation . collect_stats:trustedsec-ptf-releases . collect_stats:facebookresearch-fairseq-code_frequency . collect_stats:facebookresearch-fairseq-commit_activity . collect_stats:facebookresearch-fairseq-participation . collect_stats:facebookresearch-fairseq-releases . collect_stats:mingrammer-diagrams-code_frequency . collect_stats:mingrammer-diagrams-commit_activity . collect_stats:mingrammer-diagrams-participation . collect_stats:mingrammer-diagrams-releases . collect_stats:mkleehammer-pyodbc-code_frequency . collect_stats:mkleehammer-pyodbc-commit_activity . collect_stats:mkleehammer-pyodbc-participation . collect_stats:mkleehammer-pyodbc-releases . collect_stats:aaPanel-BaoTa-code_frequency . collect_stats:aaPanel-BaoTa-commit_activity . collect_stats:aaPanel-BaoTa-participation . collect_stats:aaPanel-BaoTa-releases . collect_stats:mlflow-mlflow-code_frequency . collect_stats:mlflow-mlflow-commit_activity . collect_stats:mlflow-mlflow-participation . collect_stats:mlflow-mlflow-releases . collect_stats:deepchem-deepchem-code_frequency . collect_stats:deepchem-deepchem-commit_activity . collect_stats:deepchem-deepchem-participation . collect_stats:deepchem-deepchem-releases . collect_stats:frappe-bench-code_frequency . collect_stats:frappe-bench-commit_activity . collect_stats:frappe-bench-participation . collect_stats:frappe-bench-releases . collect_stats:matplotlib-matplotlib-code_frequency . collect_stats:matplotlib-matplotlib-commit_activity . collect_stats:matplotlib-matplotlib-participation . collect_stats:matplotlib-matplotlib-releases . collect_stats:pydata-pandas-datareader-code_frequency . collect_stats:pydata-pandas-datareader-commit_activity . collect_stats:pydata-pandas-datareader-participation . collect_stats:pydata-pandas-datareader-releases . collect_stats:TencentARC-GFPGAN-code_frequency . collect_stats:TencentARC-GFPGAN-commit_activity . collect_stats:TencentARC-GFPGAN-participation . collect_stats:TencentARC-GFPGAN-releases . collect_stats:ThoughtfulDev-EagleEye-code_frequency . collect_stats:ThoughtfulDev-EagleEye-commit_activity . collect_stats:ThoughtfulDev-EagleEye-participation . collect_stats:ThoughtfulDev-EagleEye-releases . collect_stats:Uberi-speech_recognition-code_frequency . collect_stats:Uberi-speech_recognition-commit_activity . collect_stats:Uberi-speech_recognition-participation . collect_stats:Uberi-speech_recognition-releases . collect_stats:scikit-learn-contrib-imbalanced-learn-code_frequency . collect_stats:scikit-learn-contrib-imbalanced-learn-commit_activity . collect_stats:scikit-learn-contrib-imbalanced-learn-participation . collect_stats:scikit-learn-contrib-imbalanced-learn-releases . collect_stats:Lightning-AI-lightning-code_frequency . collect_stats:Lightning-AI-lightning-commit_activity . collect_stats:Lightning-AI-lightning-participation . collect_stats:Lightning-AI-lightning-releases . collect_stats:strawlab-python-pcl-code_frequency . collect_stats:strawlab-python-pcl-commit_activity . collect_stats:strawlab-python-pcl-participation . collect_stats:strawlab-python-pcl-releases . collect_stats:jopohl-urh-code_frequency . collect_stats:jopohl-urh-commit_activity . collect_stats:jopohl-urh-participation . collect_stats:jopohl-urh-releases . collect_stats:GreaterWMS-GreaterWMS-code_frequency . collect_stats:GreaterWMS-GreaterWMS-commit_activity . collect_stats:GreaterWMS-GreaterWMS-participation . collect_stats:GreaterWMS-GreaterWMS-releases . collect_stats:LionSec-katoolin-code_frequency . collect_stats:LionSec-katoolin-commit_activity . collect_stats:LionSec-katoolin-participation . collect_stats:LionSec-katoolin-releases . collect_stats:ManimCommunity-manim-code_frequency . collect_stats:ManimCommunity-manim-commit_activity . collect_stats:ManimCommunity-manim-participation . collect_stats:ManimCommunity-manim-releases . collect_stats:dmlc-gluon-cv-code_frequency . collect_stats:dmlc-gluon-cv-commit_activity . collect_stats:dmlc-gluon-cv-participation . collect_stats:dmlc-gluon-cv-releases . collect_stats:gorakhargosh-watchdog-code_frequency . collect_stats:gorakhargosh-watchdog-commit_activity . collect_stats:gorakhargosh-watchdog-participation . collect_stats:gorakhargosh-watchdog-releases . collect_stats:biolab-orange3-code_frequency . collect_stats:biolab-orange3-commit_activity . collect_stats:biolab-orange3-participation . collect_stats:biolab-orange3-releases . collect_stats:NullArray-AutoSploit-code_frequency . collect_stats:NullArray-AutoSploit-commit_activity . collect_stats:NullArray-AutoSploit-participation . collect_stats:NullArray-AutoSploit-releases . collect_stats:webpy-webpy-code_frequency . collect_stats:webpy-webpy-commit_activity . collect_stats:webpy-webpy-participation . collect_stats:webpy-webpy-releases . collect_stats:sherlock-project-sherlock-code_frequency . collect_stats:sherlock-project-sherlock-commit_activity . collect_stats:sherlock-project-sherlock-participation . collect_stats:sherlock-project-sherlock-releases . collect_stats:celery-celery-code_frequency . collect_stats:celery-celery-commit_activity . collect_stats:celery-celery-participation . collect_stats:celery-celery-releases . collect_stats:xinntao-Real-ESRGAN-code_frequency . collect_stats:xinntao-Real-ESRGAN-commit_activity . collect_stats:xinntao-Real-ESRGAN-participation . collect_stats:xinntao-Real-ESRGAN-releases . collect_stats:drivendata-cookiecutter-data-science-code_frequency . collect_stats:drivendata-cookiecutter-data-science-commit_activity . collect_stats:drivendata-cookiecutter-data-science-participation . collect_stats:drivendata-cookiecutter-data-science-releases . collect_stats:pgmpy-pgmpy-code_frequency . collect_stats:pgmpy-pgmpy-commit_activity . collect_stats:pgmpy-pgmpy-participation . collect_stats:pgmpy-pgmpy-releases . collect_stats:Chia-Network-chia-blockchain-code_frequency . collect_stats:Chia-Network-chia-blockchain-commit_activity . collect_stats:Chia-Network-chia-blockchain-participation . collect_stats:Chia-Network-chia-blockchain-releases . collect_stats:thunil-TecoGAN-code_frequency . collect_stats:thunil-TecoGAN-commit_activity . collect_stats:thunil-TecoGAN-participation . collect_stats:thunil-TecoGAN-releases . collect_stats:CouchPotato-CouchPotatoServer-code_frequency . collect_stats:CouchPotato-CouchPotatoServer-commit_activity . collect_stats:CouchPotato-CouchPotatoServer-participation . collect_stats:CouchPotato-CouchPotatoServer-releases . collect_stats:boto-boto3-code_frequency . collect_stats:boto-boto3-commit_activity . collect_stats:boto-boto3-participation . collect_stats:boto-boto3-releases . collect_stats:RaRe-Technologies-gensim-code_frequency . collect_stats:RaRe-Technologies-gensim-commit_activity . collect_stats:RaRe-Technologies-gensim-participation . collect_stats:RaRe-Technologies-gensim-releases . collect_stats:CharlesPikachu-Games-code_frequency . collect_stats:CharlesPikachu-Games-commit_activity . collect_stats:CharlesPikachu-Games-participation . collect_stats:CharlesPikachu-Games-releases . collect_stats:pytorch-tutorials-code_frequency . collect_stats:pytorch-tutorials-commit_activity . collect_stats:pytorch-tutorials-participation . collect_stats:pytorch-tutorials-releases . collect_stats:PyMySQL-PyMySQL-code_frequency . collect_stats:PyMySQL-PyMySQL-commit_activity . collect_stats:PyMySQL-PyMySQL-participation . collect_stats:PyMySQL-PyMySQL-releases . collect_stats:google-jax-code_frequency . collect_stats:google-jax-commit_activity . collect_stats:google-jax-participation . collect_stats:google-jax-releases . collect_stats:asweigart-pyautogui-code_frequency . collect_stats:asweigart-pyautogui-commit_activity . collect_stats:asweigart-pyautogui-participation . collect_stats:asweigart-pyautogui-releases . collect_stats:Hironsan-BossSensor-code_frequency . collect_stats:Hironsan-BossSensor-commit_activity . collect_stats:Hironsan-BossSensor-participation . collect_stats:Hironsan-BossSensor-releases . collect_stats:tensorflow-agents-code_frequency . collect_stats:tensorflow-agents-commit_activity . collect_stats:tensorflow-agents-participation . collect_stats:tensorflow-agents-releases . collect_stats:Zulko-moviepy-code_frequency . collect_stats:Zulko-moviepy-commit_activity . collect_stats:Zulko-moviepy-participation . collect_stats:Zulko-moviepy-releases . collect_stats:wting-autojump-code_frequency . collect_stats:wting-autojump-commit_activity . collect_stats:wting-autojump-participation . collect_stats:wting-autojump-releases . collect_stats:python-mypy-code_frequency . collect_stats:python-mypy-commit_activity . collect_stats:python-mypy-participation . collect_stats:python-mypy-releases . collect_stats:docker-docker-py-code_frequency . collect_stats:docker-docker-py-commit_activity . collect_stats:docker-docker-py-participation . collect_stats:docker-docker-py-releases . collect_stats:conda-conda-code_frequency . collect_stats:conda-conda-commit_activity . collect_stats:conda-conda-participation . collect_stats:conda-conda-releases . collect_stats:aristocratos-bpytop-code_frequency . collect_stats:aristocratos-bpytop-commit_activity . collect_stats:aristocratos-bpytop-participation . collect_stats:aristocratos-bpytop-releases . collect_stats:pypa-pip-code_frequency . collect_stats:pypa-pip-commit_activity . collect_stats:pypa-pip-participation . collect_stats:pypa-pip-releases . collect_stats:InstaPy-InstaPy-code_frequency . collect_stats:InstaPy-InstaPy-commit_activity . collect_stats:InstaPy-InstaPy-participation . collect_stats:InstaPy-InstaPy-releases . collect_stats:jupyterhub-jupyterhub-code_frequency . collect_stats:jupyterhub-jupyterhub-commit_activity . collect_stats:jupyterhub-jupyterhub-participation . collect_stats:jupyterhub-jupyterhub-releases . collect_stats:my8100-scrapydweb-code_frequency . collect_stats:my8100-scrapydweb-commit_activity . collect_stats:my8100-scrapydweb-participation . collect_stats:my8100-scrapydweb-releases . collect_stats:coursera-dl-coursera-dl-code_frequency . collect_stats:coursera-dl-coursera-dl-commit_activity . collect_stats:coursera-dl-coursera-dl-participation . collect_stats:coursera-dl-coursera-dl-releases . collect_stats:zeromq-pyzmq-code_frequency . collect_stats:zeromq-pyzmq-commit_activity . collect_stats:zeromq-pyzmq-participation . collect_stats:zeromq-pyzmq-releases . collect_stats:frappe-erpnext-code_frequency . collect_stats:frappe-erpnext-commit_activity . collect_stats:frappe-erpnext-participation . collect_stats:frappe-erpnext-releases . collect_stats:microsoft-pyright-code_frequency . collect_stats:microsoft-pyright-commit_activity . collect_stats:microsoft-pyright-participation . collect_stats:microsoft-pyright-releases . collect_stats:sightmachine-SimpleCV-code_frequency . collect_stats:sightmachine-SimpleCV-commit_activity . collect_stats:sightmachine-SimpleCV-participation . collect_stats:sightmachine-SimpleCV-releases . collect_stats:deepmind-graph_nets-code_frequency . collect_stats:deepmind-graph_nets-commit_activity . collect_stats:deepmind-graph_nets-participation . collect_stats:deepmind-graph_nets-releases . collect_stats:sshuttle-sshuttle-code_frequency . collect_stats:sshuttle-sshuttle-commit_activity . collect_stats:sshuttle-sshuttle-participation . collect_stats:sshuttle-sshuttle-releases . collect_stats:joestump-python-oauth2-code_frequency . collect_stats:joestump-python-oauth2-commit_activity . collect_stats:joestump-python-oauth2-participation . collect_stats:joestump-python-oauth2-releases . collect_stats:ycm-core-YouCompleteMe-code_frequency . collect_stats:ycm-core-YouCompleteMe-commit_activity . collect_stats:ycm-core-YouCompleteMe-participation . collect_stats:ycm-core-YouCompleteMe-releases . collect_stats:twisted-twisted-code_frequency . collect_stats:twisted-twisted-commit_activity . collect_stats:twisted-twisted-participation . collect_stats:twisted-twisted-releases . collect_stats:SecureAuthCorp-impacket-code_frequency . collect_stats:SecureAuthCorp-impacket-commit_activity . collect_stats:SecureAuthCorp-impacket-participation . collect_stats:SecureAuthCorp-impacket-releases . collect_stats:encode-starlette-code_frequency . collect_stats:encode-starlette-commit_activity . collect_stats:encode-starlette-participation . collect_stats:encode-starlette-releases . collect_stats:nodejs-node-gyp-code_frequency . collect_stats:nodejs-node-gyp-commit_activity . collect_stats:nodejs-node-gyp-participation . collect_stats:nodejs-node-gyp-releases . collect_stats:pyserial-pyserial-code_frequency . collect_stats:pyserial-pyserial-commit_activity . collect_stats:pyserial-pyserial-participation . collect_stats:pyserial-pyserial-releases . collect_stats:keras-rl-keras-rl-code_frequency . collect_stats:keras-rl-keras-rl-commit_activity . collect_stats:keras-rl-keras-rl-participation . collect_stats:keras-rl-keras-rl-releases . collect_stats:onionshare-onionshare-code_frequency . collect_stats:onionshare-onionshare-commit_activity . collect_stats:onionshare-onionshare-participation . collect_stats:onionshare-onionshare-releases . collect_stats:mesonbuild-meson-code_frequency . collect_stats:mesonbuild-meson-commit_activity . collect_stats:mesonbuild-meson-participation . collect_stats:mesonbuild-meson-releases . collect_stats:MTG-sms-tools-code_frequency . collect_stats:MTG-sms-tools-commit_activity . collect_stats:MTG-sms-tools-participation . collect_stats:MTG-sms-tools-releases . collect_stats:mps-youtube-mps-youtube-code_frequency . collect_stats:mps-youtube-mps-youtube-commit_activity . collect_stats:mps-youtube-mps-youtube-participation . collect_stats:mps-youtube-mps-youtube-releases . collect_stats:MongoEngine-mongoengine-code_frequency . collect_stats:MongoEngine-mongoengine-commit_activity . collect_stats:MongoEngine-mongoengine-participation . collect_stats:MongoEngine-mongoengine-releases . collect_stats:deepset-ai-haystack-code_frequency . collect_stats:deepset-ai-haystack-commit_activity . collect_stats:deepset-ai-haystack-participation . collect_stats:deepset-ai-haystack-releases . collect_stats:sourabhv-FlapPyBird-code_frequency . collect_stats:sourabhv-FlapPyBird-commit_activity . collect_stats:sourabhv-FlapPyBird-participation . collect_stats:sourabhv-FlapPyBird-releases . collect_stats:open-mmlab-mmcv-code_frequency . collect_stats:open-mmlab-mmcv-commit_activity . collect_stats:open-mmlab-mmcv-participation . collect_stats:open-mmlab-mmcv-releases . collect_stats:faucetsdn-ryu-code_frequency . collect_stats:faucetsdn-ryu-commit_activity . collect_stats:faucetsdn-ryu-participation . collect_stats:faucetsdn-ryu-releases explore the data in dataframes \u00a4 import pandas df = pandas . DataFrame ( get_iterables (), columns = \"url project repo\" . split ()) . set_index ( \"url\" ) df = df . set_index ( df . index . str . rpartition ( \"/\" ) . get_level_values ( - 1 ) . rename ( \"stat\" ), append = True ) df = df . reorder_levels ([ 1 , 0 ], axis = 0 ) releases = df . loc [ \"releases\" ] . index . to_series () . map ( get_stats ) . apply ( pandas . Series ) . stack () . apply ( pandas . Series ) /tmp/ipykernel_70909/139248023.py:1: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning. releases =df.loc[\"releases\"].index.to_series().map(get_stats).apply(pandas.Series).stack().apply(pandas.Series) releases [[ \"tag_name\" , \"published_at\" ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tag_name published_at url https://api.github.com/repos/tensorflow/models/releases 0 v2.11.0 2022-11-23T00:09:34Z 1 v2.10.1 2022-11-17T05:15:35Z 2 v2.10.0 2022-09-19T21:45:22Z 3 v2.7.2 2022-07-08T06:48:39Z 4 v2.9.2 2022-05-20T04:55:27Z ... ... ... ... https://api.github.com/repos/open-mmlab/mmcv/releases 25 v1.3.13 2021-09-10T03:43:36Z 26 v1.3.12 2021-08-24T14:17:49Z 27 v1.3.11 2021-08-12T09:20:52Z 28 v1.3.10 2021-07-24T12:40:10Z 29 v1.3.9 2021-07-10T03:07:15Z 2808 rows \u00d7 2 columns freq = df . loc [ \"code_frequency\" ] . index . to_series () . map ( get_stats ) . apply ( pandas . Series ) . stack () . reset_index ( - 1 , drop = True ) . apply ( pandas . Series ) freq . columns = list ( \"w+-\" ) freq . w = freq . w . pipe ( pandas . to_datetime , unit = \"s\" ) freq = freq . set_index ( freq . index . map ( operator . itemgetter ( slice ( len ( GH ) + 7 , - len ( \"/stats/code_frequency\" ))))) import hvplot.pandas .bk-root, .bk-root .bk:before, .bk-root .bk:after { font-family: var(--jp-ui-font-size1); font-size: var(--jp-ui-font-size1); color: var(--jp-ui-font-color1); } b = freq [ list ( \"+-\" )] . abs () . sum ( axis = 1 ) . gt ( 0 ) freq [ b ][ list ( \"+-\" )] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } + - count 5.267500e+04 5.267500e+04 mean 5.047921e+03 -4.063599e+03 std 8.565557e+04 9.036993e+04 min 0.000000e+00 -1.494524e+07 25% 4.000000e+01 -6.400000e+02 50% 2.940000e+02 -1.100000e+02 75% 1.367500e+03 -1.300000e+01 max 1.461846e+07 0.000000e+00","title":"extracting info from the github statistics api"},{"location":"xxii/2022-12-13-github-api-stats.html#extracting-info-from-the-github-statistics-api","text":"i've wanted to take looks at github repositories over time. what kind of trends are there revealed over the years of projects? what can we infer from past actions that might predicate success? this work is an extension of the pyproject.toml analysis . we use the same list of respositories to generate their statistics. doit is likely a suboptimal choice for this, so we'll probably want to try dask later. with __import__ ( \"importnb\" ) . Notebook (): import __12_09_pyproject_analysis as prior import pandas df = prior . tidy_responses ( responses := prior . gather ( prior . pyproject_query )) . apply ( pandas . Series ) projects = df . url . str . rsplit ( \"/\" , 2 , expand = True )[[ 1 , 2 ]] . apply ( \"/\" . join , axis = 1 ) from info import header import requests , requests_cache , uritemplate , toolz , operator , doit requests_cache . install_cache () from info import header the endpoints we retrieve the stats from GH = \"https://api.github.com\" STATS = \"code_frequency commit_activity participation\" . split () URI = uritemplate . URITemplate ( GH + \"/repos/ {owner} / {repo} /stats/ {stat} \" ) RELEASES = uritemplate . URITemplate ( GH + \"/repos/ {owner} / {repo} /releases\" ) get_stats is a function that tries again when a 202 status is returned. def get_stats ( url ): response = requests . get ( url , ** header ) if response . status_code == 202 : __import__ ( \"time\" ) . sleep ( 1 ) return get_stats ( url ) return response . json () @doit . task_params ([ dict ( name = \"project\" , type = list , long = \"project\" , default = [])]) def task_collect_stats ( project = projects ): for url , owner , repo in get_iterables ( project ): * _ , stat = url . rpartition ( \"/\" ) yield dict ( name = \"-\" . join (( owner , repo , stat )), actions = [( toolz . curried . do ( get_stats ), [ url ])]) def get_iterables ( projects = projects ): for owner , repo in ( x . split ( \"/\" ) for x in projects ): for stat in STATS : yield URI . expand ( locals ()), owner , repo yield RELEASES . expand ( locals ()), owner , repo","title":"extracting info from the github statistics api"},{"location":"xxii/2022-12-13-github-api-stats.html#retrieve-the-data-with-doit","text":"% reload_ext doit projects_args = \" \" . join ( projects . apply ( \"--project %s \" . __mod__ )) % doit run collect_stats $ projects_args . collect_stats:tensorflow-models-code_frequency . collect_stats:tensorflow-models-commit_activity . collect_stats:tensorflow-models-participation . collect_stats:tensorflow-models-releases . collect_stats:mongodb-mongo-python-driver-code_frequency . collect_stats:mongodb-mongo-python-driver-commit_activity . collect_stats:mongodb-mongo-python-driver-participation . collect_stats:mongodb-mongo-python-driver-releases . collect_stats:3b1b-manim-code_frequency . collect_stats:3b1b-manim-commit_activity . collect_stats:3b1b-manim-participation . collect_stats:3b1b-manim-releases . collect_stats:alievk-avatarify-python-code_frequency . collect_stats:alievk-avatarify-python-commit_activity . collect_stats:alievk-avatarify-python-participation . collect_stats:alievk-avatarify-python-releases . collect_stats:python-cpython-code_frequency . collect_stats:python-cpython-commit_activity . collect_stats:python-cpython-participation . collect_stats:python-cpython-releases . collect_stats:ehmatthes-pcc-code_frequency . collect_stats:ehmatthes-pcc-commit_activity . collect_stats:ehmatthes-pcc-participation . collect_stats:ehmatthes-pcc-releases . collect_stats:openai-gym-code_frequency . collect_stats:openai-gym-commit_activity . collect_stats:openai-gym-participation . collect_stats:openai-gym-releases . collect_stats:openai-DALL-E-code_frequency . collect_stats:openai-DALL-E-commit_activity . collect_stats:openai-DALL-E-participation . collect_stats:openai-DALL-E-releases . collect_stats:tweepy-tweepy-code_frequency . collect_stats:tweepy-tweepy-commit_activity . collect_stats:tweepy-tweepy-participation . collect_stats:tweepy-tweepy-releases . collect_stats:google-research-football-code_frequency . collect_stats:google-research-football-commit_activity . collect_stats:google-research-football-participation . collect_stats:google-research-football-releases . collect_stats:pandas-dev-pandas-code_frequency . collect_stats:pandas-dev-pandas-commit_activity . collect_stats:pandas-dev-pandas-participation . collect_stats:pandas-dev-pandas-releases . collect_stats:cupy-cupy-code_frequency . collect_stats:cupy-cupy-commit_activity . collect_stats:cupy-cupy-participation . collect_stats:cupy-cupy-releases . collect_stats:explosion-spaCy-code_frequency . collect_stats:explosion-spaCy-commit_activity . collect_stats:explosion-spaCy-participation . collect_stats:explosion-spaCy-releases . collect_stats:biopython-biopython-code_frequency . collect_stats:biopython-biopython-commit_activity . collect_stats:biopython-biopython-participation . collect_stats:biopython-biopython-releases . collect_stats:shengqiangzhang-examples-of-web-crawlers-code_frequency . collect_stats:shengqiangzhang-examples-of-web-crawlers-commit_activity . collect_stats:shengqiangzhang-examples-of-web-crawlers-participation . collect_stats:shengqiangzhang-examples-of-web-crawlers-releases . collect_stats:sshwsfc-xadmin-code_frequency . collect_stats:sshwsfc-xadmin-commit_activity . collect_stats:sshwsfc-xadmin-participation . collect_stats:sshwsfc-xadmin-releases . collect_stats:facebook-prophet-code_frequency . collect_stats:facebook-prophet-commit_activity . collect_stats:facebook-prophet-participation . collect_stats:facebook-prophet-releases . collect_stats:fogleman-Minecraft-code_frequency . collect_stats:fogleman-Minecraft-commit_activity . collect_stats:fogleman-Minecraft-participation . collect_stats:fogleman-Minecraft-releases . collect_stats:magenta-magenta-code_frequency . collect_stats:magenta-magenta-commit_activity . collect_stats:magenta-magenta-participation . collect_stats:magenta-magenta-releases . collect_stats:AUTOMATIC1111-stable-diffusion-webui-code_frequency . collect_stats:AUTOMATIC1111-stable-diffusion-webui-commit_activity . collect_stats:AUTOMATIC1111-stable-diffusion-webui-participation . collect_stats:AUTOMATIC1111-stable-diffusion-webui-releases . collect_stats:python-telegram-bot-python-telegram-bot-code_frequency . collect_stats:python-telegram-bot-python-telegram-bot-commit_activity . collect_stats:python-telegram-bot-python-telegram-bot-participation . collect_stats:python-telegram-bot-python-telegram-bot-releases . collect_stats:dbcli-mycli-code_frequency . collect_stats:dbcli-mycli-commit_activity . collect_stats:dbcli-mycli-participation . collect_stats:dbcli-mycli-releases . collect_stats:ageitgey-face_recognition-code_frequency . collect_stats:ageitgey-face_recognition-commit_activity . collect_stats:ageitgey-face_recognition-participation . collect_stats:ageitgey-face_recognition-releases . collect_stats:pydantic-pydantic-code_frequency . collect_stats:pydantic-pydantic-commit_activity . collect_stats:pydantic-pydantic-participation . collect_stats:pydantic-pydantic-releases . collect_stats:plotly-plotly.py-code_frequency . collect_stats:plotly-plotly.py-commit_activity . collect_stats:plotly-plotly.py-participation . collect_stats:plotly-plotly.py-releases . collect_stats:p2pool-p2pool-code_frequency . collect_stats:p2pool-p2pool-commit_activity . collect_stats:p2pool-p2pool-participation . collect_stats:p2pool-p2pool-releases . collect_stats:pypa-pipenv-code_frequency . collect_stats:pypa-pipenv-commit_activity . collect_stats:pypa-pipenv-participation . collect_stats:pypa-pipenv-releases . collect_stats:psf-requests-code_frequency . collect_stats:psf-requests-commit_activity . collect_stats:psf-requests-participation . collect_stats:psf-requests-releases . collect_stats:ipython-ipython-code_frequency . collect_stats:ipython-ipython-commit_activity . collect_stats:ipython-ipython-participation . collect_stats:ipython-ipython-releases . collect_stats:amdegroot-ssd.pytorch-code_frequency . collect_stats:amdegroot-ssd.pytorch-commit_activity . collect_stats:amdegroot-ssd.pytorch-participation . collect_stats:amdegroot-ssd.pytorch-releases . collect_stats:apachecn-ailearning-code_frequency . collect_stats:apachecn-ailearning-commit_activity . collect_stats:apachecn-ailearning-participation . collect_stats:apachecn-ailearning-releases . collect_stats:scikit-learn-scikit-learn-code_frequency . collect_stats:scikit-learn-scikit-learn-commit_activity . collect_stats:scikit-learn-scikit-learn-participation . collect_stats:scikit-learn-scikit-learn-releases . collect_stats:pytorch-vision-code_frequency . collect_stats:pytorch-vision-commit_activity . collect_stats:pytorch-vision-participation . collect_stats:pytorch-vision-releases . collect_stats:sympy-sympy-code_frequency . collect_stats:sympy-sympy-commit_activity . collect_stats:sympy-sympy-participation . collect_stats:sympy-sympy-releases . collect_stats:onnx-onnx-code_frequency . collect_stats:onnx-onnx-commit_activity . collect_stats:onnx-onnx-participation . collect_stats:onnx-onnx-releases . collect_stats:Netflix-metaflow-code_frequency . collect_stats:Netflix-metaflow-commit_activity . collect_stats:Netflix-metaflow-participation . collect_stats:Netflix-metaflow-releases . collect_stats:heartexlabs-labelImg-code_frequency . collect_stats:heartexlabs-labelImg-commit_activity . collect_stats:heartexlabs-labelImg-participation . collect_stats:heartexlabs-labelImg-releases . collect_stats:OctoPrint-OctoPrint-code_frequency . collect_stats:OctoPrint-OctoPrint-commit_activity . collect_stats:OctoPrint-OctoPrint-participation . collect_stats:OctoPrint-OctoPrint-releases . collect_stats:wkentaro-labelme-code_frequency . collect_stats:wkentaro-labelme-commit_activity . collect_stats:wkentaro-labelme-participation . collect_stats:wkentaro-labelme-releases . collect_stats:fail2ban-fail2ban-code_frequency . collect_stats:fail2ban-fail2ban-commit_activity . collect_stats:fail2ban-fail2ban-participation . collect_stats:fail2ban-fail2ban-releases . collect_stats:Rapptz-discord.py-code_frequency . collect_stats:Rapptz-discord.py-commit_activity . collect_stats:Rapptz-discord.py-participation . collect_stats:Rapptz-discord.py-releases . collect_stats:guohongze-adminset-code_frequency . collect_stats:guohongze-adminset-commit_activity . collect_stats:guohongze-adminset-participation . collect_stats:guohongze-adminset-releases . collect_stats:threat9-routersploit-code_frequency . collect_stats:threat9-routersploit-commit_activity . collect_stats:threat9-routersploit-participation . collect_stats:threat9-routersploit-releases . collect_stats:ddbourgin-numpy-ml-code_frequency . collect_stats:ddbourgin-numpy-ml-commit_activity . collect_stats:ddbourgin-numpy-ml-participation . collect_stats:ddbourgin-numpy-ml-releases . collect_stats:pallets-flask-code_frequency . collect_stats:pallets-flask-commit_activity . collect_stats:pallets-flask-participation . collect_stats:pallets-flask-releases . collect_stats:modin-project-modin-code_frequency . collect_stats:modin-project-modin-commit_activity . collect_stats:modin-project-modin-participation . collect_stats:modin-project-modin-releases . collect_stats:spesmilo-electrum-code_frequency . collect_stats:spesmilo-electrum-commit_activity . collect_stats:spesmilo-electrum-participation . collect_stats:spesmilo-electrum-releases . collect_stats:iterative-dvc-code_frequency . collect_stats:iterative-dvc-commit_activity . collect_stats:iterative-dvc-participation . collect_stats:iterative-dvc-releases . collect_stats:tgalal-yowsup-code_frequency . collect_stats:tgalal-yowsup-commit_activity . collect_stats:tgalal-yowsup-participation . collect_stats:tgalal-yowsup-releases . collect_stats:Azure-azure-cli-code_frequency . collect_stats:Azure-azure-cli-commit_activity . collect_stats:Azure-azure-cli-participation . collect_stats:Azure-azure-cli-releases . collect_stats:shadowsocksr-backup-shadowsocksr-code_frequency . collect_stats:shadowsocksr-backup-shadowsocksr-commit_activity . collect_stats:shadowsocksr-backup-shadowsocksr-participation . collect_stats:shadowsocksr-backup-shadowsocksr-releases . collect_stats:ranger-ranger-code_frequency . collect_stats:ranger-ranger-commit_activity . collect_stats:ranger-ranger-participation . collect_stats:ranger-ranger-releases . collect_stats:localstack-localstack-code_frequency . collect_stats:localstack-localstack-commit_activity . collect_stats:localstack-localstack-participation . collect_stats:localstack-localstack-releases . collect_stats:MatrixTM-MHDDoS-code_frequency . collect_stats:MatrixTM-MHDDoS-commit_activity . collect_stats:MatrixTM-MHDDoS-participation . collect_stats:MatrixTM-MHDDoS-releases . collect_stats:soimort-you-get-code_frequency . collect_stats:soimort-you-get-commit_activity . collect_stats:soimort-you-get-participation . collect_stats:soimort-you-get-releases . collect_stats:Gameye98-Lazymux-code_frequency . collect_stats:Gameye98-Lazymux-commit_activity . collect_stats:Gameye98-Lazymux-participation . collect_stats:Gameye98-Lazymux-releases . collect_stats:allenai-allennlp-code_frequency . collect_stats:allenai-allennlp-commit_activity . collect_stats:allenai-allennlp-participation . collect_stats:allenai-allennlp-releases . collect_stats:CorentinJ-Real-Time-Voice-Cloning-code_frequency . collect_stats:CorentinJ-Real-Time-Voice-Cloning-commit_activity . collect_stats:CorentinJ-Real-Time-Voice-Cloning-participation . collect_stats:CorentinJ-Real-Time-Voice-Cloning-releases . collect_stats:RasaHQ-rasa-code_frequency . collect_stats:RasaHQ-rasa-commit_activity . collect_stats:RasaHQ-rasa-participation . collect_stats:RasaHQ-rasa-releases . collect_stats:websocket-client-websocket-client-code_frequency . collect_stats:websocket-client-websocket-client-commit_activity . collect_stats:websocket-client-websocket-client-participation . collect_stats:websocket-client-websocket-client-releases . collect_stats:scrapy-scrapy-code_frequency . collect_stats:scrapy-scrapy-commit_activity . collect_stats:scrapy-scrapy-participation . collect_stats:scrapy-scrapy-releases . collect_stats:eriklindernoren-PyTorch-YOLOv3-code_frequency . collect_stats:eriklindernoren-PyTorch-YOLOv3-commit_activity . collect_stats:eriklindernoren-PyTorch-YOLOv3-participation . collect_stats:eriklindernoren-PyTorch-YOLOv3-releases . collect_stats:trustedsec-social-engineer-toolkit-code_frequency . collect_stats:trustedsec-social-engineer-toolkit-commit_activity . collect_stats:trustedsec-social-engineer-toolkit-participation . collect_stats:trustedsec-social-engineer-toolkit-releases . collect_stats:zalando-patroni-code_frequency . collect_stats:zalando-patroni-commit_activity . collect_stats:zalando-patroni-participation . collect_stats:zalando-patroni-releases . collect_stats:aboul3la-Sublist3r-code_frequency . collect_stats:aboul3la-Sublist3r-commit_activity . collect_stats:aboul3la-Sublist3r-participation . collect_stats:aboul3la-Sublist3r-releases . collect_stats:lra-mackup-code_frequency . collect_stats:lra-mackup-commit_activity . collect_stats:lra-mackup-participation . collect_stats:lra-mackup-releases . collect_stats:MrS0m30n3-youtube-dl-gui-code_frequency . collect_stats:MrS0m30n3-youtube-dl-gui-commit_activity . collect_stats:MrS0m30n3-youtube-dl-gui-participation . collect_stats:MrS0m30n3-youtube-dl-gui-releases . collect_stats:Bitwise-01-Instagram--code_frequency . collect_stats:Bitwise-01-Instagram--commit_activity . collect_stats:Bitwise-01-Instagram--participation . collect_stats:Bitwise-01-Instagram--releases . collect_stats:bokeh-bokeh-code_frequency . collect_stats:bokeh-bokeh-commit_activity . collect_stats:bokeh-bokeh-participation . collect_stats:bokeh-bokeh-releases . collect_stats:quantopian-zipline-code_frequency . collect_stats:quantopian-zipline-commit_activity . collect_stats:quantopian-zipline-participation . collect_stats:quantopian-zipline-releases . collect_stats:jupyter-jupyter-code_frequency . collect_stats:jupyter-jupyter-commit_activity . collect_stats:jupyter-jupyter-participation . collect_stats:jupyter-jupyter-releases . collect_stats:dbcli-pgcli-code_frequency . collect_stats:dbcli-pgcli-commit_activity . collect_stats:dbcli-pgcli-participation . collect_stats:dbcli-pgcli-releases . collect_stats:aws-aws-cli-code_frequency . collect_stats:aws-aws-cli-commit_activity . collect_stats:aws-aws-cli-participation . collect_stats:aws-aws-cli-releases . collect_stats:ocrmypdf-OCRmyPDF-code_frequency . collect_stats:ocrmypdf-OCRmyPDF-commit_activity . collect_stats:ocrmypdf-OCRmyPDF-participation . collect_stats:ocrmypdf-OCRmyPDF-releases . collect_stats:apache-airflow-code_frequency . collect_stats:apache-airflow-commit_activity . collect_stats:apache-airflow-participation . collect_stats:apache-airflow-releases . collect_stats:encode-uvicorn-code_frequency . collect_stats:encode-uvicorn-commit_activity . collect_stats:encode-uvicorn-participation . collect_stats:encode-uvicorn-releases . collect_stats:mwaskom-seaborn-code_frequency . collect_stats:mwaskom-seaborn-commit_activity . collect_stats:mwaskom-seaborn-participation . collect_stats:mwaskom-seaborn-releases . collect_stats:NVIDIA-apex-code_frequency . collect_stats:NVIDIA-apex-commit_activity . collect_stats:NVIDIA-apex-participation . collect_stats:NVIDIA-apex-releases . collect_stats:numenta-nupic-code_frequency . collect_stats:numenta-nupic-commit_activity . collect_stats:numenta-nupic-participation . collect_stats:numenta-nupic-releases . collect_stats:wiseodd-generative-models-code_frequency . collect_stats:wiseodd-generative-models-commit_activity . collect_stats:wiseodd-generative-models-participation . collect_stats:wiseodd-generative-models-releases . collect_stats:TeamUltroid-Ultroid-code_frequency . collect_stats:TeamUltroid-Ultroid-commit_activity . collect_stats:TeamUltroid-Ultroid-participation . collect_stats:TeamUltroid-Ultroid-releases . collect_stats:deeppavlov-DeepPavlov-code_frequency . collect_stats:deeppavlov-DeepPavlov-commit_activity . collect_stats:deeppavlov-DeepPavlov-participation . collect_stats:deeppavlov-DeepPavlov-releases . collect_stats:openai-baselines-code_frequency . collect_stats:openai-baselines-commit_activity . collect_stats:openai-baselines-participation . collect_stats:openai-baselines-releases . collect_stats:lyst-lightfm-code_frequency . collect_stats:lyst-lightfm-commit_activity . collect_stats:lyst-lightfm-participation . collect_stats:lyst-lightfm-releases . collect_stats:deezer-spleeter-code_frequency . collect_stats:deezer-spleeter-commit_activity . collect_stats:deezer-spleeter-participation . collect_stats:deezer-spleeter-releases . collect_stats:babysor-MockingBird-code_frequency . collect_stats:babysor-MockingBird-commit_activity . collect_stats:babysor-MockingBird-participation . collect_stats:babysor-MockingBird-releases . collect_stats:tensorlayer-TensorLayer-code_frequency . collect_stats:tensorlayer-TensorLayer-commit_activity . collect_stats:tensorlayer-TensorLayer-participation . collect_stats:tensorlayer-TensorLayer-releases . collect_stats:flask-admin-flask-admin-code_frequency . collect_stats:flask-admin-flask-admin-commit_activity . collect_stats:flask-admin-flask-admin-participation . collect_stats:flask-admin-flask-admin-releases . collect_stats:codelucas-newspaper-code_frequency . collect_stats:codelucas-newspaper-commit_activity . collect_stats:codelucas-newspaper-participation . collect_stats:codelucas-newspaper-releases . collect_stats:dmlc-gluon-nlp-code_frequency . collect_stats:dmlc-gluon-nlp-commit_activity . collect_stats:dmlc-gluon-nlp-participation . collect_stats:dmlc-gluon-nlp-releases . collect_stats:python-poetry-poetry-code_frequency . collect_stats:python-poetry-poetry-commit_activity . collect_stats:python-poetry-poetry-participation . collect_stats:python-poetry-poetry-releases . collect_stats:darknessomi-musicbox-code_frequency . collect_stats:darknessomi-musicbox-commit_activity . collect_stats:darknessomi-musicbox-participation . collect_stats:darknessomi-musicbox-releases . collect_stats:pyro-ppl-pyro-code_frequency . collect_stats:pyro-ppl-pyro-commit_activity . collect_stats:pyro-ppl-pyro-participation . collect_stats:pyro-ppl-pyro-releases . collect_stats:alexjc-neural-enhance-code_frequency . collect_stats:alexjc-neural-enhance-commit_activity . collect_stats:alexjc-neural-enhance-participation . collect_stats:alexjc-neural-enhance-releases . collect_stats:QuantEcon-QuantEcon.py-code_frequency . collect_stats:QuantEcon-QuantEcon.py-commit_activity . collect_stats:QuantEcon-QuantEcon.py-participation . collect_stats:QuantEcon-QuantEcon.py-releases . collect_stats:Gallopsled-pwntools-code_frequency . collect_stats:Gallopsled-pwntools-commit_activity . collect_stats:Gallopsled-pwntools-participation . collect_stats:Gallopsled-pwntools-releases . collect_stats:amueller-word_cloud-code_frequency . collect_stats:amueller-word_cloud-commit_activity . collect_stats:amueller-word_cloud-participation . collect_stats:amueller-word_cloud-releases . collect_stats:rasbt-mlxtend-code_frequency . collect_stats:rasbt-mlxtend-commit_activity . collect_stats:rasbt-mlxtend-participation . collect_stats:rasbt-mlxtend-releases . collect_stats:google-python-fire-code_frequency . collect_stats:google-python-fire-commit_activity . collect_stats:google-python-fire-participation . collect_stats:google-python-fire-releases . collect_stats:1adrianb-face-alignment-code_frequency . collect_stats:1adrianb-face-alignment-commit_activity . collect_stats:1adrianb-face-alignment-participation . collect_stats:1adrianb-face-alignment-releases . collect_stats:django-django-code_frequency . collect_stats:django-django-commit_activity . collect_stats:django-django-participation . collect_stats:django-django-releases . collect_stats:elebumm-RedditVideoMakerBot-code_frequency . collect_stats:elebumm-RedditVideoMakerBot-commit_activity . collect_stats:elebumm-RedditVideoMakerBot-participation . collect_stats:elebumm-RedditVideoMakerBot-releases . collect_stats:Jack-Cherish-python-spider-code_frequency . collect_stats:Jack-Cherish-python-spider-commit_activity . collect_stats:Jack-Cherish-python-spider-participation . collect_stats:Jack-Cherish-python-spider-releases . collect_stats:OpenBB-finance-OpenBBTerminal-code_frequency . collect_stats:OpenBB-finance-OpenBBTerminal-commit_activity . collect_stats:OpenBB-finance-OpenBBTerminal-participation . collect_stats:OpenBB-finance-OpenBBTerminal-releases . collect_stats:networkx-networkx-code_frequency . collect_stats:networkx-networkx-commit_activity . collect_stats:networkx-networkx-participation . collect_stats:networkx-networkx-releases . collect_stats:openshift-openshift-ansible-code_frequency . collect_stats:openshift-openshift-ansible-commit_activity . collect_stats:openshift-openshift-ansible-participation . collect_stats:openshift-openshift-ansible-releases . collect_stats:heartexlabs-label-studio-code_frequency . collect_stats:heartexlabs-label-studio-commit_activity . collect_stats:heartexlabs-label-studio-participation . collect_stats:heartexlabs-label-studio-releases . collect_stats:CTFd-CTFd-code_frequency . collect_stats:CTFd-CTFd-commit_activity . collect_stats:CTFd-CTFd-participation . collect_stats:CTFd-CTFd-releases . collect_stats:nvbn-thefuck-code_frequency . collect_stats:nvbn-thefuck-commit_activity . collect_stats:nvbn-thefuck-participation . collect_stats:nvbn-thefuck-releases . collect_stats:encode-django-rest-framework-code_frequency . collect_stats:encode-django-rest-framework-commit_activity . collect_stats:encode-django-rest-framework-participation . collect_stats:encode-django-rest-framework-releases . collect_stats:tflearn-tflearn-code_frequency . collect_stats:tflearn-tflearn-commit_activity . collect_stats:tflearn-tflearn-participation . collect_stats:tflearn-tflearn-releases . collect_stats:Theano-Theano-code_frequency . collect_stats:Theano-Theano-commit_activity . collect_stats:Theano-Theano-participation . collect_stats:Theano-Theano-releases . collect_stats:TheSpeedX-TBomb-code_frequency . collect_stats:TheSpeedX-TBomb-commit_activity . collect_stats:TheSpeedX-TBomb-participation . collect_stats:TheSpeedX-TBomb-releases . collect_stats:hyperopt-hyperopt-code_frequency . collect_stats:hyperopt-hyperopt-commit_activity . collect_stats:hyperopt-hyperopt-participation . collect_stats:hyperopt-hyperopt-releases . collect_stats:donnemartin-dev-setup-code_frequency . collect_stats:donnemartin-dev-setup-commit_activity . collect_stats:donnemartin-dev-setup-participation . collect_stats:donnemartin-dev-setup-releases . collect_stats:microsoft-qlib-code_frequency . collect_stats:microsoft-qlib-commit_activity . collect_stats:microsoft-qlib-participation . collect_stats:microsoft-qlib-releases . collect_stats:googleapis-google-api-python-client-code_frequency . collect_stats:googleapis-google-api-python-client-commit_activity . collect_stats:googleapis-google-api-python-client-participation . collect_stats:googleapis-google-api-python-client-releases . collect_stats:horovod-horovod-code_frequency . collect_stats:horovod-horovod-commit_activity . collect_stats:horovod-horovod-participation . collect_stats:horovod-horovod-releases . collect_stats:scikit-image-scikit-image-code_frequency . collect_stats:scikit-image-scikit-image-commit_activity . collect_stats:scikit-image-scikit-image-participation . collect_stats:scikit-image-scikit-image-releases . collect_stats:git-cola-git-cola-code_frequency . collect_stats:git-cola-git-cola-commit_activity . collect_stats:git-cola-git-cola-participation . collect_stats:git-cola-git-cola-releases . collect_stats:nicolargo-glances-code_frequency . collect_stats:nicolargo-glances-commit_activity . collect_stats:nicolargo-glances-participation . collect_stats:nicolargo-glances-releases . collect_stats:deepmind-pysc2-code_frequency . collect_stats:deepmind-pysc2-commit_activity . collect_stats:deepmind-pysc2-participation . collect_stats:deepmind-pysc2-releases . collect_stats:librosa-librosa-code_frequency . collect_stats:librosa-librosa-commit_activity . collect_stats:librosa-librosa-participation . collect_stats:librosa-librosa-releases . collect_stats:PyMySQL-mysqlclient-code_frequency . collect_stats:PyMySQL-mysqlclient-commit_activity . collect_stats:PyMySQL-mysqlclient-participation . collect_stats:PyMySQL-mysqlclient-releases . collect_stats:trustedsec-ptf-code_frequency . collect_stats:trustedsec-ptf-commit_activity . collect_stats:trustedsec-ptf-participation . collect_stats:trustedsec-ptf-releases . collect_stats:facebookresearch-fairseq-code_frequency . collect_stats:facebookresearch-fairseq-commit_activity . collect_stats:facebookresearch-fairseq-participation . collect_stats:facebookresearch-fairseq-releases . collect_stats:mingrammer-diagrams-code_frequency . collect_stats:mingrammer-diagrams-commit_activity . collect_stats:mingrammer-diagrams-participation . collect_stats:mingrammer-diagrams-releases . collect_stats:mkleehammer-pyodbc-code_frequency . collect_stats:mkleehammer-pyodbc-commit_activity . collect_stats:mkleehammer-pyodbc-participation . collect_stats:mkleehammer-pyodbc-releases . collect_stats:aaPanel-BaoTa-code_frequency . collect_stats:aaPanel-BaoTa-commit_activity . collect_stats:aaPanel-BaoTa-participation . collect_stats:aaPanel-BaoTa-releases . collect_stats:mlflow-mlflow-code_frequency . collect_stats:mlflow-mlflow-commit_activity . collect_stats:mlflow-mlflow-participation . collect_stats:mlflow-mlflow-releases . collect_stats:deepchem-deepchem-code_frequency . collect_stats:deepchem-deepchem-commit_activity . collect_stats:deepchem-deepchem-participation . collect_stats:deepchem-deepchem-releases . collect_stats:frappe-bench-code_frequency . collect_stats:frappe-bench-commit_activity . collect_stats:frappe-bench-participation . collect_stats:frappe-bench-releases . collect_stats:matplotlib-matplotlib-code_frequency . collect_stats:matplotlib-matplotlib-commit_activity . collect_stats:matplotlib-matplotlib-participation . collect_stats:matplotlib-matplotlib-releases . collect_stats:pydata-pandas-datareader-code_frequency . collect_stats:pydata-pandas-datareader-commit_activity . collect_stats:pydata-pandas-datareader-participation . collect_stats:pydata-pandas-datareader-releases . collect_stats:TencentARC-GFPGAN-code_frequency . collect_stats:TencentARC-GFPGAN-commit_activity . collect_stats:TencentARC-GFPGAN-participation . collect_stats:TencentARC-GFPGAN-releases . collect_stats:ThoughtfulDev-EagleEye-code_frequency . collect_stats:ThoughtfulDev-EagleEye-commit_activity . collect_stats:ThoughtfulDev-EagleEye-participation . collect_stats:ThoughtfulDev-EagleEye-releases . collect_stats:Uberi-speech_recognition-code_frequency . collect_stats:Uberi-speech_recognition-commit_activity . collect_stats:Uberi-speech_recognition-participation . collect_stats:Uberi-speech_recognition-releases . collect_stats:scikit-learn-contrib-imbalanced-learn-code_frequency . collect_stats:scikit-learn-contrib-imbalanced-learn-commit_activity . collect_stats:scikit-learn-contrib-imbalanced-learn-participation . collect_stats:scikit-learn-contrib-imbalanced-learn-releases . collect_stats:Lightning-AI-lightning-code_frequency . collect_stats:Lightning-AI-lightning-commit_activity . collect_stats:Lightning-AI-lightning-participation . collect_stats:Lightning-AI-lightning-releases . collect_stats:strawlab-python-pcl-code_frequency . collect_stats:strawlab-python-pcl-commit_activity . collect_stats:strawlab-python-pcl-participation . collect_stats:strawlab-python-pcl-releases . collect_stats:jopohl-urh-code_frequency . collect_stats:jopohl-urh-commit_activity . collect_stats:jopohl-urh-participation . collect_stats:jopohl-urh-releases . collect_stats:GreaterWMS-GreaterWMS-code_frequency . collect_stats:GreaterWMS-GreaterWMS-commit_activity . collect_stats:GreaterWMS-GreaterWMS-participation . collect_stats:GreaterWMS-GreaterWMS-releases . collect_stats:LionSec-katoolin-code_frequency . collect_stats:LionSec-katoolin-commit_activity . collect_stats:LionSec-katoolin-participation . collect_stats:LionSec-katoolin-releases . collect_stats:ManimCommunity-manim-code_frequency . collect_stats:ManimCommunity-manim-commit_activity . collect_stats:ManimCommunity-manim-participation . collect_stats:ManimCommunity-manim-releases . collect_stats:dmlc-gluon-cv-code_frequency . collect_stats:dmlc-gluon-cv-commit_activity . collect_stats:dmlc-gluon-cv-participation . collect_stats:dmlc-gluon-cv-releases . collect_stats:gorakhargosh-watchdog-code_frequency . collect_stats:gorakhargosh-watchdog-commit_activity . collect_stats:gorakhargosh-watchdog-participation . collect_stats:gorakhargosh-watchdog-releases . collect_stats:biolab-orange3-code_frequency . collect_stats:biolab-orange3-commit_activity . collect_stats:biolab-orange3-participation . collect_stats:biolab-orange3-releases . collect_stats:NullArray-AutoSploit-code_frequency . collect_stats:NullArray-AutoSploit-commit_activity . collect_stats:NullArray-AutoSploit-participation . collect_stats:NullArray-AutoSploit-releases . collect_stats:webpy-webpy-code_frequency . collect_stats:webpy-webpy-commit_activity . collect_stats:webpy-webpy-participation . collect_stats:webpy-webpy-releases . collect_stats:sherlock-project-sherlock-code_frequency . collect_stats:sherlock-project-sherlock-commit_activity . collect_stats:sherlock-project-sherlock-participation . collect_stats:sherlock-project-sherlock-releases . collect_stats:celery-celery-code_frequency . collect_stats:celery-celery-commit_activity . collect_stats:celery-celery-participation . collect_stats:celery-celery-releases . collect_stats:xinntao-Real-ESRGAN-code_frequency . collect_stats:xinntao-Real-ESRGAN-commit_activity . collect_stats:xinntao-Real-ESRGAN-participation . collect_stats:xinntao-Real-ESRGAN-releases . collect_stats:drivendata-cookiecutter-data-science-code_frequency . collect_stats:drivendata-cookiecutter-data-science-commit_activity . collect_stats:drivendata-cookiecutter-data-science-participation . collect_stats:drivendata-cookiecutter-data-science-releases . collect_stats:pgmpy-pgmpy-code_frequency . collect_stats:pgmpy-pgmpy-commit_activity . collect_stats:pgmpy-pgmpy-participation . collect_stats:pgmpy-pgmpy-releases . collect_stats:Chia-Network-chia-blockchain-code_frequency . collect_stats:Chia-Network-chia-blockchain-commit_activity . collect_stats:Chia-Network-chia-blockchain-participation . collect_stats:Chia-Network-chia-blockchain-releases . collect_stats:thunil-TecoGAN-code_frequency . collect_stats:thunil-TecoGAN-commit_activity . collect_stats:thunil-TecoGAN-participation . collect_stats:thunil-TecoGAN-releases . collect_stats:CouchPotato-CouchPotatoServer-code_frequency . collect_stats:CouchPotato-CouchPotatoServer-commit_activity . collect_stats:CouchPotato-CouchPotatoServer-participation . collect_stats:CouchPotato-CouchPotatoServer-releases . collect_stats:boto-boto3-code_frequency . collect_stats:boto-boto3-commit_activity . collect_stats:boto-boto3-participation . collect_stats:boto-boto3-releases . collect_stats:RaRe-Technologies-gensim-code_frequency . collect_stats:RaRe-Technologies-gensim-commit_activity . collect_stats:RaRe-Technologies-gensim-participation . collect_stats:RaRe-Technologies-gensim-releases . collect_stats:CharlesPikachu-Games-code_frequency . collect_stats:CharlesPikachu-Games-commit_activity . collect_stats:CharlesPikachu-Games-participation . collect_stats:CharlesPikachu-Games-releases . collect_stats:pytorch-tutorials-code_frequency . collect_stats:pytorch-tutorials-commit_activity . collect_stats:pytorch-tutorials-participation . collect_stats:pytorch-tutorials-releases . collect_stats:PyMySQL-PyMySQL-code_frequency . collect_stats:PyMySQL-PyMySQL-commit_activity . collect_stats:PyMySQL-PyMySQL-participation . collect_stats:PyMySQL-PyMySQL-releases . collect_stats:google-jax-code_frequency . collect_stats:google-jax-commit_activity . collect_stats:google-jax-participation . collect_stats:google-jax-releases . collect_stats:asweigart-pyautogui-code_frequency . collect_stats:asweigart-pyautogui-commit_activity . collect_stats:asweigart-pyautogui-participation . collect_stats:asweigart-pyautogui-releases . collect_stats:Hironsan-BossSensor-code_frequency . collect_stats:Hironsan-BossSensor-commit_activity . collect_stats:Hironsan-BossSensor-participation . collect_stats:Hironsan-BossSensor-releases . collect_stats:tensorflow-agents-code_frequency . collect_stats:tensorflow-agents-commit_activity . collect_stats:tensorflow-agents-participation . collect_stats:tensorflow-agents-releases . collect_stats:Zulko-moviepy-code_frequency . collect_stats:Zulko-moviepy-commit_activity . collect_stats:Zulko-moviepy-participation . collect_stats:Zulko-moviepy-releases . collect_stats:wting-autojump-code_frequency . collect_stats:wting-autojump-commit_activity . collect_stats:wting-autojump-participation . collect_stats:wting-autojump-releases . collect_stats:python-mypy-code_frequency . collect_stats:python-mypy-commit_activity . collect_stats:python-mypy-participation . collect_stats:python-mypy-releases . collect_stats:docker-docker-py-code_frequency . collect_stats:docker-docker-py-commit_activity . collect_stats:docker-docker-py-participation . collect_stats:docker-docker-py-releases . collect_stats:conda-conda-code_frequency . collect_stats:conda-conda-commit_activity . collect_stats:conda-conda-participation . collect_stats:conda-conda-releases . collect_stats:aristocratos-bpytop-code_frequency . collect_stats:aristocratos-bpytop-commit_activity . collect_stats:aristocratos-bpytop-participation . collect_stats:aristocratos-bpytop-releases . collect_stats:pypa-pip-code_frequency . collect_stats:pypa-pip-commit_activity . collect_stats:pypa-pip-participation . collect_stats:pypa-pip-releases . collect_stats:InstaPy-InstaPy-code_frequency . collect_stats:InstaPy-InstaPy-commit_activity . collect_stats:InstaPy-InstaPy-participation . collect_stats:InstaPy-InstaPy-releases . collect_stats:jupyterhub-jupyterhub-code_frequency . collect_stats:jupyterhub-jupyterhub-commit_activity . collect_stats:jupyterhub-jupyterhub-participation . collect_stats:jupyterhub-jupyterhub-releases . collect_stats:my8100-scrapydweb-code_frequency . collect_stats:my8100-scrapydweb-commit_activity . collect_stats:my8100-scrapydweb-participation . collect_stats:my8100-scrapydweb-releases . collect_stats:coursera-dl-coursera-dl-code_frequency . collect_stats:coursera-dl-coursera-dl-commit_activity . collect_stats:coursera-dl-coursera-dl-participation . collect_stats:coursera-dl-coursera-dl-releases . collect_stats:zeromq-pyzmq-code_frequency . collect_stats:zeromq-pyzmq-commit_activity . collect_stats:zeromq-pyzmq-participation . collect_stats:zeromq-pyzmq-releases . collect_stats:frappe-erpnext-code_frequency . collect_stats:frappe-erpnext-commit_activity . collect_stats:frappe-erpnext-participation . collect_stats:frappe-erpnext-releases . collect_stats:microsoft-pyright-code_frequency . collect_stats:microsoft-pyright-commit_activity . collect_stats:microsoft-pyright-participation . collect_stats:microsoft-pyright-releases . collect_stats:sightmachine-SimpleCV-code_frequency . collect_stats:sightmachine-SimpleCV-commit_activity . collect_stats:sightmachine-SimpleCV-participation . collect_stats:sightmachine-SimpleCV-releases . collect_stats:deepmind-graph_nets-code_frequency . collect_stats:deepmind-graph_nets-commit_activity . collect_stats:deepmind-graph_nets-participation . collect_stats:deepmind-graph_nets-releases . collect_stats:sshuttle-sshuttle-code_frequency . collect_stats:sshuttle-sshuttle-commit_activity . collect_stats:sshuttle-sshuttle-participation . collect_stats:sshuttle-sshuttle-releases . collect_stats:joestump-python-oauth2-code_frequency . collect_stats:joestump-python-oauth2-commit_activity . collect_stats:joestump-python-oauth2-participation . collect_stats:joestump-python-oauth2-releases . collect_stats:ycm-core-YouCompleteMe-code_frequency . collect_stats:ycm-core-YouCompleteMe-commit_activity . collect_stats:ycm-core-YouCompleteMe-participation . collect_stats:ycm-core-YouCompleteMe-releases . collect_stats:twisted-twisted-code_frequency . collect_stats:twisted-twisted-commit_activity . collect_stats:twisted-twisted-participation . collect_stats:twisted-twisted-releases . collect_stats:SecureAuthCorp-impacket-code_frequency . collect_stats:SecureAuthCorp-impacket-commit_activity . collect_stats:SecureAuthCorp-impacket-participation . collect_stats:SecureAuthCorp-impacket-releases . collect_stats:encode-starlette-code_frequency . collect_stats:encode-starlette-commit_activity . collect_stats:encode-starlette-participation . collect_stats:encode-starlette-releases . collect_stats:nodejs-node-gyp-code_frequency . collect_stats:nodejs-node-gyp-commit_activity . collect_stats:nodejs-node-gyp-participation . collect_stats:nodejs-node-gyp-releases . collect_stats:pyserial-pyserial-code_frequency . collect_stats:pyserial-pyserial-commit_activity . collect_stats:pyserial-pyserial-participation . collect_stats:pyserial-pyserial-releases . collect_stats:keras-rl-keras-rl-code_frequency . collect_stats:keras-rl-keras-rl-commit_activity . collect_stats:keras-rl-keras-rl-participation . collect_stats:keras-rl-keras-rl-releases . collect_stats:onionshare-onionshare-code_frequency . collect_stats:onionshare-onionshare-commit_activity . collect_stats:onionshare-onionshare-participation . collect_stats:onionshare-onionshare-releases . collect_stats:mesonbuild-meson-code_frequency . collect_stats:mesonbuild-meson-commit_activity . collect_stats:mesonbuild-meson-participation . collect_stats:mesonbuild-meson-releases . collect_stats:MTG-sms-tools-code_frequency . collect_stats:MTG-sms-tools-commit_activity . collect_stats:MTG-sms-tools-participation . collect_stats:MTG-sms-tools-releases . collect_stats:mps-youtube-mps-youtube-code_frequency . collect_stats:mps-youtube-mps-youtube-commit_activity . collect_stats:mps-youtube-mps-youtube-participation . collect_stats:mps-youtube-mps-youtube-releases . collect_stats:MongoEngine-mongoengine-code_frequency . collect_stats:MongoEngine-mongoengine-commit_activity . collect_stats:MongoEngine-mongoengine-participation . collect_stats:MongoEngine-mongoengine-releases . collect_stats:deepset-ai-haystack-code_frequency . collect_stats:deepset-ai-haystack-commit_activity . collect_stats:deepset-ai-haystack-participation . collect_stats:deepset-ai-haystack-releases . collect_stats:sourabhv-FlapPyBird-code_frequency . collect_stats:sourabhv-FlapPyBird-commit_activity . collect_stats:sourabhv-FlapPyBird-participation . collect_stats:sourabhv-FlapPyBird-releases . collect_stats:open-mmlab-mmcv-code_frequency . collect_stats:open-mmlab-mmcv-commit_activity . collect_stats:open-mmlab-mmcv-participation . collect_stats:open-mmlab-mmcv-releases . collect_stats:faucetsdn-ryu-code_frequency . collect_stats:faucetsdn-ryu-commit_activity . collect_stats:faucetsdn-ryu-participation . collect_stats:faucetsdn-ryu-releases","title":"retrieve the data with doit"},{"location":"xxii/2022-12-13-github-api-stats.html#explore-the-data-in-dataframes","text":"import pandas df = pandas . DataFrame ( get_iterables (), columns = \"url project repo\" . split ()) . set_index ( \"url\" ) df = df . set_index ( df . index . str . rpartition ( \"/\" ) . get_level_values ( - 1 ) . rename ( \"stat\" ), append = True ) df = df . reorder_levels ([ 1 , 0 ], axis = 0 ) releases = df . loc [ \"releases\" ] . index . to_series () . map ( get_stats ) . apply ( pandas . Series ) . stack () . apply ( pandas . Series ) /tmp/ipykernel_70909/139248023.py:1: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning. releases =df.loc[\"releases\"].index.to_series().map(get_stats).apply(pandas.Series).stack().apply(pandas.Series) releases [[ \"tag_name\" , \"published_at\" ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tag_name published_at url https://api.github.com/repos/tensorflow/models/releases 0 v2.11.0 2022-11-23T00:09:34Z 1 v2.10.1 2022-11-17T05:15:35Z 2 v2.10.0 2022-09-19T21:45:22Z 3 v2.7.2 2022-07-08T06:48:39Z 4 v2.9.2 2022-05-20T04:55:27Z ... ... ... ... https://api.github.com/repos/open-mmlab/mmcv/releases 25 v1.3.13 2021-09-10T03:43:36Z 26 v1.3.12 2021-08-24T14:17:49Z 27 v1.3.11 2021-08-12T09:20:52Z 28 v1.3.10 2021-07-24T12:40:10Z 29 v1.3.9 2021-07-10T03:07:15Z 2808 rows \u00d7 2 columns freq = df . loc [ \"code_frequency\" ] . index . to_series () . map ( get_stats ) . apply ( pandas . Series ) . stack () . reset_index ( - 1 , drop = True ) . apply ( pandas . Series ) freq . columns = list ( \"w+-\" ) freq . w = freq . w . pipe ( pandas . to_datetime , unit = \"s\" ) freq = freq . set_index ( freq . index . map ( operator . itemgetter ( slice ( len ( GH ) + 7 , - len ( \"/stats/code_frequency\" ))))) import hvplot.pandas .bk-root, .bk-root .bk:before, .bk-root .bk:after { font-family: var(--jp-ui-font-size1); font-size: var(--jp-ui-font-size1); color: var(--jp-ui-font-color1); } b = freq [ list ( \"+-\" )] . abs () . sum ( axis = 1 ) . gt ( 0 ) freq [ b ][ list ( \"+-\" )] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } + - count 5.267500e+04 5.267500e+04 mean 5.047921e+03 -4.063599e+03 std 8.565557e+04 9.036993e+04 min 0.000000e+00 -1.494524e+07 25% 4.000000e+01 -6.400000e+02 50% 2.940000e+02 -1.100000e+02 75% 1.367500e+03 -1.300000e+01 max 1.461846e+07 0.000000e+00","title":"explore the data in dataframes"},{"location":"xxii/2022-12-15-html-tags-study.html","text":"all the tags i could be using \u00a4 i'm interested in how we write about code, and i think we should use standards in as many places as possible. the document explores some of the standard html tags that i'm not aware of, or forgot about. let's look through the standard html tags and see what opinions we can reuse. motivation \u00a4 this exercise is useful for me because i like to write in markdown which has a limited design affordances. since markdown is a superset of html we can reuse standard tags as design units in our literate programming. a concrete example is the <var/> tag \u00a4 The HTML element represents the name of a variable in a mathematical expression or a programming context. It's typically presented using an italicized version of the current typeface, although that behavior is browser-dependent. The Var Text element based on definition the <var> tag seems like perfect way to style variables defined within a program. * i do not agree with the style, but i agree with the semantics. * to derive intertextuality between code/narrative. pre tags seems to make more sense between there forms will be most consistent. the monospace indicates that this reads code. * maybe the code has a different style itself. style choices \u00a4 while toying with the document i realize that some folks might be caught up by style inconsistencies with the tags. i'm changing some styles used in this document. the goal of this is semantic consistency so style matters less today. style=\\ css var { font-family: monospace; whitespace: pre; font-weight: bold; } mark { background-color: lightblue; } a:link { font-weight: bold; } var { font-family: monospace; whitespace: pre; font-weight: bold; } mark { background-color: lightblue; } a:link { font-weight: bold; } ``` <style> {{style.splitlines(True)[1:-1] | join}} our resources \u00a4 we are going to explore documentation from mdn and w3schools because they are popular resources that populated the query all the html tags . mdn,w3schools=filter(any, ( https://developer.mozilla.org/en-US/docs/Web/HTML/Element https://www.w3schools.com/TAGS/default.asp ).splitlines()) choosing a resource \u00a4 we are going to refer to the mdn docs: * mdn has more description * the mdn docs have 20 more tags defined. after further inspection these may be mostly deprecated tags. * mdn breaks their tables into groups between the two resources there are 139 tags to explore. A = pandas.concat(dfs := pandas.read_html(io.StringIO(requests.get(mdn).text))).set_index(\"Element\") B = pandas.concat(pandas.read_html(io.StringIO(requests.get(w3schools).text))).set_index(\"Tag\") AB = A[\"Description\"].to_frame(\"mdn\").join( B[\"Description\"].rename(\"w3schools\"), how=\"outer\" ) mdn_extras = AB[\"mdn\"].index.symmetric_difference(AB[\"w3schools\"].dropna().index).to_frame().index the tags \u00a4 the html tag \u00a4 Using the language attribute on the HTML element requires the lang attribute to be defined. otherwise, most automated testing with fail. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <html> The <html> HTML element represents the root (t... document tags \u00a4 there is nothing fun and new here. i do need to visit the recommendations for the <meta> for others reasons. for accessibility reasons we need to include <title> because it is the document title. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <base> The <base> HTML element specifies the base URL... 1 <head> The <head> HTML element contains machine-reada... 2 <link> The <link> HTML element specifies relationship... 3 <meta> The <meta> HTML element represents Metadata th... 4 <style> The <style> HTML element contains style inform... 5 <title> The <title> HTML element defines the document'... content sections \u00a4 the <header> element might be practial to utilize within markdown. there are probably geo situations where address would be useful too. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <address> The <address> HTML element indicates that the ... 1 <article> The <article> HTML element represents a self-c... 2 <aside> The <aside> HTML element represents a portion ... 3 <footer> The <footer> HTML element represents a footer ... 4 <header> The <header> HTML element represents introduct... 5 <h1>, <h2>, <h3>, <h4>, <h5>, <h6> The <h1> to <h6> HTML elements represent six l... 6 <main> The <main> HTML element represents the dominan... 7 <nav> The <nav> HTML element represents a section of... 8 <section> The <section> HTML element represents a generi... .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <blockquote> The <blockquote> HTML element indicates that t... 1 <dd> The <dd> HTML element provides the description... 2 <div> The <div> HTML element is the generic containe... 3 <dl> The <dl> HTML element represents a description... 4 <dt> The <dt> HTML element specifies a term in a de... 5 <figcaption> The <figcaption> HTML element represents a cap... 6 <figure> The <figure> HTML element represents self-cont... 7 <hr> The <hr> HTML element represents a thematic br... 8 <li> The <li> HTML element is used to represent an ... 9 <menu> The <menu> HTML element is described in the HT... 10 <ol> The <ol> HTML element represents an ordered li... 11 <p> The <p> HTML element represents a paragraph. P... 12 <pre> The <pre> HTML element represents preformatted... 13 <ul> The <ul> HTML element represents an unordered ... text content \u00a4 dfs[4] inline text content \u00a4 a variable could be an: a, abbr, dfn, mark, ruby, var there are different references to coded objects like: var, code, samp these differentiations can help an author give more semantic meaning dfs[5] in a literate programming, it seems possible that we layer inline tags together my variable is an example with layers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <area> The <area> HTML element defines an area inside... 1 <audio> The <audio> HTML element is used to embed soun... 2 <img> The <img> HTML element embeds an image into th... 3 <map> The <map> HTML element is used with area eleme... 4 <track> The <track> HTML element is used as a child of... 5 <video> The <video> HTML element embeds a media player... .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <embed> The <embed> HTML element embeds external conte... 1 <iframe> The <iframe> HTML element represents a nested ... 2 <object> The <object> HTML element represents an extern... 3 <picture> The <picture> HTML element contains zero or mo... 4 <portal> The <portal> HTML element enables the embeddin... 5 <source> The <source> HTML element specifies multiple m... .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <svg> The svg element is a container that defines a ... 1 <math> The top-level element in MathML is <math>. Eve... multimedia/embeds \u00a4 don't forget your alt text. return dfs[6], dfs[7], dfs[8]","title":"all the tags i could be using"},{"location":"xxii/2022-12-15-html-tags-study.html#all-the-tags-i-could-be-using","text":"i'm interested in how we write about code, and i think we should use standards in as many places as possible. the document explores some of the standard html tags that i'm not aware of, or forgot about. let's look through the standard html tags and see what opinions we can reuse.","title":"all the tags i could be using"},{"location":"xxii/2022-12-15-html-tags-study.html#motivation","text":"this exercise is useful for me because i like to write in markdown which has a limited design affordances. since markdown is a superset of html we can reuse standard tags as design units in our literate programming.","title":"motivation"},{"location":"xxii/2022-12-15-html-tags-study.html#a-concrete-example-is-the-var-tag","text":"The HTML element represents the name of a variable in a mathematical expression or a programming context. It's typically presented using an italicized version of the current typeface, although that behavior is browser-dependent. The Var Text element based on definition the <var> tag seems like perfect way to style variables defined within a program. * i do not agree with the style, but i agree with the semantics. * to derive intertextuality between code/narrative. pre tags seems to make more sense between there forms will be most consistent. the monospace indicates that this reads code. * maybe the code has a different style itself.","title":"a concrete example is the &lt;var/&gt; tag"},{"location":"xxii/2022-12-15-html-tags-study.html#style-choices","text":"while toying with the document i realize that some folks might be caught up by style inconsistencies with the tags. i'm changing some styles used in this document. the goal of this is semantic consistency so style matters less today. style=\\ css var { font-family: monospace; whitespace: pre; font-weight: bold; } mark { background-color: lightblue; } a:link { font-weight: bold; } var { font-family: monospace; whitespace: pre; font-weight: bold; } mark { background-color: lightblue; } a:link { font-weight: bold; } ``` <style> {{style.splitlines(True)[1:-1] | join}}","title":"style choices"},{"location":"xxii/2022-12-15-html-tags-study.html#our-resources","text":"we are going to explore documentation from mdn and w3schools because they are popular resources that populated the query all the html tags . mdn,w3schools=filter(any, ( https://developer.mozilla.org/en-US/docs/Web/HTML/Element https://www.w3schools.com/TAGS/default.asp ).splitlines())","title":"our resources"},{"location":"xxii/2022-12-15-html-tags-study.html#choosing-a-resource","text":"we are going to refer to the mdn docs: * mdn has more description * the mdn docs have 20 more tags defined. after further inspection these may be mostly deprecated tags. * mdn breaks their tables into groups between the two resources there are 139 tags to explore. A = pandas.concat(dfs := pandas.read_html(io.StringIO(requests.get(mdn).text))).set_index(\"Element\") B = pandas.concat(pandas.read_html(io.StringIO(requests.get(w3schools).text))).set_index(\"Tag\") AB = A[\"Description\"].to_frame(\"mdn\").join( B[\"Description\"].rename(\"w3schools\"), how=\"outer\" ) mdn_extras = AB[\"mdn\"].index.symmetric_difference(AB[\"w3schools\"].dropna().index).to_frame().index","title":"choosing a resource"},{"location":"xxii/2022-12-15-html-tags-study.html#the-tags","text":"","title":"the tags"},{"location":"xxii/2022-12-15-html-tags-study.html#the-html-tag","text":"Using the language attribute on the HTML element requires the lang attribute to be defined. otherwise, most automated testing with fail. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <html> The <html> HTML element represents the root (t...","title":"the html tag"},{"location":"xxii/2022-12-15-html-tags-study.html#document-tags","text":"there is nothing fun and new here. i do need to visit the recommendations for the <meta> for others reasons. for accessibility reasons we need to include <title> because it is the document title. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <base> The <base> HTML element specifies the base URL... 1 <head> The <head> HTML element contains machine-reada... 2 <link> The <link> HTML element specifies relationship... 3 <meta> The <meta> HTML element represents Metadata th... 4 <style> The <style> HTML element contains style inform... 5 <title> The <title> HTML element defines the document'...","title":"document tags"},{"location":"xxii/2022-12-15-html-tags-study.html#content-sections","text":"the <header> element might be practial to utilize within markdown. there are probably geo situations where address would be useful too. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <address> The <address> HTML element indicates that the ... 1 <article> The <article> HTML element represents a self-c... 2 <aside> The <aside> HTML element represents a portion ... 3 <footer> The <footer> HTML element represents a footer ... 4 <header> The <header> HTML element represents introduct... 5 <h1>, <h2>, <h3>, <h4>, <h5>, <h6> The <h1> to <h6> HTML elements represent six l... 6 <main> The <main> HTML element represents the dominan... 7 <nav> The <nav> HTML element represents a section of... 8 <section> The <section> HTML element represents a generi... .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <blockquote> The <blockquote> HTML element indicates that t... 1 <dd> The <dd> HTML element provides the description... 2 <div> The <div> HTML element is the generic containe... 3 <dl> The <dl> HTML element represents a description... 4 <dt> The <dt> HTML element specifies a term in a de... 5 <figcaption> The <figcaption> HTML element represents a cap... 6 <figure> The <figure> HTML element represents self-cont... 7 <hr> The <hr> HTML element represents a thematic br... 8 <li> The <li> HTML element is used to represent an ... 9 <menu> The <menu> HTML element is described in the HT... 10 <ol> The <ol> HTML element represents an ordered li... 11 <p> The <p> HTML element represents a paragraph. P... 12 <pre> The <pre> HTML element represents preformatted... 13 <ul> The <ul> HTML element represents an unordered ...","title":"content sections"},{"location":"xxii/2022-12-15-html-tags-study.html#text-content","text":"dfs[4]","title":"text content"},{"location":"xxii/2022-12-15-html-tags-study.html#inline-text-content","text":"a variable could be an: a, abbr, dfn, mark, ruby, var there are different references to coded objects like: var, code, samp these differentiations can help an author give more semantic meaning dfs[5] in a literate programming, it seems possible that we layer inline tags together my variable is an example with layers. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <area> The <area> HTML element defines an area inside... 1 <audio> The <audio> HTML element is used to embed soun... 2 <img> The <img> HTML element embeds an image into th... 3 <map> The <map> HTML element is used with area eleme... 4 <track> The <track> HTML element is used as a child of... 5 <video> The <video> HTML element embeds a media player... .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <embed> The <embed> HTML element embeds external conte... 1 <iframe> The <iframe> HTML element represents a nested ... 2 <object> The <object> HTML element represents an extern... 3 <picture> The <picture> HTML element contains zero or mo... 4 <portal> The <portal> HTML element enables the embeddin... 5 <source> The <source> HTML element specifies multiple m... .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Element Description 0 <svg> The svg element is a container that defines a ... 1 <math> The top-level element in MathML is <math>. Eve...","title":"inline text content"},{"location":"xxii/2022-12-15-html-tags-study.html#multimediaembeds","text":"don't forget your alt text. return dfs[6], dfs[7], dfs[8]","title":"multimedia/embeds"},{"location":"xxii/2022-12-16-issue-activity-graphql.html","text":"retreiving my issue activity \u00a4 retreive my activity with the github graphql api. construct the query from parameters \u00a4 we use % string formatting because of the braces in graphql syntaxes. def get_stamps(user=\"tonyfast\", repo=\"iota-school/notebooks-for-all\"): return \\ query { search(type:ISSUE, query: \"user:%s repo:%s\", first: 100) { edges { node { ... on Issue { url } ... on PullRequest { url } ... on Comment { publishedAt } } } } }\\ % (user, repo) a graphql query the find the dates a user made comments on issues or pull requests. query { search(type:ISSUE, query: \"user:tonyfast repo:iota-school/notebooks-for-all\", first: 100) { edges { node { ... on Issue { url } ... on PullRequest { url } ... on Comment { publishedAt } } } } } df = pandas.DataFrame(( response := requests.post(\"https://api.github.com/graphql\", json=dict(query=get_stamps()), **header) ).json()[\"data\"][\"search\"][\"edges\"])[\"node\"].apply(pandas.Series) df.publishedAt = df.publishedAt.pipe(pandas.to_datetime) freq = df.groupby(pandas.Grouper(freq=\"1D\", key=\"publishedAt\")).count() what days where these comments made at? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } url publishedAt 2015-08-07 00:00:00+00:00 1.0 2016-01-02 00:00:00+00:00 2.0 2016-01-03 00:00:00+00:00 3.0 2016-01-11 00:00:00+00:00 1.0 2016-01-23 00:00:00+00:00 1.0 ... ... 2022-11-08 00:00:00+00:00 2.0 2022-11-16 00:00:00+00:00 1.0 2022-11-21 00:00:00+00:00 1.0 2022-11-22 00:00:00+00:00 1.0 2022-12-14 00:00:00+00:00 1.0 64 rows \u00d7 1 columns days = freq[freq.astype(bool)].dropna() days .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } url publishedAt 2015-08-07 00:00:00+00:00 1.0 2016-01-02 00:00:00+00:00 2.0 2016-01-03 00:00:00+00:00 3.0 2016-01-11 00:00:00+00:00 1.0 2016-01-23 00:00:00+00:00 1.0 ... ... 2022-11-08 00:00:00+00:00 2.0 2022-11-16 00:00:00+00:00 1.0 2022-11-21 00:00:00+00:00 1.0 2022-11-22 00:00:00+00:00 1.0 2022-12-14 00:00:00+00:00 1.0 64 rows \u00d7 1 columns days","title":"retreiving my issue activity"},{"location":"xxii/2022-12-16-issue-activity-graphql.html#retreiving-my-issue-activity","text":"retreive my activity with the github graphql api.","title":"retreiving my issue activity"},{"location":"xxii/2022-12-16-issue-activity-graphql.html#construct-the-query-from-parameters","text":"we use % string formatting because of the braces in graphql syntaxes. def get_stamps(user=\"tonyfast\", repo=\"iota-school/notebooks-for-all\"): return \\ query { search(type:ISSUE, query: \"user:%s repo:%s\", first: 100) { edges { node { ... on Issue { url } ... on PullRequest { url } ... on Comment { publishedAt } } } } }\\ % (user, repo) a graphql query the find the dates a user made comments on issues or pull requests. query { search(type:ISSUE, query: \"user:tonyfast repo:iota-school/notebooks-for-all\", first: 100) { edges { node { ... on Issue { url } ... on PullRequest { url } ... on Comment { publishedAt } } } } } df = pandas.DataFrame(( response := requests.post(\"https://api.github.com/graphql\", json=dict(query=get_stamps()), **header) ).json()[\"data\"][\"search\"][\"edges\"])[\"node\"].apply(pandas.Series) df.publishedAt = df.publishedAt.pipe(pandas.to_datetime) freq = df.groupby(pandas.Grouper(freq=\"1D\", key=\"publishedAt\")).count() what days where these comments made at? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } url publishedAt 2015-08-07 00:00:00+00:00 1.0 2016-01-02 00:00:00+00:00 2.0 2016-01-03 00:00:00+00:00 3.0 2016-01-11 00:00:00+00:00 1.0 2016-01-23 00:00:00+00:00 1.0 ... ... 2022-11-08 00:00:00+00:00 2.0 2022-11-16 00:00:00+00:00 1.0 2022-11-21 00:00:00+00:00 1.0 2022-11-22 00:00:00+00:00 1.0 2022-12-14 00:00:00+00:00 1.0 64 rows \u00d7 1 columns days = freq[freq.astype(bool)].dropna() days .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } url publishedAt 2015-08-07 00:00:00+00:00 1.0 2016-01-02 00:00:00+00:00 2.0 2016-01-03 00:00:00+00:00 3.0 2016-01-11 00:00:00+00:00 1.0 2016-01-23 00:00:00+00:00 1.0 ... ... 2022-11-08 00:00:00+00:00 2.0 2022-11-16 00:00:00+00:00 1.0 2022-11-21 00:00:00+00:00 1.0 2022-11-22 00:00:00+00:00 1.0 2022-12-14 00:00:00+00:00 1.0 64 rows \u00d7 1 columns days","title":"construct the query from parameters"},{"location":"xxii/2022-12-17-find-dependencies-in-content.html","text":"automating blog posts to work with jupyter-lite \u00a4 there is a rare occasion that i write notebooks completely in lite. most often i write in a conventional environment then need to ammend the content to work when we are in jupyterlite. %reload_ext pidgy what do we need to do to make a post work in lite? \u00a4 explicitly defined dependencies. working on a virtual file system is different than your standard file system. normally we don't have to define our environment each time, but independent of a real file system - in the browser - we need to install packages each time. patching shit if we use requests then we should used https://github.com/koenvo/pyodide-http dealing with pidgy and extensions. some packages won't work in lite so we will throw a warning when we know this fo we can infer this information or provide it explicitly in the metadata sometimes i forget imports depfinder to find packages in a notebook for python \u00a4 some of my personal style choices might fail like when i use __import__ , maybe this is a way to cut dependencies from the list. from pathlib import Path import depfinder, pandas __import__(\"requests_cache\").install_cache() def get_files(dir=\"\", glob=\"*.ipynb\") -> pandas.Index: return pandas.Index(Path(dir).rglob(glob)).rename(\"files\") def get_cells(files: pandas.Index) -> pandas.DataFrame: df = ( files.to_series().apply(Path.read_text) .apply(json.loads).apply(pandas.Series) .cells.apply(pandas.Series).stack().apply(pandas.Series) ) return df.join(get_has_pidgy(df)) can haz pidgy? \u00a4 some of these posts are in pidgy , i'll use %reload_ext pidgy when that is the situation. peek in the cells to find pidgy notebooks. def get_has_pidgy(cells): return cells[cells.cell_type.eq(\"code\")].source.apply(\"\".join).groupby( pandas.Grouper(level=0) ).apply(lambda df: df.str.contains(\"%[re]*load_ext pidgy\").any()).rename(\"pidgy\") cells = get_cells(get_files()) get the imports \u00a4 def get_import(row: pandas.Series) -> dict: get_import normalizes the cell source code for analysis by depfinder . this method catches those situations or returns the attributes of depfinder.inspection.ImportFinder source = \"\".join(row.source) if row.pidgy: source = midgy.python.Python().render(source) try: return vars(depfinder.inspection.get_imported_libs(textwrap.dedent(source), row.name[0])) except BaseException as e: return None evaluate the sources \u00a4 import depfinder, pandas, midgy __import__(\"requests_cache\").install_cache() \u00d8 = __name__ == \"__main__\" and \"__file__\" not in locals() def get_modules(cells): return ( ( results:= cells[cells.cell_type.eq(\"code\")].apply(get_import, axis=1) .dropna().apply(functools.partial(pandas.Series, dtype=\"O\")) )[results.columns[results.columns.str.endswith(\"_modules\")]] ) a snapshot of the modules import within the content pandas pathlib requests tonyfast IPython json midgy functools toolz re depfinder shlex importnb dataclasses typer pidgy nbconvert ipywidgets info textwrap typing traitlets doit dask pluggy sys requests_cache jsonref playwright nbformat inspect types ast uritemplate __ jinja2 numpy io gc pyld orjson jsonpointer urllib.parse linecache operator poser matplotlib rich mpl_toolkits importlib anyio unittest.mock __static_notebook_tags nbconvert_html5 bs4 mkdocs icalendar yaml hvplot jupyter_core shutil arrow __11_12_async_import doctest unittest __09_pyproject_analysis html tomli pytest __12_09_pyproject_analysis warnings _022_10_21_markdown_future a_little_markdown_program click markdown sympy __better_dask_shape asyncio dis traceback micropip __pycache__ ibis duckdb pyarrow nest_asyncio collections abc tqdm 0 41 30 14 14 14 13 12 12 11 10 10 7 7 7 7 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 if \u00d8: (cells := get_cells(get_files())) (cells := cells.join(get_modules(cells))) modules = cells[cells.columns[cells.columns.str.endswith(\"_modules\")]] modules = modules.stack().apply(list).apply(pandas.Series, dtype=\"O\").stack() return HTML(modules.value_counts().to_frame().T.to_html()) todo \u00a4 inject the imports back into the notebooks. where though? find magics","title":"automating blog posts to work with jupyter-lite"},{"location":"xxii/2022-12-17-find-dependencies-in-content.html#automating-blog-posts-to-work-with-jupyter-lite","text":"there is a rare occasion that i write notebooks completely in lite. most often i write in a conventional environment then need to ammend the content to work when we are in jupyterlite. %reload_ext pidgy","title":"automating blog posts to work with jupyter-lite"},{"location":"xxii/2022-12-17-find-dependencies-in-content.html#what-do-we-need-to-do-to-make-a-post-work-in-lite","text":"explicitly defined dependencies. working on a virtual file system is different than your standard file system. normally we don't have to define our environment each time, but independent of a real file system - in the browser - we need to install packages each time. patching shit if we use requests then we should used https://github.com/koenvo/pyodide-http dealing with pidgy and extensions. some packages won't work in lite so we will throw a warning when we know this fo we can infer this information or provide it explicitly in the metadata sometimes i forget imports","title":"what do we need to do to make a post work in lite?"},{"location":"xxii/2022-12-17-find-dependencies-in-content.html#depfinder-to-find-packages-in-a-notebook-for-python","text":"some of my personal style choices might fail like when i use __import__ , maybe this is a way to cut dependencies from the list. from pathlib import Path import depfinder, pandas __import__(\"requests_cache\").install_cache() def get_files(dir=\"\", glob=\"*.ipynb\") -> pandas.Index: return pandas.Index(Path(dir).rglob(glob)).rename(\"files\") def get_cells(files: pandas.Index) -> pandas.DataFrame: df = ( files.to_series().apply(Path.read_text) .apply(json.loads).apply(pandas.Series) .cells.apply(pandas.Series).stack().apply(pandas.Series) ) return df.join(get_has_pidgy(df))","title":"depfinder to find packages in a notebook for python"},{"location":"xxii/2022-12-17-find-dependencies-in-content.html#can-haz-pidgy","text":"some of these posts are in pidgy , i'll use %reload_ext pidgy when that is the situation. peek in the cells to find pidgy notebooks. def get_has_pidgy(cells): return cells[cells.cell_type.eq(\"code\")].source.apply(\"\".join).groupby( pandas.Grouper(level=0) ).apply(lambda df: df.str.contains(\"%[re]*load_ext pidgy\").any()).rename(\"pidgy\") cells = get_cells(get_files())","title":"can haz pidgy?"},{"location":"xxii/2022-12-17-find-dependencies-in-content.html#get-the-imports","text":"def get_import(row: pandas.Series) -> dict: get_import normalizes the cell source code for analysis by depfinder . this method catches those situations or returns the attributes of depfinder.inspection.ImportFinder source = \"\".join(row.source) if row.pidgy: source = midgy.python.Python().render(source) try: return vars(depfinder.inspection.get_imported_libs(textwrap.dedent(source), row.name[0])) except BaseException as e: return None","title":"get the imports"},{"location":"xxii/2022-12-17-find-dependencies-in-content.html#evaluate-the-sources","text":"import depfinder, pandas, midgy __import__(\"requests_cache\").install_cache() \u00d8 = __name__ == \"__main__\" and \"__file__\" not in locals() def get_modules(cells): return ( ( results:= cells[cells.cell_type.eq(\"code\")].apply(get_import, axis=1) .dropna().apply(functools.partial(pandas.Series, dtype=\"O\")) )[results.columns[results.columns.str.endswith(\"_modules\")]] ) a snapshot of the modules import within the content pandas pathlib requests tonyfast IPython json midgy functools toolz re depfinder shlex importnb dataclasses typer pidgy nbconvert ipywidgets info textwrap typing traitlets doit dask pluggy sys requests_cache jsonref playwright nbformat inspect types ast uritemplate __ jinja2 numpy io gc pyld orjson jsonpointer urllib.parse linecache operator poser matplotlib rich mpl_toolkits importlib anyio unittest.mock __static_notebook_tags nbconvert_html5 bs4 mkdocs icalendar yaml hvplot jupyter_core shutil arrow __11_12_async_import doctest unittest __09_pyproject_analysis html tomli pytest __12_09_pyproject_analysis warnings _022_10_21_markdown_future a_little_markdown_program click markdown sympy __better_dask_shape asyncio dis traceback micropip __pycache__ ibis duckdb pyarrow nest_asyncio collections abc tqdm 0 41 30 14 14 14 13 12 12 11 10 10 7 7 7 7 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 if \u00d8: (cells := get_cells(get_files())) (cells := cells.join(get_modules(cells))) modules = cells[cells.columns[cells.columns.str.endswith(\"_modules\")]] modules = modules.stack().apply(list).apply(pandas.Series, dtype=\"O\").stack() return HTML(modules.value_counts().to_frame().T.to_html())","title":"evaluate the sources"},{"location":"xxii/2022-12-17-find-dependencies-in-content.html#todo","text":"inject the imports back into the notebooks. where though? find magics","title":"todo"},{"location":"xxii/2022-12-18-mkdocs-task.html","text":"doit integration - a mkdocs example \u00a4 in the hunt for a computable blog, i've wanted to be able to reuse the code i write about in posts. one integration we are beginning to experiment with in this post is access to doit task specifications from posts. the tonyfast.dodo module integrates posts with tasks. doit is a great tool for orchestrating commands that operate on files. to demonstrate the integration we'll compose a task to build a mkdocs site, and deal with some pain points in the configuration. the mkdocs task \u00a4 mkdocs integrations are identified when a project has the mkdocs.yml file. this file contains all the information for your site including the documents to build. for blog style content, adding a new post means updating the configuration file. my adhd brain has a tendency to forget the configuration bit. in this file, we wrote tools to infer the files that should be included in the configuration, we sort them, and place them back in the configuration file. what follow is the incovation for mkdocs for the tonyfast site. in this document we: define functions to find posts define a doit task to execute mkdocs there is a mkdocs-material blog feature avaiable to insiders. when this is generally available a lot of this content will be moot. the mkdocs doit task \u00a4 primarily doit tasks are functions that begin with the prefix task_ ; task_mkdocs is the one task we define in this document. find posts and build the mkdocs site def task_mkdocs (): yield dict ( name = \"toc\" , actions = [ ( set_nav , ( pathlib . Path ( \"tonyfast\" ), \"*.ipynb\" , \"mkdocs.yml\" )) ], targets = [ \"mkdocs.yml\" ], uptodate = [ False ]) yield dict ( name = \"build\" , actions = [ \"mkdocs build\" ], file_dep = [ \"mkdocs.yml\" ], targets = [ \"site/index.html\" ] ) finding and sorting the posts \u00a4 find all the potential notebook posts def get_posts_from_dir ( dir , glob = \"*.ipynb\" ): for file in dir . iterdir (): if file . is_dir (): if \"checkpoints\" not in file . name : yield from get_posts_from_dir ( file , glob ) elif file . suffix in { \".ipynb\" }: if ( m := title . match ( str ( file . stem ))): yield file , m def get_posts ( dir , glob = \"*.ipynb\" ): return sorted (( ( x . relative_to ( WHERE ), y ) for x , y in get_posts_from_dir ( dir , glob ) ), reverse = True , key = lambda x : x [ 1 ] . group ( * \"ymd\" )) format the ordered entries def get_toc ( dir , glob = \"*.ipynb\" ): for x , y in list ( get_posts ( WHERE , glob )): yield \" \" * 4 + F \"- { x } \" we don't use yaml cause our mkdocs uses yaml tags. instead we put the nav at the end of the document then replace the default with the updated version. replace the nav \u00a4 def get_nav ( dir , glob = \"*.ipynb\" ): return \"\" . join ( re . split ( \"(notes\\S*:\\S*)\" , ( WHERE . parent / \"mkdocs.yml\" ) . read_text (), 1 )[: 2 ] ) + \" \\n \" + \" \\n \" . join ( get_toc ( dir , glob )) import tonyfast , pathlib , yaml , re WHERE = pathlib . Path ( tonyfast . __file__ ) . parent title = re . compile ( \"(?P<y>[0-9] {4} )-(?P<m>[0-9]{1,2})-(?P<d>[0-9]{1,2})-(?P<t>.+)\" ) def set_nav ( dir , glob = \"*.ipynb\" , target = WHERE . parent / \"mkdocs.yml\" ): pathlib . Path ( target ) . write_text ( get_nav ( pathlib . Path ( dir ), glob )) invocation \u00a4 the task is exposed in the tonyfast module python -m tonyfast tasks mkdocs this command is invoked with hatch in a virtual environment using: hatch run docs:build use with importnb -t if \"__file__\" not in locals (): ! python - m importnb - t 2022 - 12 - 18 - mkdocs - task . ipynb info mkdocs mkdocs find posts and build the mkdocs site status : run * The task has no dependencies. task_dep : - mkdocs:toc - mkdocs:build","title":"<pre>doit</pre> integration - a <pre>mkdocs</pre> example"},{"location":"xxii/2022-12-18-mkdocs-task.html#doit-integration-a-mkdocs-example","text":"in the hunt for a computable blog, i've wanted to be able to reuse the code i write about in posts. one integration we are beginning to experiment with in this post is access to doit task specifications from posts. the tonyfast.dodo module integrates posts with tasks. doit is a great tool for orchestrating commands that operate on files. to demonstrate the integration we'll compose a task to build a mkdocs site, and deal with some pain points in the configuration.","title":"doit integration - a mkdocs example"},{"location":"xxii/2022-12-18-mkdocs-task.html#the-mkdocs-task","text":"mkdocs integrations are identified when a project has the mkdocs.yml file. this file contains all the information for your site including the documents to build. for blog style content, adding a new post means updating the configuration file. my adhd brain has a tendency to forget the configuration bit. in this file, we wrote tools to infer the files that should be included in the configuration, we sort them, and place them back in the configuration file. what follow is the incovation for mkdocs for the tonyfast site. in this document we: define functions to find posts define a doit task to execute mkdocs there is a mkdocs-material blog feature avaiable to insiders. when this is generally available a lot of this content will be moot.","title":"the mkdocs task"},{"location":"xxii/2022-12-18-mkdocs-task.html#the-mkdocs-doit-task","text":"primarily doit tasks are functions that begin with the prefix task_ ; task_mkdocs is the one task we define in this document. find posts and build the mkdocs site def task_mkdocs (): yield dict ( name = \"toc\" , actions = [ ( set_nav , ( pathlib . Path ( \"tonyfast\" ), \"*.ipynb\" , \"mkdocs.yml\" )) ], targets = [ \"mkdocs.yml\" ], uptodate = [ False ]) yield dict ( name = \"build\" , actions = [ \"mkdocs build\" ], file_dep = [ \"mkdocs.yml\" ], targets = [ \"site/index.html\" ] )","title":"the mkdocs doit task"},{"location":"xxii/2022-12-18-mkdocs-task.html#finding-and-sorting-the-posts","text":"find all the potential notebook posts def get_posts_from_dir ( dir , glob = \"*.ipynb\" ): for file in dir . iterdir (): if file . is_dir (): if \"checkpoints\" not in file . name : yield from get_posts_from_dir ( file , glob ) elif file . suffix in { \".ipynb\" }: if ( m := title . match ( str ( file . stem ))): yield file , m def get_posts ( dir , glob = \"*.ipynb\" ): return sorted (( ( x . relative_to ( WHERE ), y ) for x , y in get_posts_from_dir ( dir , glob ) ), reverse = True , key = lambda x : x [ 1 ] . group ( * \"ymd\" )) format the ordered entries def get_toc ( dir , glob = \"*.ipynb\" ): for x , y in list ( get_posts ( WHERE , glob )): yield \" \" * 4 + F \"- { x } \" we don't use yaml cause our mkdocs uses yaml tags. instead we put the nav at the end of the document then replace the default with the updated version.","title":"finding and sorting the posts"},{"location":"xxii/2022-12-18-mkdocs-task.html#replace-the-nav","text":"def get_nav ( dir , glob = \"*.ipynb\" ): return \"\" . join ( re . split ( \"(notes\\S*:\\S*)\" , ( WHERE . parent / \"mkdocs.yml\" ) . read_text (), 1 )[: 2 ] ) + \" \\n \" + \" \\n \" . join ( get_toc ( dir , glob )) import tonyfast , pathlib , yaml , re WHERE = pathlib . Path ( tonyfast . __file__ ) . parent title = re . compile ( \"(?P<y>[0-9] {4} )-(?P<m>[0-9]{1,2})-(?P<d>[0-9]{1,2})-(?P<t>.+)\" ) def set_nav ( dir , glob = \"*.ipynb\" , target = WHERE . parent / \"mkdocs.yml\" ): pathlib . Path ( target ) . write_text ( get_nav ( pathlib . Path ( dir ), glob ))","title":"replace the nav"},{"location":"xxii/2022-12-18-mkdocs-task.html#invocation","text":"the task is exposed in the tonyfast module python -m tonyfast tasks mkdocs this command is invoked with hatch in a virtual environment using: hatch run docs:build use with importnb -t if \"__file__\" not in locals (): ! python - m importnb - t 2022 - 12 - 18 - mkdocs - task . ipynb info mkdocs mkdocs find posts and build the mkdocs site status : run * The task has no dependencies. task_dep : - mkdocs:toc - mkdocs:build","title":"invocation"},{"location":"xxii/2022-12-19-integrating-typer.html","text":"integrating typer into the blog \u00a4 in this blog, posts are programs and they are designed to be reused in python. that means that we want to be able to reuse code and narrative as much as possible. add command to main IMPORTED , MAIN = \"__file__\" in locals (), __name__ == \"__main__\" ; \u00d8 = not IMPORTED and MAIN a small example \u00a4 the first example we'll describe is a simple function with little consequence. this will validate our integration. def main ( greeting : str = \"howdy\" , name : str = \"tony\" ): \"\"\"a nothing function that says hi to the audience\"\"\" print ( greeting , name ) invoking the command line if \u00d8 : ! python - m tonyfast Usage: python -m tonyfast [OPTIONS] COMMAND [ARGS]... \u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 - -help -h Show this message and exit. \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 hello a nothing function that says hi to the audience \u2502 \u2502 tasks run doit tasks \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f if \u00d8 : ! python - m tonyfast hello -- name y \\ 'all howdy y'all this is a nice example to start with because it allows us to consider what our works look like as literature. a more complex example \u00a4 in a recent post, we made it possible add doit tasks to the blog. from typer import Typer , Context app = Typer ( rich_markup_mode = \"rich\" ) @app . command ( context_settings = { \"allow_extra_args\" : True , \"ignore_unknown_options\" : True }) def tasks ( ctx : Context ): \"\"\"run doit tasks\"\"\" from doit.doit_cmd import DoitMain from doit.cmd_base import ModuleTaskLoader from tonyfast import dodo return DoitMain ( ModuleTaskLoader ( dodo )) . run ( ctx . args ) verifying the doit commands if \u00d8 : ! python - m tonyfast tasks list -- status -- all R mkdocs build document with mkdocs R mkdocs:build R mkdocs:toc invoking this document as a standalone script \u00a4 if IMPORTED and MAIN : app . command ( name = \"hello\" )( main ) app () if \u00d8 : ! importnb 2022 - 12 - 19 - integrating - typer . ipynb -- help Usage: 2022-12-19-integrating-typer.ipynb [OPTIONS] COMMAND [ARGS]... \u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 - -install -completion Install completion for the current shell. \u2502 \u2502 - -show -completion Show completion for the current shell, to copy \u2502 \u2502 it or customize the installation. \u2502 \u2502 - -help Show this message and exit. \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 hello a nothing function that says hi to the audience \u2502 \u2502 tasks run doit tasks \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f","title":"integrating <pre>typer</pre> into the blog"},{"location":"xxii/2022-12-19-integrating-typer.html#integrating-typer-into-the-blog","text":"in this blog, posts are programs and they are designed to be reused in python. that means that we want to be able to reuse code and narrative as much as possible. add command to main IMPORTED , MAIN = \"__file__\" in locals (), __name__ == \"__main__\" ; \u00d8 = not IMPORTED and MAIN","title":"integrating typer into the blog"},{"location":"xxii/2022-12-19-integrating-typer.html#a-small-example","text":"the first example we'll describe is a simple function with little consequence. this will validate our integration. def main ( greeting : str = \"howdy\" , name : str = \"tony\" ): \"\"\"a nothing function that says hi to the audience\"\"\" print ( greeting , name ) invoking the command line if \u00d8 : ! python - m tonyfast Usage: python -m tonyfast [OPTIONS] COMMAND [ARGS]... \u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 - -help -h Show this message and exit. \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 hello a nothing function that says hi to the audience \u2502 \u2502 tasks run doit tasks \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f if \u00d8 : ! python - m tonyfast hello -- name y \\ 'all howdy y'all this is a nice example to start with because it allows us to consider what our works look like as literature.","title":"a small example"},{"location":"xxii/2022-12-19-integrating-typer.html#a-more-complex-example","text":"in a recent post, we made it possible add doit tasks to the blog. from typer import Typer , Context app = Typer ( rich_markup_mode = \"rich\" ) @app . command ( context_settings = { \"allow_extra_args\" : True , \"ignore_unknown_options\" : True }) def tasks ( ctx : Context ): \"\"\"run doit tasks\"\"\" from doit.doit_cmd import DoitMain from doit.cmd_base import ModuleTaskLoader from tonyfast import dodo return DoitMain ( ModuleTaskLoader ( dodo )) . run ( ctx . args ) verifying the doit commands if \u00d8 : ! python - m tonyfast tasks list -- status -- all R mkdocs build document with mkdocs R mkdocs:build R mkdocs:toc","title":"a more complex example"},{"location":"xxii/2022-12-19-integrating-typer.html#invoking-this-document-as-a-standalone-script","text":"if IMPORTED and MAIN : app . command ( name = \"hello\" )( main ) app () if \u00d8 : ! importnb 2022 - 12 - 19 - integrating - typer . ipynb -- help Usage: 2022-12-19-integrating-typer.ipynb [OPTIONS] COMMAND [ARGS]... \u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 - -install -completion Install completion for the current shell. \u2502 \u2502 - -show -completion Show completion for the current shell, to copy \u2502 \u2502 it or customize the installation. \u2502 \u2502 - -help Show this message and exit. \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 hello a nothing function that says hi to the audience \u2502 \u2502 tasks run doit tasks \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f","title":"invoking this document as a standalone script"},{"location":"xxii/2022-12-20-pyscript-nbconvert.html","text":"rendering a notebook with pyscript \u00a4 let's convert a notebook to a pyscript document. we'll using nbconvert and jinja2 to transform a notebook into a standalone interactive document. jinja2 template overrides for nbconvert \u00a4 our pyscript extends the base jupyterlab styling. template = \"\"\" { %- e xtends 'lab/index.html.j2' -%}\"\"\" include pyscript css and javascript assets in the <head> template += \"\"\" {%- block header -%} {{super()}} <link rel=\"stylesheet\" href=\"https://pyscript.net/latest/pyscript.css\" /> <script defer src=\"https://pyscript.net/latest/pyscript.js\"></script> { %- e ndblock header -%}\"\"\" requirejs and pyscript do not play well together , and the block below prevents requirejs from loading. template += \"\"\" {%- block html_head_js -%} { %- e ndblock html_head_js -%}\"\"\" from a notebook's source we can infer the packages it needs to run. the packages are placed in the <py-config> tag. continue reading to see how the get_imports_from_cells filter is defined. template += \"\"\" {% block body_header %} {{super()}} <py-config> packages = {{nb | get_imports_from_cells}} </py-config> { % e ndblock body_header %}\"\"\" we'll replace standard pygments cell inputs with a <py-repl> with autogenerate disabled. template += \"\"\" {% block input %} <py-repl output=\"out-{{cell.id}}\"> {{cell.source | escape | dedent}} </py-repl> { % e ndblock input %}\"\"\" we use any existing outputs as the dead pixels. the outputs are replaced the first pyscript executes in a nearby cell. template += \"\"\" {% block output %} <div id=\"out-{{cell.id}}\">{{super()}}</div> { % e ndblock output %} {% block codecell %} {{super()}} { % i f not cell.outputs %} <div id=\"out-{{cell.id}}\"></div> { % e ndif %} { % e ndblock codecell %} \"\"\" our template is defined for <py-script> documents. next we introduce template into the nbconvert machinery. nbconvert exporting machinery \u00a4 nbconvert is the primary machinery used to transform notebook documents into other file formats. it is a wrapper around the shape of the notebook and a jinja2 environment. import depfinder ; from pathlib import Path ; from functools import partial from nbconvert.exporters import HTMLExporter , TemplateExporter inferring dependencies \u00a4 the py-config defines the environment. we use depfinder to do that. def get_imports_from_cells ( nb ): imports = set () for cell in nb . cells : imports . update ( get_imports_from_cell ( cell )) if imports . intersection ({ \"requests\" , \"httpx\" , \"urllib\" }): # add more later imports . add ( \"pyodide-http\" ) return list ( imports ) def get_imports_from_cell ( cell ): import depfinder __import__ ( \"requests_cache\" ) . install_cache () if cell [ \"cell_type\" ] == \"code\" : try : yield from depfinder . inspection . get_imported_libs ( textwrap . dedent ( \"\" . join ( cell [ \"source\" ]))) . required_modules except BaseException as e : pass the nbconvert exporter \u00a4 get_exporter generates a new notebook file converter. adds filters used in template puts a template on the jinja2.DictLoader with our custom template def get_exporter ( template = template ): import textwrap , html , jinja2 exporter = HTMLExporter ( template_file = \"pyscript.j2\" , filters = dict ( dedent = textwrap . dedent , get_imports_from_cells = get_imports_from_cells , escape = html . escape ) ) for loader in exporter . environment . loader . loaders : if isinstance ( loader , jinja2 . DictLoader ): loader . mapping [ \"pyscript.j2\" ] = template return exporter pyscript transformation functions \u00a4 get_pyscript turns a file into a string of py-script html. def get_pyscript ( file ): return get_exporter ( template ) . from_filename ( file )[ 0 ] pyscript transforms a file and writes the py-script document to disk. def pyscript ( file : Path , target : Path = None , write : bool = True ): \"\"\"generate a pyscript version of a notebook\"\"\" body = get_pyscript ( file ) if write : if not target : target = file . with_suffix ( F \" { file . suffix } .html\" ) target . write_text ( body ) print ( F \"created { target } \" ) if __name__ == \"__main__\" and \"__file__\" not in locals (): ! python - m tonyfast pyscript 2022 - 12 - 19 - integrating - typer . ipynb from IPython.display import display , IFrame display ( IFrame ( * \"2022-12-19-integrating-typer.ipynb.html 100% 600\" . split ())) created 2022-12-19-integrating-typer.ipynb.html","title":"rendering a notebook with <pre>pyscript</pre>"},{"location":"xxii/2022-12-20-pyscript-nbconvert.html#rendering-a-notebook-with-pyscript","text":"let's convert a notebook to a pyscript document. we'll using nbconvert and jinja2 to transform a notebook into a standalone interactive document.","title":"rendering a notebook with pyscript"},{"location":"xxii/2022-12-20-pyscript-nbconvert.html#jinja2-template-overrides-for-nbconvert","text":"our pyscript extends the base jupyterlab styling. template = \"\"\" { %- e xtends 'lab/index.html.j2' -%}\"\"\" include pyscript css and javascript assets in the <head> template += \"\"\" {%- block header -%} {{super()}} <link rel=\"stylesheet\" href=\"https://pyscript.net/latest/pyscript.css\" /> <script defer src=\"https://pyscript.net/latest/pyscript.js\"></script> { %- e ndblock header -%}\"\"\" requirejs and pyscript do not play well together , and the block below prevents requirejs from loading. template += \"\"\" {%- block html_head_js -%} { %- e ndblock html_head_js -%}\"\"\" from a notebook's source we can infer the packages it needs to run. the packages are placed in the <py-config> tag. continue reading to see how the get_imports_from_cells filter is defined. template += \"\"\" {% block body_header %} {{super()}} <py-config> packages = {{nb | get_imports_from_cells}} </py-config> { % e ndblock body_header %}\"\"\" we'll replace standard pygments cell inputs with a <py-repl> with autogenerate disabled. template += \"\"\" {% block input %} <py-repl output=\"out-{{cell.id}}\"> {{cell.source | escape | dedent}} </py-repl> { % e ndblock input %}\"\"\" we use any existing outputs as the dead pixels. the outputs are replaced the first pyscript executes in a nearby cell. template += \"\"\" {% block output %} <div id=\"out-{{cell.id}}\">{{super()}}</div> { % e ndblock output %} {% block codecell %} {{super()}} { % i f not cell.outputs %} <div id=\"out-{{cell.id}}\"></div> { % e ndif %} { % e ndblock codecell %} \"\"\" our template is defined for <py-script> documents. next we introduce template into the nbconvert machinery.","title":"jinja2 template overrides for nbconvert"},{"location":"xxii/2022-12-20-pyscript-nbconvert.html#nbconvert-exporting-machinery","text":"nbconvert is the primary machinery used to transform notebook documents into other file formats. it is a wrapper around the shape of the notebook and a jinja2 environment. import depfinder ; from pathlib import Path ; from functools import partial from nbconvert.exporters import HTMLExporter , TemplateExporter","title":"nbconvert exporting machinery"},{"location":"xxii/2022-12-20-pyscript-nbconvert.html#inferring-dependencies","text":"the py-config defines the environment. we use depfinder to do that. def get_imports_from_cells ( nb ): imports = set () for cell in nb . cells : imports . update ( get_imports_from_cell ( cell )) if imports . intersection ({ \"requests\" , \"httpx\" , \"urllib\" }): # add more later imports . add ( \"pyodide-http\" ) return list ( imports ) def get_imports_from_cell ( cell ): import depfinder __import__ ( \"requests_cache\" ) . install_cache () if cell [ \"cell_type\" ] == \"code\" : try : yield from depfinder . inspection . get_imported_libs ( textwrap . dedent ( \"\" . join ( cell [ \"source\" ]))) . required_modules except BaseException as e : pass","title":"inferring dependencies"},{"location":"xxii/2022-12-20-pyscript-nbconvert.html#the-nbconvert-exporter","text":"get_exporter generates a new notebook file converter. adds filters used in template puts a template on the jinja2.DictLoader with our custom template def get_exporter ( template = template ): import textwrap , html , jinja2 exporter = HTMLExporter ( template_file = \"pyscript.j2\" , filters = dict ( dedent = textwrap . dedent , get_imports_from_cells = get_imports_from_cells , escape = html . escape ) ) for loader in exporter . environment . loader . loaders : if isinstance ( loader , jinja2 . DictLoader ): loader . mapping [ \"pyscript.j2\" ] = template return exporter","title":"the nbconvert exporter"},{"location":"xxii/2022-12-20-pyscript-nbconvert.html#pyscript-transformation-functions","text":"get_pyscript turns a file into a string of py-script html. def get_pyscript ( file ): return get_exporter ( template ) . from_filename ( file )[ 0 ] pyscript transforms a file and writes the py-script document to disk. def pyscript ( file : Path , target : Path = None , write : bool = True ): \"\"\"generate a pyscript version of a notebook\"\"\" body = get_pyscript ( file ) if write : if not target : target = file . with_suffix ( F \" { file . suffix } .html\" ) target . write_text ( body ) print ( F \"created { target } \" ) if __name__ == \"__main__\" and \"__file__\" not in locals (): ! python - m tonyfast pyscript 2022 - 12 - 19 - integrating - typer . ipynb from IPython.display import display , IFrame display ( IFrame ( * \"2022-12-19-integrating-typer.ipynb.html 100% 600\" . split ())) created 2022-12-19-integrating-typer.ipynb.html","title":"pyscript transformation functions"},{"location":"xxii/2022-12-21-lite-build.html","text":"jupyterlite blog integration \u00a4 i've always thought of blog posts as a means, not an ends. my dream has always been content that myself and others can execute themselves. often this goal has been hindered by the need for infrastructure. advances in the jupyterlite have made it possible to realize this vision without infrastructure. jupyterlite is jupyterlab running completely in the browser without the need for a local server. this means that folks can redirect from a static post into an interactive version they can try themselves. jupyterlite notebooks are different \u00a4 when working with a standard jupyter implementation we are interacting with a server within an implicit environment. when we write our notebooks we assume that we can import modules because they are in our environment. however, in jupyterlite we need to explicitly define our dependencies for every notebook. we define our dependencies by inserting the pip line magic: %pip install my dependencies this statement is superfluous under normal circumstances so it doesn't need to exist in the source. instead we use the depfinder project to infer the projects imported by our notebook. the inferred dependencies are then inserted in to the first line of the first code cell of the notebook. the doit lite implementation \u00a4 in the code that follow we define a doit task that: 1. builds a jupyterlite site for this blog 2. make the dependencies compatible with jupyterlite def task_lite (): \"\"\"build the jupyter lite site and append requirements\"\"\" return dict ( actions = [ \"jupyter lite build --contents tonyfast --output-dir site/run\" , ( set_files_imports , ( pathlib . Path ( \"site/run/files\" ),)) ], clean = [ \"rm -rf site/run\" ] ) import typing , tonyfast , pathlib , textwrap , re , json discovering imports with depfinder \u00a4 following sections we'll build the methods for discovering imports with depfinder set_files_imports iterates through a directory and amends notebooks to work in jupyterlite def set_files_imports ( FILES : typing . Iterable [ pathlib . Path ] = ( FILES := ( WHERE := pathlib . Path ( tonyfast . __file__ ) . parent . parent ) / \"site/run/files\" )) -> None : for file in FILES . rglob ( \"*.ipynb\" ): set_file_imports ( file ) get_imports finds the imports in each cell def get_imports ( cell : dict , pidgy = False ) -> set : import depfinder __import__ ( \"requests_cache\" ) . install_cache () source = \"\" . join ( cell [ \"source\" ]) if pidgy : source = tangle . render ( source ) source = textwrap . dedent ( source ) try : found = depfinder . inspection . get_imported_libs ( source ) return found . required_modules . union ( found . sketchy_modules ) except BaseException as e : return get_deps transforms inputs to dependencies. some dependencies may require extra features to work in jupyterlite and they are appended here. mapping = dict ( bs4 = \"beautifulsoup4\" ) def get_deps ( deps : set ) -> set : if \"requests\" in deps : deps . add ( \"pyodide-http\" ) if \"pandas\" in deps : deps . add ( \"jinja2\" ) return { mapping . get ( x , x ) for x in deps if not x . startswith ( \"_\" ) or x not in { \"tonyfast\" } } handling pidgy documents \u00a4 some documents might use [pidgy] syntax that need to be dealt with. PIDGY = re . compile ( \"^\\s*%(re)?load_ext\\s*pidgy\" ) from midgy import Python ; tangle = Python () def has_pidgy ( nb : dict ): yes = False for _ , cell in iter_code_cells ( nb ): yes = yes or PIDGY . match ( \"\" . join ( cell [ \"source\" ])) and True return yes updating the jupyterlite notebooks \u00a4 these methods are meant to operate on the contents of a jupyterlite not the raw notebooks. set_file_imports operates in one file discovers dependencies and writes code back to the source. def set_file_imports ( file : pathlib . Path ) -> None : data = json . loads ( file . read_text ()) deps , first = set (), None pidgy = has_pidgy ( data ) for no , cell in iter_code_cells ( data ): if first is None : first = no if pidgy : data [ \"cells\" ][ no ][ \"metadata\" ] . setdefault ( \"jupyter\" , {})[ \"source_hidden\" ] = True deps . update ( get_imports ( cell , pidgy ) or []) deps = get_deps ( deps ) if pidgy : deps . add ( \"pidgy\" ) if deps and ( first is not None ): cell = data [ \"cells\" ][ first ] was_str = isinstance ( cell [ \"source\" ], str ) if was_str : cell [ \"source\" ] = cell [ \"source\" ] . splitlines ( 1 ) for i , line in enumerate ( list ( cell [ \"source\" ])): if ( left := line . lstrip ()): if left . startswith (( \"%pip install\" ,)): break indent = len ( line ) - len ( left ) if \"pyodide-http\" in deps : data [ \"cells\" ][ first ][ \"source\" ] . insert ( i , \" \" * indent + \"__import__('pyodide_http').patch_all() \\n \" ) data [ \"cells\" ][ first ][ \"source\" ] . insert ( i , \" \" * indent + \"%pip install \" + \" \" . join ( deps ) + \" \\n \" ) print ( F \"writing { len ( set ( deps )) } pip requirements to { file } \" ) file . write_text ( json . dumps ( data , indent = 2 )) break else : print ( F 'no deps for { file } ' ) set_files_imports sets the dependencies for a lot of files. def set_files_imports ( FILES : typing . Iterable [ pathlib . Path ] = FILES ): for file in FILES . rglob ( \"*.ipynb\" ): set_file_imports ( file ) iter_code_cells iterates through just the code cells. def iter_code_cells ( nb : dict ) -> typing . Iterator [ tuple [ int , dict ]]: for i , cell in enumerate ( nb [ \"cells\" ]): if cell [ \"cell_type\" ] == \"code\" : yield i , cell usage \u00a4 from the tonyfast module, requires deps if ( I := '__file__' not in locals ()): ! python - m tonyfast tasks info lite lite build the jupyter lite site and append requirements status : run * The task has no dependencies. from post with importnb if ( I := '__file__' not in locals ()): ! importnb - t 2022 - 12 - 21 - lite - build . ipynb list lite build the jupyter lite site and append requirements run this task from hatch in the root of the project. the hatch environment has all the necessary dependencies defined. hatch run lite:build","title":"<pre>jupyterlite</pre> blog integration"},{"location":"xxii/2022-12-21-lite-build.html#jupyterlite-blog-integration","text":"i've always thought of blog posts as a means, not an ends. my dream has always been content that myself and others can execute themselves. often this goal has been hindered by the need for infrastructure. advances in the jupyterlite have made it possible to realize this vision without infrastructure. jupyterlite is jupyterlab running completely in the browser without the need for a local server. this means that folks can redirect from a static post into an interactive version they can try themselves.","title":"jupyterlite blog integration"},{"location":"xxii/2022-12-21-lite-build.html#jupyterlite-notebooks-are-different","text":"when working with a standard jupyter implementation we are interacting with a server within an implicit environment. when we write our notebooks we assume that we can import modules because they are in our environment. however, in jupyterlite we need to explicitly define our dependencies for every notebook. we define our dependencies by inserting the pip line magic: %pip install my dependencies this statement is superfluous under normal circumstances so it doesn't need to exist in the source. instead we use the depfinder project to infer the projects imported by our notebook. the inferred dependencies are then inserted in to the first line of the first code cell of the notebook.","title":"jupyterlite notebooks are different"},{"location":"xxii/2022-12-21-lite-build.html#the-doit-lite-implementation","text":"in the code that follow we define a doit task that: 1. builds a jupyterlite site for this blog 2. make the dependencies compatible with jupyterlite def task_lite (): \"\"\"build the jupyter lite site and append requirements\"\"\" return dict ( actions = [ \"jupyter lite build --contents tonyfast --output-dir site/run\" , ( set_files_imports , ( pathlib . Path ( \"site/run/files\" ),)) ], clean = [ \"rm -rf site/run\" ] ) import typing , tonyfast , pathlib , textwrap , re , json","title":"the doit lite implementation"},{"location":"xxii/2022-12-21-lite-build.html#discovering-imports-with-depfinder","text":"following sections we'll build the methods for discovering imports with depfinder set_files_imports iterates through a directory and amends notebooks to work in jupyterlite def set_files_imports ( FILES : typing . Iterable [ pathlib . Path ] = ( FILES := ( WHERE := pathlib . Path ( tonyfast . __file__ ) . parent . parent ) / \"site/run/files\" )) -> None : for file in FILES . rglob ( \"*.ipynb\" ): set_file_imports ( file ) get_imports finds the imports in each cell def get_imports ( cell : dict , pidgy = False ) -> set : import depfinder __import__ ( \"requests_cache\" ) . install_cache () source = \"\" . join ( cell [ \"source\" ]) if pidgy : source = tangle . render ( source ) source = textwrap . dedent ( source ) try : found = depfinder . inspection . get_imported_libs ( source ) return found . required_modules . union ( found . sketchy_modules ) except BaseException as e : return get_deps transforms inputs to dependencies. some dependencies may require extra features to work in jupyterlite and they are appended here. mapping = dict ( bs4 = \"beautifulsoup4\" ) def get_deps ( deps : set ) -> set : if \"requests\" in deps : deps . add ( \"pyodide-http\" ) if \"pandas\" in deps : deps . add ( \"jinja2\" ) return { mapping . get ( x , x ) for x in deps if not x . startswith ( \"_\" ) or x not in { \"tonyfast\" } }","title":"discovering imports with depfinder"},{"location":"xxii/2022-12-21-lite-build.html#handling-pidgy-documents","text":"some documents might use [pidgy] syntax that need to be dealt with. PIDGY = re . compile ( \"^\\s*%(re)?load_ext\\s*pidgy\" ) from midgy import Python ; tangle = Python () def has_pidgy ( nb : dict ): yes = False for _ , cell in iter_code_cells ( nb ): yes = yes or PIDGY . match ( \"\" . join ( cell [ \"source\" ])) and True return yes","title":"handling pidgy documents"},{"location":"xxii/2022-12-21-lite-build.html#updating-the-jupyterlite-notebooks","text":"these methods are meant to operate on the contents of a jupyterlite not the raw notebooks. set_file_imports operates in one file discovers dependencies and writes code back to the source. def set_file_imports ( file : pathlib . Path ) -> None : data = json . loads ( file . read_text ()) deps , first = set (), None pidgy = has_pidgy ( data ) for no , cell in iter_code_cells ( data ): if first is None : first = no if pidgy : data [ \"cells\" ][ no ][ \"metadata\" ] . setdefault ( \"jupyter\" , {})[ \"source_hidden\" ] = True deps . update ( get_imports ( cell , pidgy ) or []) deps = get_deps ( deps ) if pidgy : deps . add ( \"pidgy\" ) if deps and ( first is not None ): cell = data [ \"cells\" ][ first ] was_str = isinstance ( cell [ \"source\" ], str ) if was_str : cell [ \"source\" ] = cell [ \"source\" ] . splitlines ( 1 ) for i , line in enumerate ( list ( cell [ \"source\" ])): if ( left := line . lstrip ()): if left . startswith (( \"%pip install\" ,)): break indent = len ( line ) - len ( left ) if \"pyodide-http\" in deps : data [ \"cells\" ][ first ][ \"source\" ] . insert ( i , \" \" * indent + \"__import__('pyodide_http').patch_all() \\n \" ) data [ \"cells\" ][ first ][ \"source\" ] . insert ( i , \" \" * indent + \"%pip install \" + \" \" . join ( deps ) + \" \\n \" ) print ( F \"writing { len ( set ( deps )) } pip requirements to { file } \" ) file . write_text ( json . dumps ( data , indent = 2 )) break else : print ( F 'no deps for { file } ' ) set_files_imports sets the dependencies for a lot of files. def set_files_imports ( FILES : typing . Iterable [ pathlib . Path ] = FILES ): for file in FILES . rglob ( \"*.ipynb\" ): set_file_imports ( file ) iter_code_cells iterates through just the code cells. def iter_code_cells ( nb : dict ) -> typing . Iterator [ tuple [ int , dict ]]: for i , cell in enumerate ( nb [ \"cells\" ]): if cell [ \"cell_type\" ] == \"code\" : yield i , cell","title":"updating the jupyterlite notebooks"},{"location":"xxii/2022-12-21-lite-build.html#usage","text":"from the tonyfast module, requires deps if ( I := '__file__' not in locals ()): ! python - m tonyfast tasks info lite lite build the jupyter lite site and append requirements status : run * The task has no dependencies. from post with importnb if ( I := '__file__' not in locals ()): ! importnb - t 2022 - 12 - 21 - lite - build . ipynb list lite build the jupyter lite site and append requirements run this task from hatch in the root of the project. the hatch environment has all the necessary dependencies defined. hatch run lite:build","title":"usage"},{"location":"xxii/2022-12-22-pyproject-analysis.html","text":"analyzing pyproject.toml configurations of popular projects \u00a4 get_pyproject_config_data loads a dataframe created in a previous post . it contains the results of a graphql query to posted to the github api that returnsthe pyproject files for some of the most popular python projects on github. def get_pyproject_config_data () -> \"pandas.DataFrame\" : with importnb . Notebook (): from __09_pyproject_analysis import tidy_configs , tidy_responses , gather , pyproject_query return tidy_configs ( df := tidy_responses ( responses := gather ( pyproject_query , max = 15 ))) the shape of our dataframe - df from get_pyproject_config_data - is: on the rows: one project per row on the columns: the keys found in the projects pyproject.toml import importnb , pandas display (( df := get_pyproject_config_data ()) . head ( 3 )) F \"there are { len ( df ) } `pyproject.toml` files in the dataset.\" .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tool build-system project mypy flake8 url https://github.com/open-telemetry/opentelemetry-python {'black': {'line-length': 79, 'exclude': '( /( # generated files .tox| venv| .*/build/lib/.*| exporter/opentelemetry-exporter-jaeger-proto-grpc/src/opentelemetry/exporter/jaeger/proto/grpc/gen| exporter/opentelemetry-exporter-jaeger-thrift/src/opentelemetry/exporter/jaeger/thrift/gen| exporter/opentelemetry-exporter-zipkin-proto-http/src/opentelemetry/exporter/zipkin/proto/http/v2/gen| opentelemetry-proto/src/opentelemetry/proto/.*/.*| scripts )/ ) '}, 'pytest': {'ini_options': {'addopts': '-rs -v', 'log_cli': True, 'log_cli_level': 'warning'}}} NaN NaN NaN NaN https://github.com/freemocap/freemocap {'taskipy': {'tasks': {'setup': 'pre-commit install', 'test': 'python -m unittest src/tests/**/test_*', 'installer': './bin/installer.sh', 'format': 'black src/'}}} NaN NaN NaN NaN https://github.com/3b1b/manim NaN {'requires': ['setuptools', 'wheel']} NaN NaN NaN what tools are used most? \u00a4 PEPXXX defines the tool key as a place that third party applications can store configuration information. when we explode the df.tool in tools we find a frame with all the third party tools named. ( tools := df . tool . dropna () . apply ( pandas . Series )) . head ( 3 ) . fillna ( \"\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } black pytest taskipy pyright hatch isort mutmut check-wheel-contents flit coverage ... jupyter-releaser check-manifest vendoring commitizen scriv autoflake tbump autopub poetry-version-plugin typeshed url https://github.com/open-telemetry/opentelemetry-python {'line-length': 79, 'exclude': '( /( # generated files .tox| venv| .*/build/lib/.*| exporter/opentelemetry-exporter-jaeger-proto-grpc/src/opentelemetry/exporter/jaeger/proto/grpc/gen| exporter/opentelemetry-exporter-jaeger-thrift/src/opentelemetry/exporter/jaeger/thrift/gen| exporter/opentelemetry-exporter-zipkin-proto-http/src/opentelemetry/exporter/zipkin/proto/http/v2/gen| opentelemetry-proto/src/opentelemetry/proto/.*/.*| scripts )/ ) '} {'ini_options': {'addopts': '-rs -v', 'log_cli': True, 'log_cli_level': 'warning'}} ... https://github.com/freemocap/freemocap {'tasks': {'setup': 'pre-commit install', 'test': 'python -m unittest src/tests/**/test_*', 'installer': './bin/installer.sh', 'format': 'black src/'}} ... https://github.com/openai/gym {'ini_options': {'filterwarnings': ['ignore:.*step API.*:DeprecationWarning']}} {'include': ['gym/**', 'tests/**'], 'exclude': ['**/node_modules', '**/__pycache__'], 'strict': [], 'typeCheckingMode': 'basic', 'pythonVersion': '3.6', 'pythonPlatform': 'All', 'typeshedPath': 'typeshed', 'enableTypeIgnoreComments': True, 'reportMissingImports': 'none', 'reportMissingTypeStubs': False, 'reportInvalidTypeVarUse': 'none', 'reportGeneralTypeIssues': 'none', 'reportUntypedFunctionDecorator': 'none', 'reportPrivateUsage': 'warning', 'reportUnboundVariable': 'warning'} ... 3 rows \u00d7 54 columns F \"there are { len ( tools . columns ) } tools used in the { len ( df ) } pyproject.toml files.\" the top12 most frequently defined tools in the pyproject.toml s are tool_counts = tools . isna () . astype ( int ) . sub ( 1 ) . abs () . sum () . sort_values ( ascending = False ) ( top12 := tool_counts . iloc [: 12 ]) . to_frame ( \"counts\" ) . T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } black isort pytest mypy coverage poetry setuptools_scm hatch setuptools pylint towncrier pyright counts 123 85 67 42 34 32 21 15 14 14 11 10 from the perspective of these popular projects: there is strong community adoption of black and isort . from this data it might be a recommended convention to format your code and sort your imports. pytest 's third place popularity recommends that we test our projects mypy suggests that type hinting is feature of some popular projects coverage poetry setuptools_scm hatch next we find build systems \u00a4 df [ \"build-system\" ] . dropna () . apply ( pandas . Series ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } requires build-backend dependencies url https://github.com/3b1b/manim [setuptools, wheel] NaN NaN https://github.com/deepmind/hanabi-learning-environment [setuptools, wheel, scikit-build, cmake, ninja] NaN NaN https://github.com/miguelgrinberg/Flask-SocketIO [setuptools>=42, wheel] setuptools.build_meta NaN https://github.com/pypa/pipx [hatchling>=0.15.0] hatchling.build NaN https://github.com/py-pdf/PyPDF2 [flit_core >=3.2,<4] flit_core.buildapi NaN ... ... ... ... https://github.com/deepset-ai/haystack [hatchling>=1.8.0] hatchling.build NaN https://github.com/dedupeio/dedupe [setuptools==63, wheel, cython] setuptools.build_meta NaN https://github.com/sktime/sktime [setuptools>61, wheel, toml, build] setuptools.build_meta NaN https://github.com/enthought/mayavi [oldest-supported-numpy, setuptools, vtk, wheel] NaN NaN https://github.com/holoviz/datashader [param, pyct, setuptools] setuptools.build_meta NaN 173 rows \u00d7 3 columns into the projects? \u00a4 df . project . dropna () . apply ( pandas . Series ) . head ( 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name description readme license requires-python keywords authors classifiers dependencies dynamic urls scripts maintainers optional-dependencies entry-points gui-scripts version url","title":"analyzing <pre>pyproject.toml</pre> configurations of popular projects"},{"location":"xxii/2022-12-22-pyproject-analysis.html#analyzing-pyprojecttoml-configurations-of-popular-projects","text":"get_pyproject_config_data loads a dataframe created in a previous post . it contains the results of a graphql query to posted to the github api that returnsthe pyproject files for some of the most popular python projects on github. def get_pyproject_config_data () -> \"pandas.DataFrame\" : with importnb . Notebook (): from __09_pyproject_analysis import tidy_configs , tidy_responses , gather , pyproject_query return tidy_configs ( df := tidy_responses ( responses := gather ( pyproject_query , max = 15 ))) the shape of our dataframe - df from get_pyproject_config_data - is: on the rows: one project per row on the columns: the keys found in the projects pyproject.toml import importnb , pandas display (( df := get_pyproject_config_data ()) . head ( 3 )) F \"there are { len ( df ) } `pyproject.toml` files in the dataset.\" .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tool build-system project mypy flake8 url https://github.com/open-telemetry/opentelemetry-python {'black': {'line-length': 79, 'exclude': '( /( # generated files .tox| venv| .*/build/lib/.*| exporter/opentelemetry-exporter-jaeger-proto-grpc/src/opentelemetry/exporter/jaeger/proto/grpc/gen| exporter/opentelemetry-exporter-jaeger-thrift/src/opentelemetry/exporter/jaeger/thrift/gen| exporter/opentelemetry-exporter-zipkin-proto-http/src/opentelemetry/exporter/zipkin/proto/http/v2/gen| opentelemetry-proto/src/opentelemetry/proto/.*/.*| scripts )/ ) '}, 'pytest': {'ini_options': {'addopts': '-rs -v', 'log_cli': True, 'log_cli_level': 'warning'}}} NaN NaN NaN NaN https://github.com/freemocap/freemocap {'taskipy': {'tasks': {'setup': 'pre-commit install', 'test': 'python -m unittest src/tests/**/test_*', 'installer': './bin/installer.sh', 'format': 'black src/'}}} NaN NaN NaN NaN https://github.com/3b1b/manim NaN {'requires': ['setuptools', 'wheel']} NaN NaN NaN","title":"analyzing pyproject.toml configurations of popular projects"},{"location":"xxii/2022-12-22-pyproject-analysis.html#what-tools-are-used-most","text":"PEPXXX defines the tool key as a place that third party applications can store configuration information. when we explode the df.tool in tools we find a frame with all the third party tools named. ( tools := df . tool . dropna () . apply ( pandas . Series )) . head ( 3 ) . fillna ( \"\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } black pytest taskipy pyright hatch isort mutmut check-wheel-contents flit coverage ... jupyter-releaser check-manifest vendoring commitizen scriv autoflake tbump autopub poetry-version-plugin typeshed url https://github.com/open-telemetry/opentelemetry-python {'line-length': 79, 'exclude': '( /( # generated files .tox| venv| .*/build/lib/.*| exporter/opentelemetry-exporter-jaeger-proto-grpc/src/opentelemetry/exporter/jaeger/proto/grpc/gen| exporter/opentelemetry-exporter-jaeger-thrift/src/opentelemetry/exporter/jaeger/thrift/gen| exporter/opentelemetry-exporter-zipkin-proto-http/src/opentelemetry/exporter/zipkin/proto/http/v2/gen| opentelemetry-proto/src/opentelemetry/proto/.*/.*| scripts )/ ) '} {'ini_options': {'addopts': '-rs -v', 'log_cli': True, 'log_cli_level': 'warning'}} ... https://github.com/freemocap/freemocap {'tasks': {'setup': 'pre-commit install', 'test': 'python -m unittest src/tests/**/test_*', 'installer': './bin/installer.sh', 'format': 'black src/'}} ... https://github.com/openai/gym {'ini_options': {'filterwarnings': ['ignore:.*step API.*:DeprecationWarning']}} {'include': ['gym/**', 'tests/**'], 'exclude': ['**/node_modules', '**/__pycache__'], 'strict': [], 'typeCheckingMode': 'basic', 'pythonVersion': '3.6', 'pythonPlatform': 'All', 'typeshedPath': 'typeshed', 'enableTypeIgnoreComments': True, 'reportMissingImports': 'none', 'reportMissingTypeStubs': False, 'reportInvalidTypeVarUse': 'none', 'reportGeneralTypeIssues': 'none', 'reportUntypedFunctionDecorator': 'none', 'reportPrivateUsage': 'warning', 'reportUnboundVariable': 'warning'} ... 3 rows \u00d7 54 columns F \"there are { len ( tools . columns ) } tools used in the { len ( df ) } pyproject.toml files.\" the top12 most frequently defined tools in the pyproject.toml s are tool_counts = tools . isna () . astype ( int ) . sub ( 1 ) . abs () . sum () . sort_values ( ascending = False ) ( top12 := tool_counts . iloc [: 12 ]) . to_frame ( \"counts\" ) . T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } black isort pytest mypy coverage poetry setuptools_scm hatch setuptools pylint towncrier pyright counts 123 85 67 42 34 32 21 15 14 14 11 10 from the perspective of these popular projects: there is strong community adoption of black and isort . from this data it might be a recommended convention to format your code and sort your imports. pytest 's third place popularity recommends that we test our projects mypy suggests that type hinting is feature of some popular projects coverage poetry setuptools_scm hatch next we find","title":"what tools are used most?"},{"location":"xxii/2022-12-22-pyproject-analysis.html#build-systems","text":"df [ \"build-system\" ] . dropna () . apply ( pandas . Series ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } requires build-backend dependencies url https://github.com/3b1b/manim [setuptools, wheel] NaN NaN https://github.com/deepmind/hanabi-learning-environment [setuptools, wheel, scikit-build, cmake, ninja] NaN NaN https://github.com/miguelgrinberg/Flask-SocketIO [setuptools>=42, wheel] setuptools.build_meta NaN https://github.com/pypa/pipx [hatchling>=0.15.0] hatchling.build NaN https://github.com/py-pdf/PyPDF2 [flit_core >=3.2,<4] flit_core.buildapi NaN ... ... ... ... https://github.com/deepset-ai/haystack [hatchling>=1.8.0] hatchling.build NaN https://github.com/dedupeio/dedupe [setuptools==63, wheel, cython] setuptools.build_meta NaN https://github.com/sktime/sktime [setuptools>61, wheel, toml, build] setuptools.build_meta NaN https://github.com/enthought/mayavi [oldest-supported-numpy, setuptools, vtk, wheel] NaN NaN https://github.com/holoviz/datashader [param, pyct, setuptools] setuptools.build_meta NaN 173 rows \u00d7 3 columns","title":"build systems"},{"location":"xxii/2022-12-22-pyproject-analysis.html#into-the-projects","text":"df . project . dropna () . apply ( pandas . Series ) . head ( 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name description readme license requires-python keywords authors classifiers dependencies dynamic urls scripts maintainers optional-dependencies entry-points gui-scripts version url","title":"into the projects?"},{"location":"xxii/2022-12-23-mkdocs-plugin.html","tags":["mkdocs-plugin"],"text":"mkdocs plugin for jupyter notebooks \u00a4 i think i want more control of how mkdocs renders notebooks. i've been using mkdocs-jupyter for a while and it is great, but i need more knobs. my particular need is to configure nbconvert exporters with more fine grain control than what mkdocs-jupyter offers. the major difference is we are going to target markdown output rather than html. checklist for successfully integrating the plugin \u00a4 steps to adding a mkdocs plugin to this the tonyfast project: add plugin to mkdocs.yml plugins : - markdown_notebook define plugin entry point for tonyfast [project.entry-points.\"mkdocs.plugins\"] markdown_notebook = \"tonyfast.mkdocs:MarkdownNotebook\" build the MarkdownNotebook plugin integrate the plugin from tonyfast.mkdocs import MarkdownNotebook add and improve the nbconvert export display renderers building the mkdocs plugin \u00a4 import json , nbconvert , nbformat , pathlib , mkdocs.plugins , warnings , re , functools warnings . filterwarnings ( \"ignore\" , category = DeprecationWarning ) the [ mkdocs plugin] we now use a custom template and expo defined in 2022-12-31-markdownish-notebook.ipynb with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxii.__markdownish_notebook import template , PidgyExporter class MarkdownNotebook ( mkdocs . plugins . BasePlugin ): exporter_cls = PidgyExporter config_scheme = ( # ('foo', mkdocs.config.config_options.Type(str, default='a default value')), ) @functools . lru_cache def get_exporter ( self , key = \"mkdocs\" , ** kw ): with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxii.__markdownish_notebook import template , HEAD , replace_attachments kw . setdefault ( \"template_file\" , key ) exporter = self . exporter_cls ( ** kw ) exporter . environment . filters . setdefault ( \"attachment\" , replace_attachments ) from jinja2 import DictLoader for loader in exporter . environment . loader . loaders : if isinstance ( loader , DictLoader ): loader . mapping [ key ] = template loader . mapping [ \"HEAD\" ] = HEAD break return exporter def on_page_read_source ( self , page , config ): if page . file . is_modified (): if page . file . src_uri . endswith (( \".ipynb\" , )): body = pathlib . Path ( page . file . abs_src_path ) . read_text () nb = nbformat . v4 . reads ( body ) exporter = self . get_exporter () return \" \\n \" . join (( \"---\" , json . dumps ( nb . metadata ), \"---\" , # add metadata as front matter exporter . from_notebook_node ( nb )[ 0 ] )) def on_post_page ( self , output , page , config ): if '<script type=\"application/vnd.jupyter.widget-view+json\">' in output : left , sep , right = output . partition ( \"</head\" ) exporter = self . get_exporter () return left + self . get_exporter () . environment . get_template ( \"HEAD\" ) . render ( resources = dict ( jupyter_widgets_base_url = exporter . jupyter_widgets_base_url , html_manager_semver_range = exporter . html_manager_semver_range , widget_renderer_url = exporter . widget_renderer_url , require_js_url = exporter . require_js_url )) + sep + right def on_page_markdown ( self , markdown , page , config , files ): import markdown title = markdown . Markdown ( extensions = config [ 'markdown_extensions' ]) . convert ( page . title ) page . title = title [ len ( \"<p>\" ): - len ( \"</p>\" )] . strip () . replace ( \"code>\" , \"pre>\" ) we trick mkdocs into thinking notebook files are markdown extensions. mkdocs . utils . markdown_extensions += \".ipynb\" , # this feels naughty. {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"<pre>mkdocs</pre> plugin for jupyter notebooks"},{"location":"xxii/2022-12-23-mkdocs-plugin.html#mkdocs-plugin-for-jupyter-notebooks","text":"i think i want more control of how mkdocs renders notebooks. i've been using mkdocs-jupyter for a while and it is great, but i need more knobs. my particular need is to configure nbconvert exporters with more fine grain control than what mkdocs-jupyter offers. the major difference is we are going to target markdown output rather than html.","title":"mkdocs plugin for jupyter notebooks"},{"location":"xxii/2022-12-23-mkdocs-plugin.html#checklist-for-successfully-integrating-the-plugin","text":"steps to adding a mkdocs plugin to this the tonyfast project: add plugin to mkdocs.yml plugins : - markdown_notebook define plugin entry point for tonyfast [project.entry-points.\"mkdocs.plugins\"] markdown_notebook = \"tonyfast.mkdocs:MarkdownNotebook\" build the MarkdownNotebook plugin integrate the plugin from tonyfast.mkdocs import MarkdownNotebook add and improve the nbconvert export display renderers","title":"checklist for successfully integrating the plugin"},{"location":"xxii/2022-12-23-mkdocs-plugin.html#building-the-mkdocs-plugin","text":"import json , nbconvert , nbformat , pathlib , mkdocs.plugins , warnings , re , functools warnings . filterwarnings ( \"ignore\" , category = DeprecationWarning ) the [ mkdocs plugin] we now use a custom template and expo defined in 2022-12-31-markdownish-notebook.ipynb with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxii.__markdownish_notebook import template , PidgyExporter class MarkdownNotebook ( mkdocs . plugins . BasePlugin ): exporter_cls = PidgyExporter config_scheme = ( # ('foo', mkdocs.config.config_options.Type(str, default='a default value')), ) @functools . lru_cache def get_exporter ( self , key = \"mkdocs\" , ** kw ): with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxii.__markdownish_notebook import template , HEAD , replace_attachments kw . setdefault ( \"template_file\" , key ) exporter = self . exporter_cls ( ** kw ) exporter . environment . filters . setdefault ( \"attachment\" , replace_attachments ) from jinja2 import DictLoader for loader in exporter . environment . loader . loaders : if isinstance ( loader , DictLoader ): loader . mapping [ key ] = template loader . mapping [ \"HEAD\" ] = HEAD break return exporter def on_page_read_source ( self , page , config ): if page . file . is_modified (): if page . file . src_uri . endswith (( \".ipynb\" , )): body = pathlib . Path ( page . file . abs_src_path ) . read_text () nb = nbformat . v4 . reads ( body ) exporter = self . get_exporter () return \" \\n \" . join (( \"---\" , json . dumps ( nb . metadata ), \"---\" , # add metadata as front matter exporter . from_notebook_node ( nb )[ 0 ] )) def on_post_page ( self , output , page , config ): if '<script type=\"application/vnd.jupyter.widget-view+json\">' in output : left , sep , right = output . partition ( \"</head\" ) exporter = self . get_exporter () return left + self . get_exporter () . environment . get_template ( \"HEAD\" ) . render ( resources = dict ( jupyter_widgets_base_url = exporter . jupyter_widgets_base_url , html_manager_semver_range = exporter . html_manager_semver_range , widget_renderer_url = exporter . widget_renderer_url , require_js_url = exporter . require_js_url )) + sep + right def on_page_markdown ( self , markdown , page , config , files ): import markdown title = markdown . Markdown ( extensions = config [ 'markdown_extensions' ]) . convert ( page . title ) page . title = title [ len ( \"<p>\" ): - len ( \"</p>\" )] . strip () . replace ( \"code>\" , \"pre>\" ) we trick mkdocs into thinking notebook files are markdown extensions. mkdocs . utils . markdown_extensions += \".ipynb\" , # this feels naughty. {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"building the mkdocs plugin"},{"location":"xxii/2022-12-30-typer-magic.html","tags":["ipython-extension"],"text":"using typer in ipython magics \u00a4 wrap function to consume system arguments using typer register the magic with IPython examples ipython extensions wrapping the function \u00a4 we needs to process system arguments as a string when use magics. the line in a line magic or the first line in a cell magic may take arguments. wrap_magic transforms our function into a magic style method that has the following signature. def magicish(line: str, cell: str =None): ... it uses click by way of typer to deserialize the line of system arguments. these parameters are passed to the original function. the magic function's help/docstring is replaced with the help from the command line interface. def wrap_magic ( function , cell_key = \"cell\" ): import typer , click , shlex , functools app = typer . Typer ( add_completion = False , context_settings = { \"help_option_names\" : [ \"-h\" , \"--help\" ]}) app . command ()( function ) ctx = click . Context ( typer . main . get_command ( app )) @functools . wraps ( function ) def magic ( line , cell = None ): try : ctx . command . parse_args ( ctx , shlex . split ( line )) except click . exceptions . Exit : return if cell_key in ctx . params : ctx . params [ cell_key ] = cell return function ( ** ctx . params ) magic . __doc__ = \" \\n \" . join (( function . __doc__ or \"\" , get_help ( ctx ))) return magic register the magic with IPython \u00a4 register a line, cell or line/cell magic method. def register_magic ( function , name = None , cell_key = \"cell\" ): import inspect from IPython import get_ipython shell = get_ipython () cache_rich_console () signature = inspect . signature ( function ) wrapper = wrap_magic ( function , cell_key = cell_key ) kind = \"line\" if cell_key in signature . parameters : kind = \"line_cell\" if signature . parameters [ cell_key ] . default is inspect . _empty : kind = \"cell\" shell . register_magic_function ( wrapper , kind , name ) shell . log . info ( F \"registered { repr ( function ) } as magic named { name or function . __name__ } \" ) return function getting the help \u00a4 when rich is installed, which it often is, we need to do some tomfoolery in our interactive condition we want access to the rich console that typer uses. we're going to wrap a cache around typer s method so that each we recieve the same rich.console.Console each time the function is invoked. def cache_rich_console ( cache = {}): import typer , functools if not cache : cache . setdefault ( \"_get_rich_console\" , typer . rich_utils . _get_rich_console ) typer . rich_utils . _get_rich_console = functools . lru_cache ( cache [ \"_get_rich_console\" ]) when the Console is consistent we can capture it's output and reclaim the help information def get_help ( ctx ): with typer . rich_utils . _get_rich_console () . capture () as console : ctx . get_help () return console . get () examples \u00a4 register a line magic if ( \u00d8 := \"__file__\" not in locals ()): import typer @register_magic def hello ( count : int = 5 , name : str = typer . Option ( \"world\" , help = \"a name to repeat\" ), msg : str = \"<3\" ): \"\"\"a function that says hello\"\"\" print ( name * count , msg ) assert \"hello\" not in ( shell := get_ipython ()) . magics_manager . magics [ \"cell\" ] % hello worldworldworldworldworld <3 register a cell magic because the cell parameter was found in the signature. this is by convention, and should be configurable. if \u00d8 : @register_magic def yall ( count : int = 5 , name : str = typer . Option ( \"world\" , help = \"a name to repeat\" ), cell : str = \"xoxo\" ): \"\"\"a function that says hello to yall\"\"\" hello ( count , name , msg = cell ) assert \"yall\" in shell . magics_manager . magics [ \"cell\" ], \"the method didn't get registered.\" verify that we can import register_magic for reuse. if \u00d8 : with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxii.__typer_magic import register_magic as imported_register_magic def load_ipython_extension ( shell ): shell . user_ns . setdefault ( register_magic . __name__ , register_magic ) def unload_ipython_extension ( shell ): pass","title":"using <pre>typer</pre> in ipython magics"},{"location":"xxii/2022-12-30-typer-magic.html#using-typer-in-ipython-magics","text":"wrap function to consume system arguments using typer register the magic with IPython examples ipython extensions","title":"using typer in ipython magics"},{"location":"xxii/2022-12-30-typer-magic.html#wrapping-the-function","text":"we needs to process system arguments as a string when use magics. the line in a line magic or the first line in a cell magic may take arguments. wrap_magic transforms our function into a magic style method that has the following signature. def magicish(line: str, cell: str =None): ... it uses click by way of typer to deserialize the line of system arguments. these parameters are passed to the original function. the magic function's help/docstring is replaced with the help from the command line interface. def wrap_magic ( function , cell_key = \"cell\" ): import typer , click , shlex , functools app = typer . Typer ( add_completion = False , context_settings = { \"help_option_names\" : [ \"-h\" , \"--help\" ]}) app . command ()( function ) ctx = click . Context ( typer . main . get_command ( app )) @functools . wraps ( function ) def magic ( line , cell = None ): try : ctx . command . parse_args ( ctx , shlex . split ( line )) except click . exceptions . Exit : return if cell_key in ctx . params : ctx . params [ cell_key ] = cell return function ( ** ctx . params ) magic . __doc__ = \" \\n \" . join (( function . __doc__ or \"\" , get_help ( ctx ))) return magic","title":"wrapping the function"},{"location":"xxii/2022-12-30-typer-magic.html#register-the-magic-with-ipython","text":"register a line, cell or line/cell magic method. def register_magic ( function , name = None , cell_key = \"cell\" ): import inspect from IPython import get_ipython shell = get_ipython () cache_rich_console () signature = inspect . signature ( function ) wrapper = wrap_magic ( function , cell_key = cell_key ) kind = \"line\" if cell_key in signature . parameters : kind = \"line_cell\" if signature . parameters [ cell_key ] . default is inspect . _empty : kind = \"cell\" shell . register_magic_function ( wrapper , kind , name ) shell . log . info ( F \"registered { repr ( function ) } as magic named { name or function . __name__ } \" ) return function","title":"register the magic with IPython"},{"location":"xxii/2022-12-30-typer-magic.html#getting-the-help","text":"when rich is installed, which it often is, we need to do some tomfoolery in our interactive condition we want access to the rich console that typer uses. we're going to wrap a cache around typer s method so that each we recieve the same rich.console.Console each time the function is invoked. def cache_rich_console ( cache = {}): import typer , functools if not cache : cache . setdefault ( \"_get_rich_console\" , typer . rich_utils . _get_rich_console ) typer . rich_utils . _get_rich_console = functools . lru_cache ( cache [ \"_get_rich_console\" ]) when the Console is consistent we can capture it's output and reclaim the help information def get_help ( ctx ): with typer . rich_utils . _get_rich_console () . capture () as console : ctx . get_help () return console . get ()","title":"getting the help"},{"location":"xxii/2022-12-30-typer-magic.html#examples","text":"register a line magic if ( \u00d8 := \"__file__\" not in locals ()): import typer @register_magic def hello ( count : int = 5 , name : str = typer . Option ( \"world\" , help = \"a name to repeat\" ), msg : str = \"<3\" ): \"\"\"a function that says hello\"\"\" print ( name * count , msg ) assert \"hello\" not in ( shell := get_ipython ()) . magics_manager . magics [ \"cell\" ] % hello worldworldworldworldworld <3 register a cell magic because the cell parameter was found in the signature. this is by convention, and should be configurable. if \u00d8 : @register_magic def yall ( count : int = 5 , name : str = typer . Option ( \"world\" , help = \"a name to repeat\" ), cell : str = \"xoxo\" ): \"\"\"a function that says hello to yall\"\"\" hello ( count , name , msg = cell ) assert \"yall\" in shell . magics_manager . magics [ \"cell\" ], \"the method didn't get registered.\" verify that we can import register_magic for reuse. if \u00d8 : with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxii.__typer_magic import register_magic as imported_register_magic def load_ipython_extension ( shell ): shell . user_ns . setdefault ( register_magic . __name__ , register_magic ) def unload_ipython_extension ( shell ): pass","title":"examples"},{"location":"xxii/2022-12-31-jupyter-config-files.html","text":"configuration files \u00a4 i'm going to include my configuration files in this package. it seems like a great place to keep them for consistency. we can import them. from tonyfast import ipython_config , jupyter_lab_config the configurations on the c symbol staying consistent with traitlets configurations. ipython_config . c then we can apply the configuration files to the property directories. IPython and jupyter_core ship their path locations import IPython.paths , jupyter_core.paths from pathlib import Path JUPYTER = Path ( jupyter_core . paths . jupyter_config_dir ()) IPYTHON = Path ( IPython . paths . get_ipython_dir ()) set the configuration files for IPython and jupyter def set_configs ( use_platform_dir : bool = True ): import shutil if use_platform_dir : shutil . copy ( jupyter_lab_config . __file__ , JUPYTER / Path ( jupyter_lab_config . __file__ ) . name ) shutil . copy ( ipython_config . __file__ , IPYTHON / Path ( ipython_config . __file__ ) . name ) else : shutil . copy ( jupyter_lab_config . __file__ , Path ( jupyter_lab_config . __file__ ) . name ) shutil . copy ( ipython_config . __file__ , Path ( ipython_config . __file__ ) . name ) if __name__ == \"__main__\" : if \"__file__\" in locals (): __import__ ( \"typer\" ) . run ( set_configs ) else : ! importnb 2022 - 12 - 31 - jupyter - config - files . ipynb -- help","title":"configuration files"},{"location":"xxii/2022-12-31-jupyter-config-files.html#configuration-files","text":"i'm going to include my configuration files in this package. it seems like a great place to keep them for consistency. we can import them. from tonyfast import ipython_config , jupyter_lab_config the configurations on the c symbol staying consistent with traitlets configurations. ipython_config . c then we can apply the configuration files to the property directories. IPython and jupyter_core ship their path locations import IPython.paths , jupyter_core.paths from pathlib import Path JUPYTER = Path ( jupyter_core . paths . jupyter_config_dir ()) IPYTHON = Path ( IPython . paths . get_ipython_dir ()) set the configuration files for IPython and jupyter def set_configs ( use_platform_dir : bool = True ): import shutil if use_platform_dir : shutil . copy ( jupyter_lab_config . __file__ , JUPYTER / Path ( jupyter_lab_config . __file__ ) . name ) shutil . copy ( ipython_config . __file__ , IPYTHON / Path ( ipython_config . __file__ ) . name ) else : shutil . copy ( jupyter_lab_config . __file__ , Path ( jupyter_lab_config . __file__ ) . name ) shutil . copy ( ipython_config . __file__ , Path ( ipython_config . __file__ ) . name ) if __name__ == \"__main__\" : if \"__file__\" in locals (): __import__ ( \"typer\" ) . run ( set_configs ) else : ! importnb 2022 - 12 - 31 - jupyter - config - files . ipynb -- help","title":"configuration files"},{"location":"xxii/2022-12-31-markdownish-notebook.html","text":"a template for exporting markdownish notebooks \u00a4 we need a combination of markdown and html templates to operate optimally with mkdocs. this document frankensteins templates and gives us some nice pixels. get_template creates our hybrid template from the initial TEMPLATE and goes on to append blocks from nbconvert templates. def get_template () -> str : blocks = __import__ ( \"collections\" ) . ChainMap ( * map ( get_blocks , \"classic/base.html.j2 classic/index.html.j2\" . split ())) template = TEMPLATE for k in KEEP : if k in blocks : template += blocks [ k ] . group ( \"block\" ) + \" \\n \" * 3 return template we KEEP some blocks from the classic templates KEEP = \"notebook_css execute_result stream_stdout stream_stderr \\ data_svg data_html data_png data_jpg error traceback_line data_widget_state data_widget_view\" . split () get_blocks loads a template and returns a dictionary of its blocks. def get_blocks ( alias ) -> dict [ str , ( T := __import__ ( \"typing\" )) . Pattern ]: template = get_exporter () . environment . get_template ( alias ) with open ( template . filename ) as file : body = file . read () return dict ( yield_blocks ( body )) yield all the blocks recursing into the found matches def yield_blocks ( string ) -> T . Iterator [ tuple [ str , T . Pattern ]]: from tonyfast.regexs import jinja_block for m in jinja_block . finditer ( string ): yield m . group ( \"name\" ), m yield from yield_blocks ( m . group ( \"inner\" )) initialize our exporter and jinja environment i want to hide the input code, but no remove it; a custom exporter felt like a better way. import nbconvert , re class PidgyExporter ( nbconvert . exporters . HTMLExporter ): def from_notebook_node ( self , nb , resources = None , ** kw ): resources = self . _init_resources ( dict ( is_pidgy = is_pidgy ( nb ))) return super () . from_notebook_node ( nb , resources , ** kw ) we moved is_pidgy and PIDGY from the mkdocs plugin because them make more self here. def is_pidgy ( nb ): for cell in nb [ \"cells\" ]: if cell [ \"cell_type\" ] == \"code\" : if PIDGY . match ( \"\" . join ( cell [ \"source\" ])): return True return False PIDGY = re . compile ( r \"\\s*%(re)?load_ext pidgy\" ) def replace_attachments ( cell ): source = \"\" . join ( cell [ \"source\" ]) if cell . get ( \"attachments\" ): for k , v in cell [ \"attachments\" ] . items (): for t , v in v . items (): source = source . replace ( \"attachment:\" + k , \"data:\" + t + \";base64,\" + v ) return source @ ( cache := __import__ ( \"functools\" ) . lru_cache ) def get_exporter () -> \"nbconvert.TemplateExporter\" : exporter = PidgyExporter () exporter . environment . filters . setdefault ( \"highlight_code\" , lambda x : x ) exporter . environment . filters . setdefault ( \"attachment\" , replace_attachments ) return exporter generating the template \u00a4 this template holds so of our own logic. TEMPLATE = \"\"\"{ %- e xtends 'display_priority.j2' -%} {% block body_footer %} {{super()}} { % s et mimetype = 'application/vnd.jupyter.widget-state+json'%} { % i f mimetype in nb.metadata.get(\"widgets\", {} )%} <script type=\"{{ mimetype }}\"> {{ nb.metadata.widgets[mimetype] | json_dumps }} </script> { % e ndif %} </script> { % e ndblock %} {% block data_markdown scoped %} {{output.data['text/markdown']}} { % e ndblock data_markdown %} {% block markdowncell scoped %} {{cell | attachment}} { % e ndblock markdowncell %} {% block input %} { % i f not resources.is_pidgy %} ``````````````````````````````````````````````````````````````python {{cell.source}} `````````````````````````````````````````````````````````````` { % e ndif %} { % e ndblock %} \"\"\" HEAD = \"\"\"{ % f rom 'base/jupyter_widgets.html.j2' import jupyter_widgets %} <script src=\"{{ resources.require_js_url }}\"></script> {{ jupyter_widgets(resources.jupyter_widgets_base_url, resources.html_manager_semver_range, resources.widget_renderer_url) }} \"\"\" we can then combine our base template with existing nbconvert ones to compute the final template. template = get_template () the generated template \u00a4 if \"__file__\" not in locals (): display ({ \"text/markdown\" : F \"``````````````````````````html+jinja \\n { template } \\n ``````````````````````````\" }, raw = True ) {% - extends 'display_priority.j2' - %} {% block body_footer %} {{ super () }} {% set mimetype = 'application/vnd.jupyter.widget-state+json' %} {% if mimetype in nb.metadata.get ( \"widgets\" ,{}) %} < script type = \" {{ mimetype }} \" > {{ nb.metadata.widgets [ mimetype ] | json_dumps }} </ script > {% endif %} </ script > {% endblock %} {% block input %} {% if resources.is_pidgy %} < div class = \"cell source\" hidden > {% endif %} ````````````````````````python {{ cell.source }} ```````````````````````` {% if resources.is_pidgy %} </ div > {% endif %} {% endblock input %} {% block data_markdown scoped %} {{ output.data [ 'text/markdown' ] }} {% endblock data_markdown %} {% block markdowncell scoped %} {{ cell | attachment }} {% endblock markdowncell %} {% block notebook_css %} {{ resources.include_css ( \"static/style.css\" ) }} < style type = \"text/css\" > /* Overrides of notebook CSS for static HTML export */ body { overflow : visible ; padding : 8 px ; } div # notebook { overflow : visible ; border-top : none ; } {% - if resources.global_content_filter.no_prompt - %} div # notebook-container { padding : 6 ex 12 ex 8 ex 12 ex ; } {% - endif - %} @ media print { body { margin : 0 ; } div . cell { display : block ; page-break-inside : avoid ; } div . output_wrapper { display : block ; page-break-inside : avoid ; } div . output { display : block ; page-break-inside : avoid ; } } </ style > {% endblock notebook_css %} {% block execute_result - %} {% - set extra_class = \"output_execute_result\" - %} {% block data_priority scoped %} {{ super () }} {% endblock data_priority %} {% - set extra_class = \"\" - %} {% - endblock execute_result %} {% block stream_stdout - %} < div class = \"output_subarea output_stream output_stdout output_text\" > < pre > {{ - output.text | ansi2html - }} </ pre > </ div > {% - endblock stream_stdout %} {% block stream_stderr - %} < div class = \"output_subarea output_stream output_stderr output_text\" > < pre > {{ - output.text | ansi2html - }} </ pre > </ div > {% - endblock stream_stderr %} {% block data_svg scoped - %} < div class = \"output_svg output_subarea {{ extra_class }} \" > {% - if output.svg_filename %} < img src = \" {{ output.svg_filename | posix_path }} \" > {% - else %} {{ output.data [ 'image/svg+xml' ] }} {% - endif %} </ div > {% - endblock data_svg %} {% block data_html scoped - %} < div class = \"output_html rendered_html output_subarea {{ extra_class }} \" > {% - if output.get ( 'metadata' , {}) .get ( 'text/html' , {}) .get ( 'isolated' ) - %} < iframe class = \"isolated-iframe\" style = \"height:520px; width:100%; margin:0; padding: 0\" frameborder = \"0\" scrolling = \"auto\" src = \"data:text/html;base64, {{ output.data [ 'text/html' ] | text_base64 }} \" > </ iframe > {% - else - %} {{ output.data [ 'text/html' ] }} {% - endif - %} </ div > {% - endblock data_html %} {% block data_png scoped %} < div class = \"output_png output_subarea {{ extra_class }} \" > {% - if 'image/png' in output.metadata.get ( 'filenames' , {}) %} < img src = \" {{ output.metadata.filenames [ 'image/png' ] | posix_path }} \" {% - else %} < img src = \"data:image/png;base64, {{ output.data [ 'image/png' ] }} \" {% - endif %} {% - set width = output | get_metadata ( 'width' , 'image/png' ) - %} {% - if width is not none %} width = {{ width }} {% - endif %} {% - set height = output | get_metadata ( 'height' , 'image/png' ) - %} {% - if height is not none %} height= {{ height }} {% - endif %} {% - if output | get_metadata ( 'unconfined' , 'image/png' ) %} class = \"unconfined\" {% - endif %} {% - set alttext =( output | get_metadata ( 'alt' , 'image/png' )) or ( cell | get_metadata ( 'alt' )) - %} {% - if alttext is not none %} alt = \" {{ alttext }} \" {% - endif %} > </ div > {% - endblock data_png %} {% block data_jpg scoped %} < div class = \"output_jpeg output_subarea {{ extra_class }} \" > {% - if 'image/jpeg' in output.metadata.get ( 'filenames' , {}) %} < img src = \" {{ output.metadata.filenames [ 'image/jpeg' ] | posix_path }} \" {% - else %} < img src = \"data:image/jpeg;base64, {{ output.data [ 'image/jpeg' ] }} \" {% - endif %} {% - set width = output | get_metadata ( 'width' , 'image/jpeg' ) - %} {% - if width is not none %} width = {{ width }} {% - endif %} {% - set height = output | get_metadata ( 'height' , 'image/jpeg' ) - %} {% - if height is not none %} height= {{ height }} {% - endif %} {% - if output | get_metadata ( 'unconfined' , 'image/jpeg' ) %} class = \"unconfined\" {% - endif %} {% - set alttext =( output | get_metadata ( 'alt' , 'image/jpeg' )) or ( cell | get_metadata ( 'alt' )) - %} {% - if alttext is not none %} alt = \" {{ alttext }} \" {% - endif %} > </ div > {% - endblock data_jpg %} {% block error - %} < div class = \"output_subarea output_text output_error\" > < pre > {{ - super () - }} </ pre > </ div > {% - endblock error %} {% - block traceback_line %} {{ line | ansi2html }} {% - endblock traceback_line %} {% - block data_widget_state scoped %} {% set div_id = uuid4 () %} {% set datatype_list = output.data | filter_data_type %} {% set datatype = datatype_list [ 0 ] %} < div id = \" {{ div_id }} \" class = \"output_subarea output_widget_state {{ extra_class }} \" > < script type = \"text/javascript\" > var element = $ ( '# {{ div_id }} ' ); </ script > < script type = \" {{ datatype }} \" > {{ output.data [ datatype ] | json_dumps }} </ script > </ div > {% - endblock data_widget_state - %} {% - block data_widget_view scoped %} {% set div_id = uuid4 () %} {% set datatype_list = output.data | filter_data_type %} {% set datatype = datatype_list [ 0 ] %} < div id = \" {{ div_id }} \" class = \"output_subarea output_widget_view {{ extra_class }} \" > < script type = \"text/javascript\" > var element = $ ( '# {{ div_id }} ' ); </ script > < script type = \" {{ datatype }} \" > {{ output.data [ datatype ] | json_dumps }} </ script > </ div > {% - endblock data_widget_view - %} {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"a template for exporting markdownish notebooks"},{"location":"xxii/2022-12-31-markdownish-notebook.html#a-template-for-exporting-markdownish-notebooks","text":"we need a combination of markdown and html templates to operate optimally with mkdocs. this document frankensteins templates and gives us some nice pixels. get_template creates our hybrid template from the initial TEMPLATE and goes on to append blocks from nbconvert templates. def get_template () -> str : blocks = __import__ ( \"collections\" ) . ChainMap ( * map ( get_blocks , \"classic/base.html.j2 classic/index.html.j2\" . split ())) template = TEMPLATE for k in KEEP : if k in blocks : template += blocks [ k ] . group ( \"block\" ) + \" \\n \" * 3 return template we KEEP some blocks from the classic templates KEEP = \"notebook_css execute_result stream_stdout stream_stderr \\ data_svg data_html data_png data_jpg error traceback_line data_widget_state data_widget_view\" . split () get_blocks loads a template and returns a dictionary of its blocks. def get_blocks ( alias ) -> dict [ str , ( T := __import__ ( \"typing\" )) . Pattern ]: template = get_exporter () . environment . get_template ( alias ) with open ( template . filename ) as file : body = file . read () return dict ( yield_blocks ( body )) yield all the blocks recursing into the found matches def yield_blocks ( string ) -> T . Iterator [ tuple [ str , T . Pattern ]]: from tonyfast.regexs import jinja_block for m in jinja_block . finditer ( string ): yield m . group ( \"name\" ), m yield from yield_blocks ( m . group ( \"inner\" )) initialize our exporter and jinja environment i want to hide the input code, but no remove it; a custom exporter felt like a better way. import nbconvert , re class PidgyExporter ( nbconvert . exporters . HTMLExporter ): def from_notebook_node ( self , nb , resources = None , ** kw ): resources = self . _init_resources ( dict ( is_pidgy = is_pidgy ( nb ))) return super () . from_notebook_node ( nb , resources , ** kw ) we moved is_pidgy and PIDGY from the mkdocs plugin because them make more self here. def is_pidgy ( nb ): for cell in nb [ \"cells\" ]: if cell [ \"cell_type\" ] == \"code\" : if PIDGY . match ( \"\" . join ( cell [ \"source\" ])): return True return False PIDGY = re . compile ( r \"\\s*%(re)?load_ext pidgy\" ) def replace_attachments ( cell ): source = \"\" . join ( cell [ \"source\" ]) if cell . get ( \"attachments\" ): for k , v in cell [ \"attachments\" ] . items (): for t , v in v . items (): source = source . replace ( \"attachment:\" + k , \"data:\" + t + \";base64,\" + v ) return source @ ( cache := __import__ ( \"functools\" ) . lru_cache ) def get_exporter () -> \"nbconvert.TemplateExporter\" : exporter = PidgyExporter () exporter . environment . filters . setdefault ( \"highlight_code\" , lambda x : x ) exporter . environment . filters . setdefault ( \"attachment\" , replace_attachments ) return exporter","title":"a template for exporting markdownish notebooks"},{"location":"xxii/2022-12-31-markdownish-notebook.html#generating-the-template","text":"this template holds so of our own logic. TEMPLATE = \"\"\"{ %- e xtends 'display_priority.j2' -%} {% block body_footer %} {{super()}} { % s et mimetype = 'application/vnd.jupyter.widget-state+json'%} { % i f mimetype in nb.metadata.get(\"widgets\", {} )%} <script type=\"{{ mimetype }}\"> {{ nb.metadata.widgets[mimetype] | json_dumps }} </script> { % e ndif %} </script> { % e ndblock %} {% block data_markdown scoped %} {{output.data['text/markdown']}} { % e ndblock data_markdown %} {% block markdowncell scoped %} {{cell | attachment}} { % e ndblock markdowncell %} {% block input %} { % i f not resources.is_pidgy %} ``````````````````````````````````````````````````````````````python {{cell.source}} `````````````````````````````````````````````````````````````` { % e ndif %} { % e ndblock %} \"\"\" HEAD = \"\"\"{ % f rom 'base/jupyter_widgets.html.j2' import jupyter_widgets %} <script src=\"{{ resources.require_js_url }}\"></script> {{ jupyter_widgets(resources.jupyter_widgets_base_url, resources.html_manager_semver_range, resources.widget_renderer_url) }} \"\"\" we can then combine our base template with existing nbconvert ones to compute the final template. template = get_template ()","title":"generating the template"},{"location":"xxii/2022-12-31-markdownish-notebook.html#the-generated-template","text":"if \"__file__\" not in locals (): display ({ \"text/markdown\" : F \"``````````````````````````html+jinja \\n { template } \\n ``````````````````````````\" }, raw = True ) {% - extends 'display_priority.j2' - %} {% block body_footer %} {{ super () }} {% set mimetype = 'application/vnd.jupyter.widget-state+json' %} {% if mimetype in nb.metadata.get ( \"widgets\" ,{}) %} < script type = \" {{ mimetype }} \" > {{ nb.metadata.widgets [ mimetype ] | json_dumps }} </ script > {% endif %} </ script > {% endblock %} {% block input %} {% if resources.is_pidgy %} < div class = \"cell source\" hidden > {% endif %} ````````````````````````python {{ cell.source }} ```````````````````````` {% if resources.is_pidgy %} </ div > {% endif %} {% endblock input %} {% block data_markdown scoped %} {{ output.data [ 'text/markdown' ] }} {% endblock data_markdown %} {% block markdowncell scoped %} {{ cell | attachment }} {% endblock markdowncell %} {% block notebook_css %} {{ resources.include_css ( \"static/style.css\" ) }} < style type = \"text/css\" > /* Overrides of notebook CSS for static HTML export */ body { overflow : visible ; padding : 8 px ; } div # notebook { overflow : visible ; border-top : none ; } {% - if resources.global_content_filter.no_prompt - %} div # notebook-container { padding : 6 ex 12 ex 8 ex 12 ex ; } {% - endif - %} @ media print { body { margin : 0 ; } div . cell { display : block ; page-break-inside : avoid ; } div . output_wrapper { display : block ; page-break-inside : avoid ; } div . output { display : block ; page-break-inside : avoid ; } } </ style > {% endblock notebook_css %} {% block execute_result - %} {% - set extra_class = \"output_execute_result\" - %} {% block data_priority scoped %} {{ super () }} {% endblock data_priority %} {% - set extra_class = \"\" - %} {% - endblock execute_result %} {% block stream_stdout - %} < div class = \"output_subarea output_stream output_stdout output_text\" > < pre > {{ - output.text | ansi2html - }} </ pre > </ div > {% - endblock stream_stdout %} {% block stream_stderr - %} < div class = \"output_subarea output_stream output_stderr output_text\" > < pre > {{ - output.text | ansi2html - }} </ pre > </ div > {% - endblock stream_stderr %} {% block data_svg scoped - %} < div class = \"output_svg output_subarea {{ extra_class }} \" > {% - if output.svg_filename %} < img src = \" {{ output.svg_filename | posix_path }} \" > {% - else %} {{ output.data [ 'image/svg+xml' ] }} {% - endif %} </ div > {% - endblock data_svg %} {% block data_html scoped - %} < div class = \"output_html rendered_html output_subarea {{ extra_class }} \" > {% - if output.get ( 'metadata' , {}) .get ( 'text/html' , {}) .get ( 'isolated' ) - %} < iframe class = \"isolated-iframe\" style = \"height:520px; width:100%; margin:0; padding: 0\" frameborder = \"0\" scrolling = \"auto\" src = \"data:text/html;base64, {{ output.data [ 'text/html' ] | text_base64 }} \" > </ iframe > {% - else - %} {{ output.data [ 'text/html' ] }} {% - endif - %} </ div > {% - endblock data_html %} {% block data_png scoped %} < div class = \"output_png output_subarea {{ extra_class }} \" > {% - if 'image/png' in output.metadata.get ( 'filenames' , {}) %} < img src = \" {{ output.metadata.filenames [ 'image/png' ] | posix_path }} \" {% - else %} < img src = \"data:image/png;base64, {{ output.data [ 'image/png' ] }} \" {% - endif %} {% - set width = output | get_metadata ( 'width' , 'image/png' ) - %} {% - if width is not none %} width = {{ width }} {% - endif %} {% - set height = output | get_metadata ( 'height' , 'image/png' ) - %} {% - if height is not none %} height= {{ height }} {% - endif %} {% - if output | get_metadata ( 'unconfined' , 'image/png' ) %} class = \"unconfined\" {% - endif %} {% - set alttext =( output | get_metadata ( 'alt' , 'image/png' )) or ( cell | get_metadata ( 'alt' )) - %} {% - if alttext is not none %} alt = \" {{ alttext }} \" {% - endif %} > </ div > {% - endblock data_png %} {% block data_jpg scoped %} < div class = \"output_jpeg output_subarea {{ extra_class }} \" > {% - if 'image/jpeg' in output.metadata.get ( 'filenames' , {}) %} < img src = \" {{ output.metadata.filenames [ 'image/jpeg' ] | posix_path }} \" {% - else %} < img src = \"data:image/jpeg;base64, {{ output.data [ 'image/jpeg' ] }} \" {% - endif %} {% - set width = output | get_metadata ( 'width' , 'image/jpeg' ) - %} {% - if width is not none %} width = {{ width }} {% - endif %} {% - set height = output | get_metadata ( 'height' , 'image/jpeg' ) - %} {% - if height is not none %} height= {{ height }} {% - endif %} {% - if output | get_metadata ( 'unconfined' , 'image/jpeg' ) %} class = \"unconfined\" {% - endif %} {% - set alttext =( output | get_metadata ( 'alt' , 'image/jpeg' )) or ( cell | get_metadata ( 'alt' )) - %} {% - if alttext is not none %} alt = \" {{ alttext }} \" {% - endif %} > </ div > {% - endblock data_jpg %} {% block error - %} < div class = \"output_subarea output_text output_error\" > < pre > {{ - super () - }} </ pre > </ div > {% - endblock error %} {% - block traceback_line %} {{ line | ansi2html }} {% - endblock traceback_line %} {% - block data_widget_state scoped %} {% set div_id = uuid4 () %} {% set datatype_list = output.data | filter_data_type %} {% set datatype = datatype_list [ 0 ] %} < div id = \" {{ div_id }} \" class = \"output_subarea output_widget_state {{ extra_class }} \" > < script type = \"text/javascript\" > var element = $ ( '# {{ div_id }} ' ); </ script > < script type = \" {{ datatype }} \" > {{ output.data [ datatype ] | json_dumps }} </ script > </ div > {% - endblock data_widget_state - %} {% - block data_widget_view scoped %} {% set div_id = uuid4 () %} {% set datatype_list = output.data | filter_data_type %} {% set datatype = datatype_list [ 0 ] %} < div id = \" {{ div_id }} \" class = \"output_subarea output_widget_view {{ extra_class }} \" > < script type = \"text/javascript\" > var element = $ ( '# {{ div_id }} ' ); </ script > < script type = \" {{ datatype }} \" > {{ output.data [ datatype ] | json_dumps }} </ script > </ div > {% - endblock data_widget_view - %} {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"the generated template"},{"location":"xxii/oct/2022-10-05-dask-search.html","text":"searching notebooks \u00a4 sorry haters. notebooks are here to stay. their growth and adoption means that they'll present newer problems. one forthcoming challenge with notebooks and their adoption is the ability to search notebooks across space and time. in this notebook, we build tooling to search notebooks and think about the question we might ask to our notebooks. Searching notebooks as structured data. What questions would you ask? today? a year from today? a lifetime from today? notebook schema \u00a4 one of the reasons we can search notebooks is their consistent structure defined by the nbformat SCHEMA . the schema provides both a description of the document format along with type information about the notebook data. import nbformat.v4 , jsonref , IPython.display as SOME COMPACT = nbformat . validator . _get_schema_json ( nbformat . v4 ) the COMPACT should be expanded to allow for easier access the components of the schema. if we don't that we need to really on the implicit structure of the schema document. SCHEMA = jsonref . JsonRef . replace_refs ( COMPACT ) SOME . JSON ( SCHEMA , root = SCHEMA [ \"description\" ]); for this demonstration we are going to avoid anything dealing with the top level metadata. our goal is to explore the contents of cells and think about the questions we may ask on the cell sources and outputs. cell schema \u00a4 below we extra the expected CELL keys from the SCHEMA CELL , CELLS = SCHEMA [ \"properties\" ][ \"cells\" ], { \"nid\" } for s in CELL [ \"items\" ][ \"oneOf\" ]: CELLS . update ( s . get ( \"properties\" , \"\" )) CELLS = sorted ( CELLS ) CELLS_META_EXPLICIT = dict ( execution_count = \"float64\" , nid = int , cell_type = \"category\" ) CELLS_META = tuple (( k , CELLS_META_EXPLICIT . get ( k , \"object\" )) for k in CELLS ) F \"the expected cell keys are { CELLS } \" loading our notebook data. \u00a4 we're going to use dask to accelerate our efforts. dask will help us looking across files in a fast way, and we can speak dataframes natively. import dask.dataframe , pandas , jsonref , json ; from dask import delayed ; from pathlib import Path our dataframe is going to be constructed from a bunch of parallel files reads. each file is passed through get_cell to return a pandas.DataFrame . def get_cell ( path ): with open ( path ) as file : if str ( path ) . endswith (( \".ipynb\" ,)): cells = json . load ( file )[ \"cells\" ] elif str ( path ) . endswith (( \".md\" ,)): cells = dict ( metadata = {}, cells = [ dict ( cell_type = \"markdown\" , source = \"\" . join ( file ) )]) df = pandas . DataFrame ( cells ) df . index . name = \"nid\" df = df . reset_index ( \"nid\" ) if \"source\" not in df : df = pandas . DataFrame ( columns = CELLS ) else : df . execution_count = df . execution_count . fillna ( - 1 ) # -1 is outside the valid schema, but we don't validate here! df . source = df . source . apply ( \"\" . join ) df . index = [ path ] * len ( df ) df . index . name = \"path\" for k , _ in CELLS_META : if k not in df . columns : df [ k ] = None df [ k ] = df [ k ] . astype ( \"O\" ) return df [ CELLS ] get_cells loads, tidies, and separates cells, outputs and metadata def get_delayeds ( dir , recursive = False ): dir = Path ( dir ) files = ( recursive and dir . rglob or dir . glob )( \"*.ipynb\" ) return dask . dataframe . from_delayed ( list ( map ( delayed ( get_cell ), files )) ) def get_cells ( dir = None , recursive = False ): return get_delayeds ( dir or Path . cwd (), recursive ) . pipe ( lambda df : ( df , df . pop ( \"outputs\" ), df . pop ( \"metadata\" )) ) L = \"__file__\" not in locals () print ( L ) if L : cells , outputs , metadata = get_cells ( \"../..\" ); display ( cells ) True Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } attachments cell_type execution_count id nid source npartitions=8 object object object object object object ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Dask Name: drop_by_shallow_copy, 32 tasks find cells with imports in them if L : cells [ cells . source . str . match ( \"\\s*import\\s+.*\" )] . compute () . T . pipe ( display ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } path ../../2022-06-28-.ipynb ../../2022-06-24-.ipynb ../../2022-03-06-schemata-scratch.ipynb ../../2022-03-06-schemata-scratch.ipynb attachments None None None None cell_type code code code code execution_count 9.0 2.0 7.0 6.0 id c744325a-8f73-4428-b381-c1f4ee5fdb06 ba46c5ef-78e6-48b2-bee3-cd6be3606fe5 ab6ff11f-a7d1-4e59-9fdd-cac7313a7cf4 5d3d2316-b2b0-4f47-ad0d-9b677b1f7e6a nid 6 1 52 126 source \\n import graphviz\\n hommage = graph... import functools, abc import sys import urllib find some urls? if L : cells . source . str . extract ( \"(http[s]://\\S+)\" ) . dropna () . compute () . T . pipe ( display ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } path ../../2022-06-28-.ipynb ../../2022-06-28-.ipynb ../../2022-06-28-.ipynb ../../2022-06-28-.ipynb ../../2022-06-28-.ipynb ../../2022-04-12-.ipynb ../../2022-04-12-.ipynb ../../2022-03-06-schemata-scratch.ipynb ../../2022-03-06-schemata-scratch.ipynb ../../2022-03-06-schemata-scratch.ipynb ../../2022-03-06-schemata-scratch.ipynb ../../2022-03-06-schemata-scratch.ipynb 0 https://raw.githubusercontent.com/SchemaStore/... https://joss.theoj.org/papers/in/Jupyter%20Not... https://raw.githubusercontent.com/SchemaStore/... https://raw.githubusercontent.com/jupyter/nbfo... https://c.tenor.com/JHjG5vxW9zIAAAAd/missy-ell... https://github.com/jupyterlab/lumino\", https://github.com/jupyterlab/jupyterlab@master\", https://json-schema.org/draft/next/meta/valida... https://json-schema.org/draft/2020-12/schema'] https://json-schema.org/draft/2020-12/schema\"] https://test.json-schema.org/dynamic-resolutio... https://avatars.githubusercontent.com/u/423627... break \u00a4 what storage integrate with contents manager what queries working on this notebook revealed an issue with importnb's json parser than needs some care. this document is code and can be used with the statement from tonyfast import search","title":"searching notebooks"},{"location":"xxii/oct/2022-10-05-dask-search.html#searching-notebooks","text":"sorry haters. notebooks are here to stay. their growth and adoption means that they'll present newer problems. one forthcoming challenge with notebooks and their adoption is the ability to search notebooks across space and time. in this notebook, we build tooling to search notebooks and think about the question we might ask to our notebooks. Searching notebooks as structured data. What questions would you ask? today? a year from today? a lifetime from today?","title":"searching notebooks"},{"location":"xxii/oct/2022-10-05-dask-search.html#notebook-schema","text":"one of the reasons we can search notebooks is their consistent structure defined by the nbformat SCHEMA . the schema provides both a description of the document format along with type information about the notebook data. import nbformat.v4 , jsonref , IPython.display as SOME COMPACT = nbformat . validator . _get_schema_json ( nbformat . v4 ) the COMPACT should be expanded to allow for easier access the components of the schema. if we don't that we need to really on the implicit structure of the schema document. SCHEMA = jsonref . JsonRef . replace_refs ( COMPACT ) SOME . JSON ( SCHEMA , root = SCHEMA [ \"description\" ]); for this demonstration we are going to avoid anything dealing with the top level metadata. our goal is to explore the contents of cells and think about the questions we may ask on the cell sources and outputs.","title":"notebook schema"},{"location":"xxii/oct/2022-10-05-dask-search.html#cell-schema","text":"below we extra the expected CELL keys from the SCHEMA CELL , CELLS = SCHEMA [ \"properties\" ][ \"cells\" ], { \"nid\" } for s in CELL [ \"items\" ][ \"oneOf\" ]: CELLS . update ( s . get ( \"properties\" , \"\" )) CELLS = sorted ( CELLS ) CELLS_META_EXPLICIT = dict ( execution_count = \"float64\" , nid = int , cell_type = \"category\" ) CELLS_META = tuple (( k , CELLS_META_EXPLICIT . get ( k , \"object\" )) for k in CELLS ) F \"the expected cell keys are { CELLS } \"","title":"cell schema"},{"location":"xxii/oct/2022-10-05-dask-search.html#loading-our-notebook-data","text":"we're going to use dask to accelerate our efforts. dask will help us looking across files in a fast way, and we can speak dataframes natively. import dask.dataframe , pandas , jsonref , json ; from dask import delayed ; from pathlib import Path our dataframe is going to be constructed from a bunch of parallel files reads. each file is passed through get_cell to return a pandas.DataFrame . def get_cell ( path ): with open ( path ) as file : if str ( path ) . endswith (( \".ipynb\" ,)): cells = json . load ( file )[ \"cells\" ] elif str ( path ) . endswith (( \".md\" ,)): cells = dict ( metadata = {}, cells = [ dict ( cell_type = \"markdown\" , source = \"\" . join ( file ) )]) df = pandas . DataFrame ( cells ) df . index . name = \"nid\" df = df . reset_index ( \"nid\" ) if \"source\" not in df : df = pandas . DataFrame ( columns = CELLS ) else : df . execution_count = df . execution_count . fillna ( - 1 ) # -1 is outside the valid schema, but we don't validate here! df . source = df . source . apply ( \"\" . join ) df . index = [ path ] * len ( df ) df . index . name = \"path\" for k , _ in CELLS_META : if k not in df . columns : df [ k ] = None df [ k ] = df [ k ] . astype ( \"O\" ) return df [ CELLS ] get_cells loads, tidies, and separates cells, outputs and metadata def get_delayeds ( dir , recursive = False ): dir = Path ( dir ) files = ( recursive and dir . rglob or dir . glob )( \"*.ipynb\" ) return dask . dataframe . from_delayed ( list ( map ( delayed ( get_cell ), files )) ) def get_cells ( dir = None , recursive = False ): return get_delayeds ( dir or Path . cwd (), recursive ) . pipe ( lambda df : ( df , df . pop ( \"outputs\" ), df . pop ( \"metadata\" )) ) L = \"__file__\" not in locals () print ( L ) if L : cells , outputs , metadata = get_cells ( \"../..\" ); display ( cells ) True Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } attachments cell_type execution_count id nid source npartitions=8 object object object object object object ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Dask Name: drop_by_shallow_copy, 32 tasks find cells with imports in them if L : cells [ cells . source . str . match ( \"\\s*import\\s+.*\" )] . compute () . T . pipe ( display ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } path ../../2022-06-28-.ipynb ../../2022-06-24-.ipynb ../../2022-03-06-schemata-scratch.ipynb ../../2022-03-06-schemata-scratch.ipynb attachments None None None None cell_type code code code code execution_count 9.0 2.0 7.0 6.0 id c744325a-8f73-4428-b381-c1f4ee5fdb06 ba46c5ef-78e6-48b2-bee3-cd6be3606fe5 ab6ff11f-a7d1-4e59-9fdd-cac7313a7cf4 5d3d2316-b2b0-4f47-ad0d-9b677b1f7e6a nid 6 1 52 126 source \\n import graphviz\\n hommage = graph... import functools, abc import sys import urllib find some urls? if L : cells . source . str . extract ( \"(http[s]://\\S+)\" ) . dropna () . compute () . T . pipe ( display ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } path ../../2022-06-28-.ipynb ../../2022-06-28-.ipynb ../../2022-06-28-.ipynb ../../2022-06-28-.ipynb ../../2022-06-28-.ipynb ../../2022-04-12-.ipynb ../../2022-04-12-.ipynb ../../2022-03-06-schemata-scratch.ipynb ../../2022-03-06-schemata-scratch.ipynb ../../2022-03-06-schemata-scratch.ipynb ../../2022-03-06-schemata-scratch.ipynb ../../2022-03-06-schemata-scratch.ipynb 0 https://raw.githubusercontent.com/SchemaStore/... https://joss.theoj.org/papers/in/Jupyter%20Not... https://raw.githubusercontent.com/SchemaStore/... https://raw.githubusercontent.com/jupyter/nbfo... https://c.tenor.com/JHjG5vxW9zIAAAAd/missy-ell... https://github.com/jupyterlab/lumino\", https://github.com/jupyterlab/jupyterlab@master\", https://json-schema.org/draft/next/meta/valida... https://json-schema.org/draft/2020-12/schema'] https://json-schema.org/draft/2020-12/schema\"] https://test.json-schema.org/dynamic-resolutio... https://avatars.githubusercontent.com/u/423627...","title":"loading our notebook data."},{"location":"xxii/oct/2022-10-05-dask-search.html#break","text":"what storage integrate with contents manager what queries working on this notebook revealed an issue with importnb's json parser than needs some care. this document is code and can be used with the statement from tonyfast import search","title":"break"},{"location":"xxii/oct/2022-10-19-mobius-text.html","text":"turn some text into a mobius surface \u00a4 a good reference was: https://algebra-fun.gitee.io/blog/2020/06/16/Joy-%E7%BB%98%E5%88%B6Mobius/ if __import__ ( \"sys\" ) . platform == \"emscripten\" : await __import__ ( \"micropip\" ) . install ( \"pandas matplotlib ipympl\" . split ()) import pandas , matplotlib , io , numpy from mpl_toolkits.mplot3d import Axes3D % matplotlib agg get_text transforms some text into a dataframe that will allow us to plot a mobius strip. def get_text ( text = \"deathbeds\" , repeat = 2 ): return get_text_array ( get_text_figure ( text , repeat ) ) . pipe ( get_parameterized_text ) . pipe ( get_xyz ) get_text_figure transforms text to pixels. def get_text_figure ( text = \"deathbeds\" , repeat = 1 ): matplotlib . pyplot . gca () . text ( 0 , 0 , text * repeat , size = 1000 ) matplotlib . pyplot . gca () . axis ( \"off\" ) fig = matplotlib . pyplot . gcf () return fig get_text_array structure those pixels as dataframe def get_text_array ( fig ): data = io . BytesIO () fig . savefig ( data , bbox_inches = \"tight\" ) data . seek ( 0 ) where = numpy . where ( matplotlib . pyplot . imread ( data ) . sum ( 2 )[:: - 4 , :: 20 ] < 2 ) return pandas . concat ( [ pandas . Series ( where [ 0 ], name = \"x\" ), pandas . Series ( where [ 1 ], name = \"y\" )], axis = 1 ) . set_index ( \"x\" ) get_parameterized_text parameterizes x onto 0..1 and s one 0..2\u03c0 def get_parameterized_text ( df ): extent = df . index . max () - df . index . min () df [ \"t\" ] = df . index . to_series () df . t = df . t . sub ( df . t . min ()) . div ( extent ) . sub ( .5 ) extent = df . y . max () - df . y . min () df [ \"s\" ] = df . y . add ( df . y . min ()) . div ( extent ) . mul ( 2 * numpy . pi ) return df get_xyz moves the parameterization in xyz space. def get_xyz ( df , R = 100 , W = 50 ): return df . assign ( x = ( R + W * df . t * df . s . div ( 2 ) . apply ( numpy . cos )) * df . s . apply ( numpy . cos ), y = ( R + W * df . t * df . s . div ( 2 ) . apply ( numpy . cos )) * df . s . apply ( numpy . sin ), z = W * df . t . mul ( df . s . div ( 2 ) . apply ( numpy . sin )), ) get_plot takes our structured data and plots in 3d. def get_plot ( df ): fig = matplotlib . pyplot . gcf () ax = Axes3D ( fig , auto_add_to_figure = False ) fig . add_axes ( ax ) ax . scatter3D ( df . x , df . y , df . z , c = [( .1 , .1 , .1 , .015 )], marker = \"v\" , s = 200 , edgecolor = None ) return fig use the interactive 3-d dispklay % matplotlib ipympl plot it all matplotlib . pyplot . gcf () . set_size_inches (( 6 , 6 )) get_text ( \"deathbeds\" ) . pipe ( get_plot ) Figure","title":"turn some text into a mobius surface"},{"location":"xxii/oct/2022-10-19-mobius-text.html#turn-some-text-into-a-mobius-surface","text":"a good reference was: https://algebra-fun.gitee.io/blog/2020/06/16/Joy-%E7%BB%98%E5%88%B6Mobius/ if __import__ ( \"sys\" ) . platform == \"emscripten\" : await __import__ ( \"micropip\" ) . install ( \"pandas matplotlib ipympl\" . split ()) import pandas , matplotlib , io , numpy from mpl_toolkits.mplot3d import Axes3D % matplotlib agg get_text transforms some text into a dataframe that will allow us to plot a mobius strip. def get_text ( text = \"deathbeds\" , repeat = 2 ): return get_text_array ( get_text_figure ( text , repeat ) ) . pipe ( get_parameterized_text ) . pipe ( get_xyz ) get_text_figure transforms text to pixels. def get_text_figure ( text = \"deathbeds\" , repeat = 1 ): matplotlib . pyplot . gca () . text ( 0 , 0 , text * repeat , size = 1000 ) matplotlib . pyplot . gca () . axis ( \"off\" ) fig = matplotlib . pyplot . gcf () return fig get_text_array structure those pixels as dataframe def get_text_array ( fig ): data = io . BytesIO () fig . savefig ( data , bbox_inches = \"tight\" ) data . seek ( 0 ) where = numpy . where ( matplotlib . pyplot . imread ( data ) . sum ( 2 )[:: - 4 , :: 20 ] < 2 ) return pandas . concat ( [ pandas . Series ( where [ 0 ], name = \"x\" ), pandas . Series ( where [ 1 ], name = \"y\" )], axis = 1 ) . set_index ( \"x\" ) get_parameterized_text parameterizes x onto 0..1 and s one 0..2\u03c0 def get_parameterized_text ( df ): extent = df . index . max () - df . index . min () df [ \"t\" ] = df . index . to_series () df . t = df . t . sub ( df . t . min ()) . div ( extent ) . sub ( .5 ) extent = df . y . max () - df . y . min () df [ \"s\" ] = df . y . add ( df . y . min ()) . div ( extent ) . mul ( 2 * numpy . pi ) return df get_xyz moves the parameterization in xyz space. def get_xyz ( df , R = 100 , W = 50 ): return df . assign ( x = ( R + W * df . t * df . s . div ( 2 ) . apply ( numpy . cos )) * df . s . apply ( numpy . cos ), y = ( R + W * df . t * df . s . div ( 2 ) . apply ( numpy . cos )) * df . s . apply ( numpy . sin ), z = W * df . t . mul ( df . s . div ( 2 ) . apply ( numpy . sin )), ) get_plot takes our structured data and plots in 3d. def get_plot ( df ): fig = matplotlib . pyplot . gcf () ax = Axes3D ( fig , auto_add_to_figure = False ) fig . add_axes ( ax ) ax . scatter3D ( df . x , df . y , df . z , c = [( .1 , .1 , .1 , .015 )], marker = \"v\" , s = 200 , edgecolor = None ) return fig use the interactive 3-d dispklay % matplotlib ipympl plot it all matplotlib . pyplot . gcf () . set_size_inches (( 6 , 6 )) get_text ( \"deathbeds\" ) . pipe ( get_plot ) Figure","title":"turn some text into a mobius surface"},{"location":"xxii/oct/2022-10-20-meeting-link-widgets.html","text":"making meeting links \u00a4 i need to be able to make meetings with folks. those meetings will often contain: some content real time collaboration video chat the notebooks builds a link for a video chat and real collaboration. this system feels pretty de-platformated using jupyterlab s native rtc and jitsi video chat. ping me on twitter if you want a tour sometime. in the future we can build: binder links, icalendar events. try : import ipywidgets , icalendar , dataclasses , uritemplate , traitlets , arrow except : await __import__ ( \"micropip\" ) . install ( \"ipywidgets icalendar dataclasses uritemplate traitlets arrow\" . split ()) import ipywidgets , icalendar , dataclasses , uritemplate , traitlets , arrow from functools import partial ; from ipywidgets import * video = Text ( \"in-this-house-we-wear-sweatpants\" , description = \"\ud83d\udcf9 video chat\" ) rtc = Text ( \"exquisite-potluck\" , description = \"\ud83e\udd1d real time collaboration\" ) document = Text ( \"\" , description = \"\ud83d\udcc4 file path\" ) input = list ( zip ( \"JVC-PUBLIC room path\" . split (), [ video , rtc , document ])) lite_url = HTML ( \"\" , description = \"\ud83d\udca1 lite\" ) update_lite_url formats the input widgets using a uri template. def update_lite_url ( * \u0394 , lite_template = \"https://tonyfast.github.io/tonyfast/run/lab{?room,JVC-PUBLIC,path}\" ): url = uritemplate . URITemplate ( lite_template ) . expand ( dict ( ( k , v . value ) for k , v in input if v . value )) lite_url . value = F \"\"\"<a href=\" { url } \"> { url } </a>\"\"\" initialize the widget interactions for k , widget in input : widget . observe ( update_lite_url , \"value\" ) update_lite_url () make the app and start tinkering with it. app = VBox ([ x [ 1 ] for x in input ] + [ lite_url ]); app i suspect i'll reuse this document to meet with folks, and build some more tooling for it.","title":"making meeting links"},{"location":"xxii/oct/2022-10-20-meeting-link-widgets.html#making-meeting-links","text":"i need to be able to make meetings with folks. those meetings will often contain: some content real time collaboration video chat the notebooks builds a link for a video chat and real collaboration. this system feels pretty de-platformated using jupyterlab s native rtc and jitsi video chat. ping me on twitter if you want a tour sometime. in the future we can build: binder links, icalendar events. try : import ipywidgets , icalendar , dataclasses , uritemplate , traitlets , arrow except : await __import__ ( \"micropip\" ) . install ( \"ipywidgets icalendar dataclasses uritemplate traitlets arrow\" . split ()) import ipywidgets , icalendar , dataclasses , uritemplate , traitlets , arrow from functools import partial ; from ipywidgets import * video = Text ( \"in-this-house-we-wear-sweatpants\" , description = \"\ud83d\udcf9 video chat\" ) rtc = Text ( \"exquisite-potluck\" , description = \"\ud83e\udd1d real time collaboration\" ) document = Text ( \"\" , description = \"\ud83d\udcc4 file path\" ) input = list ( zip ( \"JVC-PUBLIC room path\" . split (), [ video , rtc , document ])) lite_url = HTML ( \"\" , description = \"\ud83d\udca1 lite\" ) update_lite_url formats the input widgets using a uri template. def update_lite_url ( * \u0394 , lite_template = \"https://tonyfast.github.io/tonyfast/run/lab{?room,JVC-PUBLIC,path}\" ): url = uritemplate . URITemplate ( lite_template ) . expand ( dict ( ( k , v . value ) for k , v in input if v . value )) lite_url . value = F \"\"\"<a href=\" { url } \"> { url } </a>\"\"\" initialize the widget interactions for k , widget in input : widget . observe ( update_lite_url , \"value\" ) update_lite_url () make the app and start tinkering with it. app = VBox ([ x [ 1 ] for x in input ] + [ lite_url ]); app i suspect i'll reuse this document to meet with folks, and build some more tooling for it.","title":"making meeting links"},{"location":"xxii/oct/2022-10-21-markdown-future.html","text":"when markdown and python collide \u00a4 a story about literacy written in the literate programming style of pidgy notebooks commonly communicate with two languages: markdown & python. what happen when we dissolve the boundaries between markdown & python or language & code entirely. class STATE : NB = \"__file__\" not in globals () and __name__ == \"__main__\" SCRIPT = \"__file__\" in globals () and __name__ != \"__main__\" MAIN = \"__file__\" in globals () and __name__ == \"__main__\" LITE = __import__ ( \"sys\" ) . platform == \"emscripten\" # execute this code in lite. if STATE.LITE: try: %reload_ext pidgy except ModuleNotFoundError: import micropip await micropip.install(\"pidgy importnb qrcode midgy ipywidgets\".split(), pre=True) import pidgy , pathlib , IPython shell = IPython . get_ipython () if STATE . NB : % reload_ext pidgy shell . displays_manager . template_cls = pidgy . weave . IPythonHtml import pidgy, pathlib, IPython shell = IPython.get_ipython() if STATE.NB: %reload_ext pidgy shell.displays_manager.template_cls = pidgy.weave.IPythonHtml < style > img { height : 500 px ! important ; } . jupyter - wrapper . jp - Cell - inputWrapper { display : none ; } </ style > < script src = \"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\" ></ script > < script > mermaid . initialize ({ startOnLoad : true , securityLevel : \"loose\" }, \".mermaid\" ); </ script > img { height: 500px !important; } .jupyter-wrapper .jp-Cell-inputWrapper { display: none; } mermaid.initialize({startOnLoad:true, securityLevel: \"loose\"}, \".mermaid\"); ## literate computing with literary machines using literate programming for mass computational literacy literacy rings through the history of computer science . in Annette Vee 's [Understanding Computer Programming as a Literacy] we get a high level view of literacy through history and in the our little microcosm of computer programming . > We compare mass ability to read and write software with mass literacy , and predict equally pervasive changes to society . history = \\ ``` mermaid gantt title Literacy in computer programming dateFormat YYYY axisFormat % Y section moderninity understanding media : mcluhan , 1964 , 2022 mother of all demos : englebert , 1968 , 2022 section post moderninity literate programming : knuth , 1984 , 2022 literary machines : nelson , 1987 , 2022 www : tbl , 1989 , 2022 section post web computer programming for everybody : cp4e , 1999 , 2022 ipython : fperez , 2001 , 2022 markdown : gruber , 2004 , 2022 ipython notebooks : ipynb , 2011 , 2022 anaconda : ana , 2012 , 2022 literate computing : lc , 2013 , 2022 understanding computer programming as a literacy : vee , 2013 , 2022 the birth & death of javascript : js , 2014 , 2022 big split : split , 2015 , 2022 jupyterlab : lab , 2018 , 2022 jupyterlite : lab , 2021 , 2022 click vee href \"https://licsjournal.org/index.php/LiCS/article/view/794/608\" click knuth href \"http://www.literateprogramming.com/knuthweb.pdf\" click nelson href \"https://monoskop.org/images/b/be/Nelson_Ted_Literary_Machines_c1987_chs_0-1.pdf\" click lc href \"https://web.archive.org/web/20220510083647/http://blog.fperez.org/2013/04/literate-computing-and-computational.html\" click cp4e href \"https://www.python.org/doc/essays/cp4e/\" ``` shell . displays_manager . template_cls = pidgy . weave . IPythonMarkdown literate computing with literary machines using literate programming for mass computational literacy \u00a4 literacy rings through the history of computer science. in Annette Vee's [Understanding Computer Programming as a Literacy] we get a high level view of literacy through history and in the our little microcosm of computer programming. We compare mass ability to read and write software with mass literacy, and predict equally pervasive changes to society. history =\\ mermaid gantt title Literacy in computer programming dateFormat YYYY axisFormat %Y section moderninity understanding media :mcluhan, 1964, 2022 mother of all demos :englebert, 1968, 2022 section post moderninity literate programming :knuth, 1984, 2022 literary machines :nelson, 1987, 2022 www :tbl, 1989, 2022 section post web computer programming for everybody :cp4e, 1999, 2022 ipython :fperez, 2001, 2022 markdown :gruber, 2004, 2022 ipython notebooks :ipynb, 2011, 2022 anaconda :ana, 2012, 2022 literate computing :lc, 2013, 2022 understanding computer programming as a literacy :vee, 2013, 2022 the birth & death of javascript: js, 2014, 2022 big split :split, 2015, 2022 jupyterlab :lab, 2018, 2022 jupyterlite :lab, 2021, 2022 click vee href \"https://licsjournal.org/index.php/LiCS/article/view/794/608\" click knuth href \"http://www.literateprogramming.com/knuthweb.pdf\" click nelson href \"https://monoskop.org/images/b/be/Nelson_Ted_Literary_Machines_c1987_chs_0-1.pdf\" click lc href \"https://web.archive.org/web/20220510083647/http://blog.fperez.org/2013/04/literate-computing-and-computational.html\" click cp4e href \"https://www.python.org/doc/essays/cp4e/\" shell.displays_manager.template_cls = pidgy.weave.IPythonMarkdown donald knuth's diagram for literate programming \u00a4 # tangle (code) and weave (display) knit = \\ ``` mermaid flowchart LR web -- tangle --- pas pas -- pascal --- rel web -- weave --- tex tex -- TEX --- dvi ``` tangle (code) and weave (display) \u00a4 knit =\\ mermaid flowchart LR web-- tangle ---pas pas-- pascal ---rel web-- weave ---tex tex-- TEX ---dvi ## `midgy/pidgy` family of input kernels in `IPython` literate inputs with markdown and python as the document and programming languages < div style = \"font-size: 2rem;\" > import midgy as tangle , pidgy as weave </ div > we 'll explore a little program in markdown inside a bigger notebook program. hold tight. midgy/pidgy family of input kernels in IPython \u00a4 literate inputs with markdown and python as the document and programming languages import midgy as tangle, pidgy as weave we'll explore a little program in markdown inside a bigger notebook program. hold tight. ### how does `IPython` python? * ` shell . kernel . do_execute ` * ` shell . run_cell ` * tangle * ` shell . transform_cell ` * ` shell . compile . ast_parse ` * ` shell . transform_ast ` * ` shell . run_ast_nodes ` * weave * if ` shell . ast_node_interactivity ` * ` shell . display_formatter . format ` ` IPython ` is very hackable ! that is what makes it fun along with all the development happening in python . how does IPython python? \u00a4 shell.kernel.do_execute shell.run_cell tangle shell.transform_cell shell.compile.ast_parse shell.transform_ast shell.run_ast_nodes weave if shell.ast_node_interactivity shell.display_formatter.format IPython is very hackable! that is what makes it fun along with all the development happening in python. %% file a - little - markdown - program . md this markdown file is a program ! \u03b4\u03f5\u03b1\u03c4\u03ba = \\ death to pseudo code , long live pseudo code if __name__ == \"__main__\" : print ( \u03b4\u03f5\u03b1\u03c4\u03ba . upper ()) Overwriting a-little-markdown-program.md we can run this like a script. WTF! !midgy a-little-markdown-program.md import midgy.loader with midgy . loader . Markdown () as loader : import a_little_markdown_program a_little_markdown_program , a_little_markdown_program . \u03b4\u03f5\u03b1\u03c4\u03ba import midgy.loader with midgy.loader.Markdown() as loader: import a_little_markdown_program a_little_markdown_program, a_little_markdown_program.\u03b4\u03f5\u03b1\u03c4\u03ba ` a_little_markdown_program ` is a module like any other ` {{ a_little_markdown_program }} ` \ud83d\udc07 and we even generated a compiled version of the markdown source \ud83c\udfa9 if not STATE . LITE : assert next ( ( pathlib . Path ( a_little_markdown_program . __file__ ) . parent / \"__pycache__\" ) . glob ( \"a-little-markdown-program.*.pyc\" ) ), \\ verifies there is a compiled file . a_little_markdown_program is a module like any other <module 'a-little-markdown-program' from '/home/tbone/Documents/tonyfast/tonyfast/xxii/oct/a-little-markdown-program.md'> \ud83d\udc07and we even generated a compiled version of the markdown source\ud83c\udfa9 if not STATE.LITE: assert next( (pathlib.Path(a_little_markdown_program.__file__).parent / \"__pycache__\").glob(\"a-little-markdown-program.*.pyc\") ),\\ verifies there is a compiled file. ## [literate computing widgets and `IPython` interactive displays](2022-10-21-pidgy-displays.ipynb) literate computing widgets and IPython interactive displays \u00a4 ## reusing and importing notebooks we can import this presentation . import midgy.loader with midgy . loader . Markdown ( extensions = [ \".ipynb\" , \".md\" ]): import _022_10_21_markdown_future \"__file__\" not in locals () and display ( _022_10_21_markdown_future , _022_10_21_markdown_future . a_little_markdown_program , _022_10_21_markdown_future . a_little_markdown_program . \u03b4\u03f5\u03b1\u03c4\u03ba ) Overwriting a-little-markdown-program.md gantt title Literacy in computer programming dateFormat YYYY axisFormat %Y section moderninity understanding media :mcluhan, 1964, 2022 mother of all demos :englebert, 1968, 2022 section post moderninity literate programming :knuth, 1984, 2022 literary machines :nelson, 1987, 2022 www :tbl, 1989, 2022 section post web computer programming for everybody :cp4e, 1999, 2022 ipython :fperez, 2001, 2022 markdown :gruber, 2004, 2022 ipython notebooks :ipynb, 2011, 2022 anaconda :ana, 2012, 2022 literate computing :lc, 2013, 2022 understanding computer programming as a literacy :vee, 2013, 2022 the birth & death of javascript: js, 2014, 2022 big split :split, 2015, 2022 jupyterlab :lab, 2018, 2022 jupyterlite :lab, 2021, 2022 click vee href \"https://licsjournal.org/index.php/LiCS/article/view/794/608\" click knuth href \"http://www.literateprogramming.com/knuthweb.pdf\" click nelson href \"https://monoskop.org/images/b/be/Nelson_Ted_Literary_Machines_c1987_chs_0-1.pdf\" click lc href \"https://web.archive.org/web/20220510083647/http://blog.fperez.org/2013/04/literate-computing-and-computational.html\" click cp4e href \"https://www.python.org/doc/essays/cp4e/\" flowchart LR web-- tangle ---pas pas-- pascal ---rel web-- weave ---tex tex-- TEX ---dvi reusing and importing notebooks \u00a4 we can import this presentation. import midgy.loader with midgy.loader.Markdown(extensions=[\".ipynb\", \".md\"]): import _022_10_21_markdown_future \"__file__\" not in locals() and display( _022_10_21_markdown_future, _022_10_21_markdown_future.a_little_markdown_program, _022_10_21_markdown_future.a_little_markdown_program.\u03b4\u03f5\u03b1\u03c4\u03ba ) ### what about reusing that diagram display ( IPython . display . Markdown ( _022_10_21_markdown_future . history ), IPython . display . Markdown ( _022_10_21_markdown_future . knit ) ) gantt title Literacy in computer programming dateFormat YYYY axisFormat %Y section moderninity understanding media :mcluhan, 1964, 2022 mother of all demos :englebert, 1968, 2022 section post moderninity literate programming :knuth, 1984, 2022 literary machines :nelson, 1987, 2022 www :tbl, 1989, 2022 section post web computer programming for everybody :cp4e, 1999, 2022 ipython :fperez, 2001, 2022 markdown :gruber, 2004, 2022 ipython notebooks :ipynb, 2011, 2022 anaconda :ana, 2012, 2022 literate computing :lc, 2013, 2022 understanding computer programming as a literacy :vee, 2013, 2022 the birth & death of javascript: js, 2014, 2022 big split :split, 2015, 2022 jupyterlab :lab, 2018, 2022 jupyterlite :lab, 2021, 2022 click vee href \"https://licsjournal.org/index.php/LiCS/article/view/794/608\" click knuth href \"http://www.literateprogramming.com/knuthweb.pdf\" click nelson href \"https://monoskop.org/images/b/be/Nelson_Ted_Literary_Machines_c1987_chs_0-1.pdf\" click lc href \"https://web.archive.org/web/20220510083647/http://blog.fperez.org/2013/04/literate-computing-and-computational.html\" click cp4e href \"https://www.python.org/doc/essays/cp4e/\" flowchart LR web-- tangle ---pas pas-- pascal ---rel web-- weave ---tex tex-- TEX ---dvi what about reusing that diagram \u00a4 display( IPython.display.Markdown(_022_10_21_markdown_future.history), IPython.display.Markdown(_022_10_21_markdown_future.knit) ) i 'm not saying importing markdown or notebooks is a good idea. coffeescript died didn 't it? sometimes it is too soon . it can be done and some folks might thrive working that way . i'm not saying importing markdown or notebooks is a good idea. coffeescript died didn't it? sometimes it is too soon. it can be done and some folks might thrive working that way. ### an inclusive future for code ! []( https : // pbs . twimg . com / media / FfmxiqlaAAIzTvT ? format = jpg & name = large ) in this demo , we used markdown and notebooks for source . literate programs explicitly define a document and program language . notebooks are literate programs . there are times when when code and narrative are inseparable . ``` future docker up my - paper . pdf # can a paper contain containers? importpdf my - paper -- input - file my - data . csv # could you run a pdf with your data? with importdocx (): import my_word_document # https://nbviewer.org/github/deathbeds/deathbeds.github.io/blob/master/deathbeds/2019-03-08-say-word.ipynb ``` an inclusive future for code \u00a4 in this demo, we used markdown and notebooks for source. literate programs explicitly define a document and program language. notebooks are literate programs. there are times when when code and narrative are inseparable. docker up my-paper.pdf # can a paper contain containers? importpdf my-paper --input-file my-data.csv # could you run a pdf with your data? with importdocx(): import my_word_document # https://nbviewer.org/github/deathbeds/deathbeds.github.io/blob/master/deathbeds/2019-03-08-say-word.ipynb ## the power of markdown often notebooks or markdown are means not an ends . * how many agendas have we made in ` hackmd `? * how many ` README ` s guided our way ? * how much good code is lost in notebooks ? the power of markdown \u00a4 often notebooks or markdown are means not an ends. how many agendas have we made in hackmd ? how many README s guided our way? how much good code is lost in notebooks? def qr ( url , size = 8 ): q = __import__ ( \"qrcode\" ) . QRCode ( version = 1 , error_correction = 1 , box_size = size , border = 1 , mask_pattern = None ,) q . add_data ( url ), q . make () return q . make_image () ## what if the code was just the start something? y 'all, like all y' all can modify this document in jupyterlite on github pages . < style > [ data - mime - type = \"image/png\" ] img { max - width : 200 px ; max - height : 200 px ; } </ style > qr ( https : // tonyfast . github . io / tonyfast / run / lab / index . html ? path = xxii / oct / 2022 - 10 - 21 - markdown - future . ipynb & room = deathbeds - more - like - deft - breads ) what if the code was just the start something? \u00a4 y'all, like all y'all can modify this document in jupyterlite on github pages. [data-mime-type=\"image/png\"] img { max-width: 200px; max-height: 200px; } qr( https://tonyfast.github.io/tonyfast/run/lab/index.html?path=xxii/oct/2022-10-21-markdown-future.ipynb&room=deathbeds-more-like-deft-breads )","title":"when markdown and python collide"},{"location":"xxii/oct/2022-10-21-markdown-future.html#when-markdown-and-python-collide","text":"a story about literacy written in the literate programming style of pidgy notebooks commonly communicate with two languages: markdown & python. what happen when we dissolve the boundaries between markdown & python or language & code entirely. class STATE : NB = \"__file__\" not in globals () and __name__ == \"__main__\" SCRIPT = \"__file__\" in globals () and __name__ != \"__main__\" MAIN = \"__file__\" in globals () and __name__ == \"__main__\" LITE = __import__ ( \"sys\" ) . platform == \"emscripten\" # execute this code in lite. if STATE.LITE: try: %reload_ext pidgy except ModuleNotFoundError: import micropip await micropip.install(\"pidgy importnb qrcode midgy ipywidgets\".split(), pre=True) import pidgy , pathlib , IPython shell = IPython . get_ipython () if STATE . NB : % reload_ext pidgy shell . displays_manager . template_cls = pidgy . weave . IPythonHtml import pidgy, pathlib, IPython shell = IPython.get_ipython() if STATE.NB: %reload_ext pidgy shell.displays_manager.template_cls = pidgy.weave.IPythonHtml < style > img { height : 500 px ! important ; } . jupyter - wrapper . jp - Cell - inputWrapper { display : none ; } </ style > < script src = \"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\" ></ script > < script > mermaid . initialize ({ startOnLoad : true , securityLevel : \"loose\" }, \".mermaid\" ); </ script > img { height: 500px !important; } .jupyter-wrapper .jp-Cell-inputWrapper { display: none; } mermaid.initialize({startOnLoad:true, securityLevel: \"loose\"}, \".mermaid\"); ## literate computing with literary machines using literate programming for mass computational literacy literacy rings through the history of computer science . in Annette Vee 's [Understanding Computer Programming as a Literacy] we get a high level view of literacy through history and in the our little microcosm of computer programming . > We compare mass ability to read and write software with mass literacy , and predict equally pervasive changes to society . history = \\ ``` mermaid gantt title Literacy in computer programming dateFormat YYYY axisFormat % Y section moderninity understanding media : mcluhan , 1964 , 2022 mother of all demos : englebert , 1968 , 2022 section post moderninity literate programming : knuth , 1984 , 2022 literary machines : nelson , 1987 , 2022 www : tbl , 1989 , 2022 section post web computer programming for everybody : cp4e , 1999 , 2022 ipython : fperez , 2001 , 2022 markdown : gruber , 2004 , 2022 ipython notebooks : ipynb , 2011 , 2022 anaconda : ana , 2012 , 2022 literate computing : lc , 2013 , 2022 understanding computer programming as a literacy : vee , 2013 , 2022 the birth & death of javascript : js , 2014 , 2022 big split : split , 2015 , 2022 jupyterlab : lab , 2018 , 2022 jupyterlite : lab , 2021 , 2022 click vee href \"https://licsjournal.org/index.php/LiCS/article/view/794/608\" click knuth href \"http://www.literateprogramming.com/knuthweb.pdf\" click nelson href \"https://monoskop.org/images/b/be/Nelson_Ted_Literary_Machines_c1987_chs_0-1.pdf\" click lc href \"https://web.archive.org/web/20220510083647/http://blog.fperez.org/2013/04/literate-computing-and-computational.html\" click cp4e href \"https://www.python.org/doc/essays/cp4e/\" ``` shell . displays_manager . template_cls = pidgy . weave . IPythonMarkdown","title":"when markdown and python collide"},{"location":"xxii/oct/2022-10-21-markdown-future.html#literate-computing-with-literary-machines-using-literate-programming-for-mass-computational-literacy","text":"literacy rings through the history of computer science. in Annette Vee's [Understanding Computer Programming as a Literacy] we get a high level view of literacy through history and in the our little microcosm of computer programming. We compare mass ability to read and write software with mass literacy, and predict equally pervasive changes to society. history =\\ mermaid gantt title Literacy in computer programming dateFormat YYYY axisFormat %Y section moderninity understanding media :mcluhan, 1964, 2022 mother of all demos :englebert, 1968, 2022 section post moderninity literate programming :knuth, 1984, 2022 literary machines :nelson, 1987, 2022 www :tbl, 1989, 2022 section post web computer programming for everybody :cp4e, 1999, 2022 ipython :fperez, 2001, 2022 markdown :gruber, 2004, 2022 ipython notebooks :ipynb, 2011, 2022 anaconda :ana, 2012, 2022 literate computing :lc, 2013, 2022 understanding computer programming as a literacy :vee, 2013, 2022 the birth & death of javascript: js, 2014, 2022 big split :split, 2015, 2022 jupyterlab :lab, 2018, 2022 jupyterlite :lab, 2021, 2022 click vee href \"https://licsjournal.org/index.php/LiCS/article/view/794/608\" click knuth href \"http://www.literateprogramming.com/knuthweb.pdf\" click nelson href \"https://monoskop.org/images/b/be/Nelson_Ted_Literary_Machines_c1987_chs_0-1.pdf\" click lc href \"https://web.archive.org/web/20220510083647/http://blog.fperez.org/2013/04/literate-computing-and-computational.html\" click cp4e href \"https://www.python.org/doc/essays/cp4e/\" shell.displays_manager.template_cls = pidgy.weave.IPythonMarkdown","title":"literate computing with literary machines using literate programming for mass computational literacy"},{"location":"xxii/oct/2022-10-21-markdown-future.html#donald-knuths-diagram-for-literate-programming","text":"# tangle (code) and weave (display) knit = \\ ``` mermaid flowchart LR web -- tangle --- pas pas -- pascal --- rel web -- weave --- tex tex -- TEX --- dvi ```","title":"donald knuth's diagram for literate programming"},{"location":"xxii/oct/2022-10-21-markdown-future.html#tangle-code-and-weave-display","text":"knit =\\ mermaid flowchart LR web-- tangle ---pas pas-- pascal ---rel web-- weave ---tex tex-- TEX ---dvi ## `midgy/pidgy` family of input kernels in `IPython` literate inputs with markdown and python as the document and programming languages < div style = \"font-size: 2rem;\" > import midgy as tangle , pidgy as weave </ div > we 'll explore a little program in markdown inside a bigger notebook program. hold tight.","title":"tangle (code) and weave (display)"},{"location":"xxii/oct/2022-10-21-markdown-future.html#midgypidgy-family-of-input-kernels-in-ipython","text":"literate inputs with markdown and python as the document and programming languages import midgy as tangle, pidgy as weave we'll explore a little program in markdown inside a bigger notebook program. hold tight. ### how does `IPython` python? * ` shell . kernel . do_execute ` * ` shell . run_cell ` * tangle * ` shell . transform_cell ` * ` shell . compile . ast_parse ` * ` shell . transform_ast ` * ` shell . run_ast_nodes ` * weave * if ` shell . ast_node_interactivity ` * ` shell . display_formatter . format ` ` IPython ` is very hackable ! that is what makes it fun along with all the development happening in python .","title":"midgy/pidgy family of input kernels in IPython"},{"location":"xxii/oct/2022-10-21-markdown-future.html#how-does-ipython-python","text":"shell.kernel.do_execute shell.run_cell tangle shell.transform_cell shell.compile.ast_parse shell.transform_ast shell.run_ast_nodes weave if shell.ast_node_interactivity shell.display_formatter.format IPython is very hackable! that is what makes it fun along with all the development happening in python. %% file a - little - markdown - program . md this markdown file is a program ! \u03b4\u03f5\u03b1\u03c4\u03ba = \\ death to pseudo code , long live pseudo code if __name__ == \"__main__\" : print ( \u03b4\u03f5\u03b1\u03c4\u03ba . upper ()) Overwriting a-little-markdown-program.md we can run this like a script. WTF! !midgy a-little-markdown-program.md import midgy.loader with midgy . loader . Markdown () as loader : import a_little_markdown_program a_little_markdown_program , a_little_markdown_program . \u03b4\u03f5\u03b1\u03c4\u03ba import midgy.loader with midgy.loader.Markdown() as loader: import a_little_markdown_program a_little_markdown_program, a_little_markdown_program.\u03b4\u03f5\u03b1\u03c4\u03ba ` a_little_markdown_program ` is a module like any other ` {{ a_little_markdown_program }} ` \ud83d\udc07 and we even generated a compiled version of the markdown source \ud83c\udfa9 if not STATE . LITE : assert next ( ( pathlib . Path ( a_little_markdown_program . __file__ ) . parent / \"__pycache__\" ) . glob ( \"a-little-markdown-program.*.pyc\" ) ), \\ verifies there is a compiled file . a_little_markdown_program is a module like any other <module 'a-little-markdown-program' from '/home/tbone/Documents/tonyfast/tonyfast/xxii/oct/a-little-markdown-program.md'> \ud83d\udc07and we even generated a compiled version of the markdown source\ud83c\udfa9 if not STATE.LITE: assert next( (pathlib.Path(a_little_markdown_program.__file__).parent / \"__pycache__\").glob(\"a-little-markdown-program.*.pyc\") ),\\ verifies there is a compiled file. ## [literate computing widgets and `IPython` interactive displays](2022-10-21-pidgy-displays.ipynb)","title":"how does IPython python?"},{"location":"xxii/oct/2022-10-21-markdown-future.html#literate-computing-widgets-and-ipython-interactive-displays","text":"## reusing and importing notebooks we can import this presentation . import midgy.loader with midgy . loader . Markdown ( extensions = [ \".ipynb\" , \".md\" ]): import _022_10_21_markdown_future \"__file__\" not in locals () and display ( _022_10_21_markdown_future , _022_10_21_markdown_future . a_little_markdown_program , _022_10_21_markdown_future . a_little_markdown_program . \u03b4\u03f5\u03b1\u03c4\u03ba ) Overwriting a-little-markdown-program.md gantt title Literacy in computer programming dateFormat YYYY axisFormat %Y section moderninity understanding media :mcluhan, 1964, 2022 mother of all demos :englebert, 1968, 2022 section post moderninity literate programming :knuth, 1984, 2022 literary machines :nelson, 1987, 2022 www :tbl, 1989, 2022 section post web computer programming for everybody :cp4e, 1999, 2022 ipython :fperez, 2001, 2022 markdown :gruber, 2004, 2022 ipython notebooks :ipynb, 2011, 2022 anaconda :ana, 2012, 2022 literate computing :lc, 2013, 2022 understanding computer programming as a literacy :vee, 2013, 2022 the birth & death of javascript: js, 2014, 2022 big split :split, 2015, 2022 jupyterlab :lab, 2018, 2022 jupyterlite :lab, 2021, 2022 click vee href \"https://licsjournal.org/index.php/LiCS/article/view/794/608\" click knuth href \"http://www.literateprogramming.com/knuthweb.pdf\" click nelson href \"https://monoskop.org/images/b/be/Nelson_Ted_Literary_Machines_c1987_chs_0-1.pdf\" click lc href \"https://web.archive.org/web/20220510083647/http://blog.fperez.org/2013/04/literate-computing-and-computational.html\" click cp4e href \"https://www.python.org/doc/essays/cp4e/\" flowchart LR web-- tangle ---pas pas-- pascal ---rel web-- weave ---tex tex-- TEX ---dvi","title":"literate computing widgets and IPython interactive displays"},{"location":"xxii/oct/2022-10-21-markdown-future.html#reusing-and-importing-notebooks","text":"we can import this presentation. import midgy.loader with midgy.loader.Markdown(extensions=[\".ipynb\", \".md\"]): import _022_10_21_markdown_future \"__file__\" not in locals() and display( _022_10_21_markdown_future, _022_10_21_markdown_future.a_little_markdown_program, _022_10_21_markdown_future.a_little_markdown_program.\u03b4\u03f5\u03b1\u03c4\u03ba ) ### what about reusing that diagram display ( IPython . display . Markdown ( _022_10_21_markdown_future . history ), IPython . display . Markdown ( _022_10_21_markdown_future . knit ) ) gantt title Literacy in computer programming dateFormat YYYY axisFormat %Y section moderninity understanding media :mcluhan, 1964, 2022 mother of all demos :englebert, 1968, 2022 section post moderninity literate programming :knuth, 1984, 2022 literary machines :nelson, 1987, 2022 www :tbl, 1989, 2022 section post web computer programming for everybody :cp4e, 1999, 2022 ipython :fperez, 2001, 2022 markdown :gruber, 2004, 2022 ipython notebooks :ipynb, 2011, 2022 anaconda :ana, 2012, 2022 literate computing :lc, 2013, 2022 understanding computer programming as a literacy :vee, 2013, 2022 the birth & death of javascript: js, 2014, 2022 big split :split, 2015, 2022 jupyterlab :lab, 2018, 2022 jupyterlite :lab, 2021, 2022 click vee href \"https://licsjournal.org/index.php/LiCS/article/view/794/608\" click knuth href \"http://www.literateprogramming.com/knuthweb.pdf\" click nelson href \"https://monoskop.org/images/b/be/Nelson_Ted_Literary_Machines_c1987_chs_0-1.pdf\" click lc href \"https://web.archive.org/web/20220510083647/http://blog.fperez.org/2013/04/literate-computing-and-computational.html\" click cp4e href \"https://www.python.org/doc/essays/cp4e/\" flowchart LR web-- tangle ---pas pas-- pascal ---rel web-- weave ---tex tex-- TEX ---dvi","title":"reusing and importing notebooks"},{"location":"xxii/oct/2022-10-21-markdown-future.html#what-about-reusing-that-diagram","text":"display( IPython.display.Markdown(_022_10_21_markdown_future.history), IPython.display.Markdown(_022_10_21_markdown_future.knit) ) i 'm not saying importing markdown or notebooks is a good idea. coffeescript died didn 't it? sometimes it is too soon . it can be done and some folks might thrive working that way . i'm not saying importing markdown or notebooks is a good idea. coffeescript died didn't it? sometimes it is too soon. it can be done and some folks might thrive working that way. ### an inclusive future for code ! []( https : // pbs . twimg . com / media / FfmxiqlaAAIzTvT ? format = jpg & name = large ) in this demo , we used markdown and notebooks for source . literate programs explicitly define a document and program language . notebooks are literate programs . there are times when when code and narrative are inseparable . ``` future docker up my - paper . pdf # can a paper contain containers? importpdf my - paper -- input - file my - data . csv # could you run a pdf with your data? with importdocx (): import my_word_document # https://nbviewer.org/github/deathbeds/deathbeds.github.io/blob/master/deathbeds/2019-03-08-say-word.ipynb ```","title":"what about reusing that diagram"},{"location":"xxii/oct/2022-10-21-markdown-future.html#an-inclusive-future-for-code","text":"in this demo, we used markdown and notebooks for source. literate programs explicitly define a document and program language. notebooks are literate programs. there are times when when code and narrative are inseparable. docker up my-paper.pdf # can a paper contain containers? importpdf my-paper --input-file my-data.csv # could you run a pdf with your data? with importdocx(): import my_word_document # https://nbviewer.org/github/deathbeds/deathbeds.github.io/blob/master/deathbeds/2019-03-08-say-word.ipynb ## the power of markdown often notebooks or markdown are means not an ends . * how many agendas have we made in ` hackmd `? * how many ` README ` s guided our way ? * how much good code is lost in notebooks ?","title":"an inclusive future for code"},{"location":"xxii/oct/2022-10-21-markdown-future.html#the-power-of-markdown","text":"often notebooks or markdown are means not an ends. how many agendas have we made in hackmd ? how many README s guided our way? how much good code is lost in notebooks? def qr ( url , size = 8 ): q = __import__ ( \"qrcode\" ) . QRCode ( version = 1 , error_correction = 1 , box_size = size , border = 1 , mask_pattern = None ,) q . add_data ( url ), q . make () return q . make_image () ## what if the code was just the start something? y 'all, like all y' all can modify this document in jupyterlite on github pages . < style > [ data - mime - type = \"image/png\" ] img { max - width : 200 px ; max - height : 200 px ; } </ style > qr ( https : // tonyfast . github . io / tonyfast / run / lab / index . html ? path = xxii / oct / 2022 - 10 - 21 - markdown - future . ipynb & room = deathbeds - more - like - deft - breads )","title":"the power of markdown"},{"location":"xxii/oct/2022-10-21-markdown-future.html#what-if-the-code-was-just-the-start-something","text":"y'all, like all y'all can modify this document in jupyterlite on github pages. [data-mime-type=\"image/png\"] img { max-width: 200px; max-height: 200px; } qr( https://tonyfast.github.io/tonyfast/run/lab/index.html?path=xxii/oct/2022-10-21-markdown-future.ipynb&room=deathbeds-more-like-deft-breads )","title":"what if the code was just the start something?"},{"location":"xxii/oct/2022-10-21-pidgy-displays.html","text":"literate computing - interactive literate programming \u00a4 literate computing relies on a rapid feedback from the computer. pidgy features multiple reactive/interactive displays for presenting your content. we'll explore the displays and meaning with two demos: 1. the cookies demo 2. the dataframe demo cookies demo \u00a4 the cookies demo is mainstay in the deathbeds repertoire. inspired by bret viktors tanglejs we explore the cookie demo in pidgy . if i eat 3 then i consume 150 calories. shell.displays_manager.template_cls = pidgy.weave.IPythonHtml if i eat 3 then i consume 150 calories. shell.displays_manager.template_cls = pidgy.weave.IPythonMarkdown cookies Dataframes \u00a4 from ipywidgets import interact_manual import pandas who = Text(\"tonyfast\") shell.displays_manager.template_cls = pidgy.weave.IPythonHtml g is undefined shell.displays_manager.template_cls = pidgy.weave.IPythonMarkdown g is undefined g = df.iloc[:5, :5] def update(who): global df, g g = df = pandas.read_json(F\"https://api.github.com/users/{who}/gists\") interact_manual(update, who=who)","title":"literate computing - interactive literate programming"},{"location":"xxii/oct/2022-10-21-pidgy-displays.html#literate-computing-interactive-literate-programming","text":"literate computing relies on a rapid feedback from the computer. pidgy features multiple reactive/interactive displays for presenting your content. we'll explore the displays and meaning with two demos: 1. the cookies demo 2. the dataframe demo","title":"literate computing - interactive literate programming"},{"location":"xxii/oct/2022-10-21-pidgy-displays.html#cookies-demo","text":"the cookies demo is mainstay in the deathbeds repertoire. inspired by bret viktors tanglejs we explore the cookie demo in pidgy . if i eat 3 then i consume 150 calories. shell.displays_manager.template_cls = pidgy.weave.IPythonHtml if i eat 3 then i consume 150 calories. shell.displays_manager.template_cls = pidgy.weave.IPythonMarkdown cookies","title":"cookies demo"},{"location":"xxii/oct/2022-10-21-pidgy-displays.html#dataframes","text":"from ipywidgets import interact_manual import pandas who = Text(\"tonyfast\") shell.displays_manager.template_cls = pidgy.weave.IPythonHtml g is undefined shell.displays_manager.template_cls = pidgy.weave.IPythonMarkdown g is undefined g = df.iloc[:5, :5] def update(who): global df, g g = df = pandas.read_json(F\"https://api.github.com/users/{who}/gists\") interact_manual(update, who=who)","title":"Dataframes"},{"location":"xxii/oct/2022-10-25-static-notebook-tags.html","text":"post processing html accessibility \u00a4 this demo illustrates how an nbconvert template's html can be editted directly using BeautifulSoup these concepts intersect multiple outstanding issues: * https://github.com/Iota-School/notebooks-for-all/issues/19#issuecomment-1251245078 * https://github.com/Iota-School/notebooks-for-all/issues/15 * https://github.com/Iota-School/notebooks-for-all/issues/20 import nbconvert_html5 from bs4 import BeautifulSoup from pathlib import Path jupyter selectors \u00a4 we need to collect these across representations nbconvert (lab/class), nbviewer, sphinx, mkdocs MAIN = \"#notebook, .jp-Notebook\" CELL = \".cell, .jp-Cell\" CODE = \".code_cell, .jp-CodeCell\" MD = \".text_cell, .jp-MarkdownCell\" OUT = \".output, .jp-OutputArea.jp-Cell-outputArea\" IN = \".code_cell .input .input_area, .jp-Editor\" PROMPT = \".input_prompt\" the Html5 exporter \u00a4 currently the class does not change anything but exposes an api from directly modify exported html. old = nbconvert_html5 . Html5 () . from_filename ( \"2022-10-25-static-notebook-tags.ipynb\" )[ 0 ] source = Path ( \"indexed-source.html\" ); source . write_text ( old ) jupyter remediations for landmarks \u00a4 we are exploring the efficacy of html5 conventions to provide accessibility landmarks in the jupyter notebook. we'll modify: * the primary container * cell inputs and outputs * executin counts def set_notebook ( soup ): set_main ( soup ); set_cells ( soup ); set_inputs ( soup ); set_prompts ( soup ) def get_html ( x , ** k ): soup = BeautifulSoup ( x , features = \"lxml\" ); set_notebook ( soup ) return str ( soup ) the setters \u00a4 def set_main ( soup ): e = soup . select_one ( MAIN ) e . attrs . pop ( \"tabindex\" , None ) e . name = \"main\" def set_main_aside ( soup ): \"\"\"[Move Metadata to the top](https://github.com/Iota-School/notebooks-for-all/issues/21)\"\"\" def set_cells ( soup ): for element in soup . select ( CODE ): set_code_cell ( element ) for element in soup . select ( MD ): set_md_cell ( element ) def set_code_cell ( e ): e . name = \"article\" # in multi kernel scenarios are cell magics the input might vary e . attrs . setdefault ( \"aria-label\" , \"code cell\" ) def set_md_cell ( e ): e . name = \"article\" e . attrs . setdefault ( \"aria-label\" , \"markdown cell\" ) def set_displays ( e ): \"\"\"introduces a section tag to the outputs\"\"\" out = e . select_one ( OUT ) out . name = \"section\" e . attrs . setdefault ( \"aria-label\" , \"code outputs\" ) def set_inputs ( soup ): for inp in soup . select ( IN ): inp . replace_with ( BeautifulSoup ( F \"<code><pre> { inp . text } </pre></code>\" , features = \"lxml\" ) . select_one ( \"code\" )) def set_prompts ( soup ): \"\"\"https://github.com/Iota-School/notebooks-for-all/issues/20#issuecomment-1247172797\"\"\" for prompt in soup . select ( PROMPT ): prompt . name = \"aside\" Running the post processor \u00a4 new = nbconvert_html5 . Html5 ( post_processor = get_html ) . from_filename ( \"2022-10-25-static-notebook-tags.ipynb\" )[ 0 ] target = Path ( \"indexed-target.html\" ); target . write_text ( new ); analysis in a headless browser \u00a4 async def get_headless ( file ): import playwright.async_api from shlex import split async with playwright . async_api . async_playwright () as play : browser = await play . chromium . launch ( args = split ( '--enable-blink-features=\"AccessibilityObjectModel\"' ), headless = True , channel = \"chrome-beta\" ) page = await browser . new_page () state = await page . goto ( file . absolute () . as_uri ()) data = await page . accessibility . snapshot () await browser . close () return data comparing results \u00a4 import pandas df = pandas.DataFrame(await get_headless(source)); df A = df.children.apply(pandas.Series).set_index(\"role\") B = pandas.DataFrame(await get_headless(target)).children.apply(pandas.Series).set_index(\"role\") display(\"old\", A.T, \"new\", B.T) usage in manual testing \u00a4 nbconvert_html5 has the hooks to work with jupyter s normal command line tool. %% file jupyter_nbconvert_config . py from unittest.mock import Mock c = locals () . get ( \"c\" , Mock ()) with __import__ ( \"importnb\" ) . Notebook (): from __static_notebook_tags import get_html c . TemplateExporter . post_processor = get_html # or put your methods in here. # what are the A/B tests today? Writing jupyter_nbconvert_config.py if __name__ == \"__main__\" and \"__file__\" not in locals (): ! jupyter nbconvert -- to html5 -- stdout 2022 - 10 - 25 - static - notebook - tags . ipynb [NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.","title":"post processing html accessibility"},{"location":"xxii/oct/2022-10-25-static-notebook-tags.html#post-processing-html-accessibility","text":"this demo illustrates how an nbconvert template's html can be editted directly using BeautifulSoup these concepts intersect multiple outstanding issues: * https://github.com/Iota-School/notebooks-for-all/issues/19#issuecomment-1251245078 * https://github.com/Iota-School/notebooks-for-all/issues/15 * https://github.com/Iota-School/notebooks-for-all/issues/20 import nbconvert_html5 from bs4 import BeautifulSoup from pathlib import Path","title":"post processing html accessibility"},{"location":"xxii/oct/2022-10-25-static-notebook-tags.html#jupyter-selectors","text":"we need to collect these across representations nbconvert (lab/class), nbviewer, sphinx, mkdocs MAIN = \"#notebook, .jp-Notebook\" CELL = \".cell, .jp-Cell\" CODE = \".code_cell, .jp-CodeCell\" MD = \".text_cell, .jp-MarkdownCell\" OUT = \".output, .jp-OutputArea.jp-Cell-outputArea\" IN = \".code_cell .input .input_area, .jp-Editor\" PROMPT = \".input_prompt\"","title":"jupyter selectors"},{"location":"xxii/oct/2022-10-25-static-notebook-tags.html#the-html5-exporter","text":"currently the class does not change anything but exposes an api from directly modify exported html. old = nbconvert_html5 . Html5 () . from_filename ( \"2022-10-25-static-notebook-tags.ipynb\" )[ 0 ] source = Path ( \"indexed-source.html\" ); source . write_text ( old )","title":"the Html5 exporter"},{"location":"xxii/oct/2022-10-25-static-notebook-tags.html#jupyter-remediations-for-landmarks","text":"we are exploring the efficacy of html5 conventions to provide accessibility landmarks in the jupyter notebook. we'll modify: * the primary container * cell inputs and outputs * executin counts def set_notebook ( soup ): set_main ( soup ); set_cells ( soup ); set_inputs ( soup ); set_prompts ( soup ) def get_html ( x , ** k ): soup = BeautifulSoup ( x , features = \"lxml\" ); set_notebook ( soup ) return str ( soup )","title":"jupyter remediations for landmarks"},{"location":"xxii/oct/2022-10-25-static-notebook-tags.html#the-setters","text":"def set_main ( soup ): e = soup . select_one ( MAIN ) e . attrs . pop ( \"tabindex\" , None ) e . name = \"main\" def set_main_aside ( soup ): \"\"\"[Move Metadata to the top](https://github.com/Iota-School/notebooks-for-all/issues/21)\"\"\" def set_cells ( soup ): for element in soup . select ( CODE ): set_code_cell ( element ) for element in soup . select ( MD ): set_md_cell ( element ) def set_code_cell ( e ): e . name = \"article\" # in multi kernel scenarios are cell magics the input might vary e . attrs . setdefault ( \"aria-label\" , \"code cell\" ) def set_md_cell ( e ): e . name = \"article\" e . attrs . setdefault ( \"aria-label\" , \"markdown cell\" ) def set_displays ( e ): \"\"\"introduces a section tag to the outputs\"\"\" out = e . select_one ( OUT ) out . name = \"section\" e . attrs . setdefault ( \"aria-label\" , \"code outputs\" ) def set_inputs ( soup ): for inp in soup . select ( IN ): inp . replace_with ( BeautifulSoup ( F \"<code><pre> { inp . text } </pre></code>\" , features = \"lxml\" ) . select_one ( \"code\" )) def set_prompts ( soup ): \"\"\"https://github.com/Iota-School/notebooks-for-all/issues/20#issuecomment-1247172797\"\"\" for prompt in soup . select ( PROMPT ): prompt . name = \"aside\"","title":"the setters"},{"location":"xxii/oct/2022-10-25-static-notebook-tags.html#running-the-post-processor","text":"new = nbconvert_html5 . Html5 ( post_processor = get_html ) . from_filename ( \"2022-10-25-static-notebook-tags.ipynb\" )[ 0 ] target = Path ( \"indexed-target.html\" ); target . write_text ( new );","title":"Running the post processor"},{"location":"xxii/oct/2022-10-25-static-notebook-tags.html#analysis-in-a-headless-browser","text":"async def get_headless ( file ): import playwright.async_api from shlex import split async with playwright . async_api . async_playwright () as play : browser = await play . chromium . launch ( args = split ( '--enable-blink-features=\"AccessibilityObjectModel\"' ), headless = True , channel = \"chrome-beta\" ) page = await browser . new_page () state = await page . goto ( file . absolute () . as_uri ()) data = await page . accessibility . snapshot () await browser . close () return data","title":"analysis in a headless browser"},{"location":"xxii/oct/2022-10-25-static-notebook-tags.html#comparing-results","text":"import pandas df = pandas.DataFrame(await get_headless(source)); df A = df.children.apply(pandas.Series).set_index(\"role\") B = pandas.DataFrame(await get_headless(target)).children.apply(pandas.Series).set_index(\"role\") display(\"old\", A.T, \"new\", B.T)","title":"comparing results"},{"location":"xxii/oct/2022-10-25-static-notebook-tags.html#usage-in-manual-testing","text":"nbconvert_html5 has the hooks to work with jupyter s normal command line tool. %% file jupyter_nbconvert_config . py from unittest.mock import Mock c = locals () . get ( \"c\" , Mock ()) with __import__ ( \"importnb\" ) . Notebook (): from __static_notebook_tags import get_html c . TemplateExporter . post_processor = get_html # or put your methods in here. # what are the A/B tests today? Writing jupyter_nbconvert_config.py if __name__ == \"__main__\" and \"__file__\" not in locals (): ! jupyter nbconvert -- to html5 -- stdout 2022 - 10 - 25 - static - notebook - tags . ipynb [NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.","title":"usage in manual testing"},{"location":"xxii/oct/2022-10-27-axe-core-playwright-python.html","text":"invoking axe-core from python playwright \u00a4 javascript is not my jam. id rather write python. this post is a result of these inconsistencies. in my accessibility, i often need to test the accessibility of a web page. the javascript way to this is to run using playwright and playwright-axe together. i couldn't find an evident way to do that. instead i peaked into the playwright-axe integration and realizes we could invoke axe through python by copying a little bit. auditting an html file \u00a4 create a headless playwright browser load the page inject axe-core audit the page close the browser from pathlib import Path async def audit ( file , ** config ): import playwright.async_api async with playwright . async_api . async_playwright () as play : browser , page = await get_browser_page ( play ) await page . goto ( Path ( file ) . absolute () . as_uri ()) await __import__ ( \"asyncio\" ) . sleep ( 3 ) await injectReadability ( page ) # await injectAxe(page) # data = await get_audit_data(page, **config) await __import__ ( \"asyncio\" ) . sleep ( 30 ) await browser . close () return data async def injectReadability ( page ): await page . evaluate ( requests . get ( \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\" ) . text ) status = await page . evaluate ( \"\"\"async () => { const readability = await import('https://cdn.skypack.dev/@mozilla/readability'); return (new readability.Readability(document)).parse(); }\"\"\" ) await audit ( \"better.html\" ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In [15], line 1 ----> 1 await audit( \" better.html \" ) Cell In [11], line 7 , in audit (file, **config) 5 await page . goto(Path(file) . absolute() . as_uri()) 6 await __import__ ( \" asyncio \" ) . sleep( 3 ) ----> 7 await injectReadability(page) 8 # await injectAxe(page) 9 # data = await get_audit_data(page, **config) 10 await __import__ ( \" asyncio \" ) . sleep( 30 ) Cell In [12], line 2 , in injectReadability (page) 1 async def injectReadability (page): ----> 2 await page . evaluate( requests . get( \" https://raw.githubusercontent.com/mozilla/readability/master/Readability.js \" ) . text) 3 status = await page . evaluate( \"\"\" async () => { 4 const readability = await import( ' https://cdn.skypack.dev/@mozilla/readability ' ); 5 return (new readability.Readability(document)).parse(); 6 } \"\"\" ) NameError : name 'requests' is not defined async def get_browser_page ( play , ** options ): from shlex import split browser = await play . chromium . launch ( args = split ( '--enable-blink-features=\"AccessibilityObjectModel\"' ), headless = False , channel = \"chrome-beta\" ) return browser , await browser . new_page () injectAxe mimics how playwright-axe loads the package. we used a cached requests object vendored from unpkg async def injectAxe ( page ): await page . evaluate ( requests . get ( \"https://unpkg.com/axe-core\" ) . text ) async def injectReadability ( page ): await page . evaluate ( requests . get ( \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\" ) . text ) status = await page . evaluate ( \"\"\"var article = new Readability(document).parse();\"\"\" ) get_audit_data extracts the axe test results and the accesssbility tree from the page. async def get_audit_data ( page , ** config ): from json import dumps return await __import__ ( \"asyncio\" ) . gather ( page . evaluate ( F \"window.axe.run(window.document, { dumps ( config ) } )\" ), page . accessibility . snapshot ()) testing a page \u00a4 we use this notebook as the test artifact by generating an html version of it with nbconvert import requests_cache , requests , pandas ; requests_cache . install_cache ( \"a11y\" ) from pathlib import Path THIS = Path ( \"2022-10-27-axe-core-playwright-python.ipynb\" ) if __name__ == \"__main__\" : ! jupyter nbconvert -- to html $ THIS [NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`. [NbConvertApp] Converting notebook 2022-10-27-axe-core-playwright-python.ipynb to html [NbConvertApp] Writing 605344 bytes to 2022-10-27-axe-core-playwright-python.html getting the accessibility audit objects \u00a4 axe contains the axe test data and tree contains the accessibility tree. axe , tree = map ( pandas . Series , await audit ( THIS . with_suffix ( \".html\" ), runOnly = \"best-practice\" . split ())) axe violations \u00a4 pandas . DataFrame ( axe . violations ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id impact tags description help helpUrl nodes 0 landmark-one-main moderate [cat.semantics, best-practice] Ensures the document has a main landmark Document should have one main landmark https://dequeuniversity.com/rules/axe/4.5/land... [{'any': [], 'all': [{'id': 'page-has-main', '... 1 region moderate [cat.keyboard, best-practice] Ensures all page content is contained by landm... All page content should be contained by landmarks https://dequeuniversity.com/rules/axe/4.5/regi... [{'any': [{'id': 'region', 'data': {'isIframe'... accessibility tree \u00a4 pandas . DataFrame ( tree . children ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } role name level 0 text Loading [MathJax]/jax/output/CommonHTML/fonts/... NaN 1 heading invoking axe-core from python playwright 1.0 2 text javascript is not my jam. id rather write pyth... NaN 3 text in my accessibility, i often need to test the ... NaN 4 text i couldn't find an evident way to do that. ins... NaN ... ... ... ... 275 text . NaN 276 text children NaN 277 text ) NaN 278 heading conclusion 2.0 279 text we can run axe in python playwright and analyz... NaN 280 rows \u00d7 3 columns conclusion \u00a4 we can run axe in python playwright and analyze results in pandas.","title":"invoking axe-core from python playwright"},{"location":"xxii/oct/2022-10-27-axe-core-playwright-python.html#invoking-axe-core-from-python-playwright","text":"javascript is not my jam. id rather write python. this post is a result of these inconsistencies. in my accessibility, i often need to test the accessibility of a web page. the javascript way to this is to run using playwright and playwright-axe together. i couldn't find an evident way to do that. instead i peaked into the playwright-axe integration and realizes we could invoke axe through python by copying a little bit.","title":"invoking axe-core from python playwright"},{"location":"xxii/oct/2022-10-27-axe-core-playwright-python.html#auditting-an-html-file","text":"create a headless playwright browser load the page inject axe-core audit the page close the browser from pathlib import Path async def audit ( file , ** config ): import playwright.async_api async with playwright . async_api . async_playwright () as play : browser , page = await get_browser_page ( play ) await page . goto ( Path ( file ) . absolute () . as_uri ()) await __import__ ( \"asyncio\" ) . sleep ( 3 ) await injectReadability ( page ) # await injectAxe(page) # data = await get_audit_data(page, **config) await __import__ ( \"asyncio\" ) . sleep ( 30 ) await browser . close () return data async def injectReadability ( page ): await page . evaluate ( requests . get ( \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\" ) . text ) status = await page . evaluate ( \"\"\"async () => { const readability = await import('https://cdn.skypack.dev/@mozilla/readability'); return (new readability.Readability(document)).parse(); }\"\"\" ) await audit ( \"better.html\" ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In [15], line 1 ----> 1 await audit( \" better.html \" ) Cell In [11], line 7 , in audit (file, **config) 5 await page . goto(Path(file) . absolute() . as_uri()) 6 await __import__ ( \" asyncio \" ) . sleep( 3 ) ----> 7 await injectReadability(page) 8 # await injectAxe(page) 9 # data = await get_audit_data(page, **config) 10 await __import__ ( \" asyncio \" ) . sleep( 30 ) Cell In [12], line 2 , in injectReadability (page) 1 async def injectReadability (page): ----> 2 await page . evaluate( requests . get( \" https://raw.githubusercontent.com/mozilla/readability/master/Readability.js \" ) . text) 3 status = await page . evaluate( \"\"\" async () => { 4 const readability = await import( ' https://cdn.skypack.dev/@mozilla/readability ' ); 5 return (new readability.Readability(document)).parse(); 6 } \"\"\" ) NameError : name 'requests' is not defined async def get_browser_page ( play , ** options ): from shlex import split browser = await play . chromium . launch ( args = split ( '--enable-blink-features=\"AccessibilityObjectModel\"' ), headless = False , channel = \"chrome-beta\" ) return browser , await browser . new_page () injectAxe mimics how playwright-axe loads the package. we used a cached requests object vendored from unpkg async def injectAxe ( page ): await page . evaluate ( requests . get ( \"https://unpkg.com/axe-core\" ) . text ) async def injectReadability ( page ): await page . evaluate ( requests . get ( \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\" ) . text ) status = await page . evaluate ( \"\"\"var article = new Readability(document).parse();\"\"\" ) get_audit_data extracts the axe test results and the accesssbility tree from the page. async def get_audit_data ( page , ** config ): from json import dumps return await __import__ ( \"asyncio\" ) . gather ( page . evaluate ( F \"window.axe.run(window.document, { dumps ( config ) } )\" ), page . accessibility . snapshot ())","title":"auditting an html file"},{"location":"xxii/oct/2022-10-27-axe-core-playwright-python.html#testing-a-page","text":"we use this notebook as the test artifact by generating an html version of it with nbconvert import requests_cache , requests , pandas ; requests_cache . install_cache ( \"a11y\" ) from pathlib import Path THIS = Path ( \"2022-10-27-axe-core-playwright-python.ipynb\" ) if __name__ == \"__main__\" : ! jupyter nbconvert -- to html $ THIS [NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`. [NbConvertApp] Converting notebook 2022-10-27-axe-core-playwright-python.ipynb to html [NbConvertApp] Writing 605344 bytes to 2022-10-27-axe-core-playwright-python.html","title":"testing a page"},{"location":"xxii/oct/2022-10-27-axe-core-playwright-python.html#getting-the-accessibility-audit-objects","text":"axe contains the axe test data and tree contains the accessibility tree. axe , tree = map ( pandas . Series , await audit ( THIS . with_suffix ( \".html\" ), runOnly = \"best-practice\" . split ()))","title":"getting the accessibility audit objects"},{"location":"xxii/oct/2022-10-27-axe-core-playwright-python.html#axe-violations","text":"pandas . DataFrame ( axe . violations ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id impact tags description help helpUrl nodes 0 landmark-one-main moderate [cat.semantics, best-practice] Ensures the document has a main landmark Document should have one main landmark https://dequeuniversity.com/rules/axe/4.5/land... [{'any': [], 'all': [{'id': 'page-has-main', '... 1 region moderate [cat.keyboard, best-practice] Ensures all page content is contained by landm... All page content should be contained by landmarks https://dequeuniversity.com/rules/axe/4.5/regi... [{'any': [{'id': 'region', 'data': {'isIframe'...","title":"axe violations"},{"location":"xxii/oct/2022-10-27-axe-core-playwright-python.html#accessibility-tree","text":"pandas . DataFrame ( tree . children ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } role name level 0 text Loading [MathJax]/jax/output/CommonHTML/fonts/... NaN 1 heading invoking axe-core from python playwright 1.0 2 text javascript is not my jam. id rather write pyth... NaN 3 text in my accessibility, i often need to test the ... NaN 4 text i couldn't find an evident way to do that. ins... NaN ... ... ... ... 275 text . NaN 276 text children NaN 277 text ) NaN 278 heading conclusion 2.0 279 text we can run axe in python playwright and analyz... NaN 280 rows \u00d7 3 columns","title":"accessibility tree"},{"location":"xxii/oct/2022-10-27-axe-core-playwright-python.html#conclusion","text":"we can run axe in python playwright and analyze results in pandas.","title":"conclusion"},{"location":"xxii/oct/2022-10-29-metadata-formatter.html","text":"inferring linked data from IPython run times. \u00a4 automatically exporting rdf data during interactive computing. we'll modify IPython s display formatter to include a system that describes python objects as json linked data. from functools import singledispatch , singledispatchmethod import types , gc , re , sys from IPython.core.formatters import DisplayFormatter , BaseFormatter , catch_format_error , JSONFormatter from traitlets import ObjectName , Unicode , Instance , List , Any from IPython import get_ipython from pathlib import Path TYPE , ID , GRAPH , CONTAINER , NEST , CONTEXT = \"@type @id @graph @container @nest @context\" . split () MAIN = __name__ == \"__main__\" ACTIVE = \"__file__\" not in locals () and MAIN shell = get_ipython () a MetadataFormatter for including linked data in IPython reprs. this class carries machinery to generate: ids for python types and objects as urns MetadataFormatter.get_id linked data representations of python objects with MetadataFormatter.get_graph MetadataFormatter.for_type, MetadataFormatter.get_id.register, MetadataFormatter.get_graph.register extra the expression of the linked data graphs. custom metadata formatter \u00a4 class MetadataFormatter ( BaseFormatter ): graph , format_type , = List (), Unicode ( 'application/ld+json' ) _return_type , print_method = ( list , dict ), ObjectName ( '_repr_metadata_' ) @singledispatchmethod def get_id ( self , object ): return next ( self . get_object ( object ), None ) @singledispatchmethod def get_graph ( self , object ): data = { TYPE : self . get_id ( type ( object ))} id = self . get_id ( object ) if id : data . setdefault ( ID , id ) if isinstance ( id , list ): return [{ ID : x , ** data } for x in id ] return data def get_object ( self , object , filter = None ): if isinstance ( filter , str ): filter = re . compile ( filter ) for referrer in ( x for x in gc . get_referrers ( object ) if isinstance ( x , dict )): yield from self . get_object_from_ns ( referrer , object , filter = filter ) def get_object_from_ns ( self , ns , object , filter = None ): weakref = ns . get ( \"__weakref__\" ) parent = None if weakref : parent = self . get_id ( weakref . __objclass__ ) else : parent = ns . get ( \"__module__\" , ns . get ( \"__name__\" )) if parent : parent += \":\" if not parent and ns is sys . modules : return object . __name__ for k in ( k for k , v in ns . items () if v is object and not k . startswith ( \"_\" )): name = F \" { parent or 'noparent' } # { k } \" if filter is not None and not filter . match ( name ): continue yield name def get_session_cell_id ( self ): data = get_ipython () . kernel . get_parent () return data [ \"metadata\" ][ \"cellId\" ], data [ \"header\" ][ \"session\" ] def set_metadata ( self , object = None , ** kwargs ): ids = dict ( zip (( \"cell:id\" , \"session:id\" ), self . get_session_cell_id ())) node = {} if object is None else self . get_graph ( object ) if isinstance ( node , dict ): node = [ node ] for node in node : self . graph . append ({ ** ids , ** node , ** kwargs }) def __call__ ( self , object ): explicit = super () . __call__ ( object ) if explicit : if isinstance ( explicit , dict ): self . set_metadata ( ** explicit ) else : for e in explicit : self . set_metadata ( ** w ) else : self . set_metadata ( object ) try : return self . graph [:] finally : self . graph . clear () custom display formatter \u00a4 the LinkedDataFormatter customizes how IPython s normal DisplayFormatter expresses metadata. class LinkedDataFormatter ( DisplayFormatter ): metadata_formatter = Instance ( MetadataFormatter , args = ()) def format ( self , object , include = None , exclude = None ): data , meta = super () . format ( object , include , exclude ) g = self . metadata_formatter ( object ) if g : meta [ GRAPH ] = g return data , meta def load_ipython_extension ( shell = get_ipython ()): shell . display_formatter = LinkedDataFormatter ( ** shell . display_formatter . _trait_values ) shell . user_ns [ \"set_metadata\" ] = shell . display_formatter . metadata_formatter . set_metadata def unload_ipython_extension ( shell = get_ipython ()): shell . display_formatter = DisplayFormatter ( ** shell . display_formatter . _trait_values ) extend how the graph is generated for tuples and strings as examples. @MetadataFormatter . get_graph . register ( tuple ) def get_graph_tuple ( self , object ): return list ( map ( self . get_graph , object )) register a different id for modules. we use their namespaces for expansion later. @MetadataFormatter . get_id . register ( types . ModuleType ) def get_name ( self , object ): return object . __name__ activate the display formatter ACTIVE and load_ipython_extension () some data for the graph \u00a4 dataframes \u00a4 create a custom graph expression for pandas.DataFrame s import pandas if ACTIVE : shell . display_formatter . metadata_formatter . get_graph . register ( pandas . DataFrame )( lambda s , x : { ID : s . get_id ( x ), TYPE : s . get_id ( type ( x )), \"pandas.DataFrame:shape\" : list ( x . shape )}) if ACTIVE : import pandas df = pandas . DataFrame () display (( df , pandas , pandas . DataFrame )) string or url \u00a4 if there is a url hidden in a string we can elevate that as metadata thereby linked it to a cell. for example, this work revists https://nbviewer.org/gist/tonyfast/16d3bc82d69890949212b46040bd86e1 so we'll include that in the graph. @MetadataFormatter . get_id . register ( str ) def get_graph_str ( self , object ): from urllib.parse import urlparse parsed = urlparse ( object ) if parsed . scheme : return object \"https://nbviewer.org/gist/tonyfast/16d3bc82d69890949212b46040bd86e1\" looking at the metadata graph \u00a4 our choice of \"@id\" and \"@type\" are jsonld conventions. through these conventions we can surface the metadata by creating a jsonld context. we can ensure a consistent structure of the notebook and thereby context. ctx = { \"cells\" : { ID : \"nb:cell\" , CONTAINER : \"@list\" , CONTEXT : { \"outputs\" : { CONTEXT : { \"metadata\" : NEST }, ID : \"cell:metadata\" }, \"id\" : { ID : \"cell:id\" , TYPE : ID }, \"cell_type\" : \"cell:type\" , \"metadata\" : { ID : \"cell:metadata\" , CONTAINER : GRAPH , CONTEXT : { \"tags\" : \"rdf:name\" } }, } }, \"@version\" : 1.1 } if ACTIVE : file = Path ( \"2022-10-29-metadata-formatter.ipynb\" ) data = __import__ ( \"json\" ) . loads ( file . read_text ()) if ACTIVE : from pyld import jsonld from IPython.display import JSON from_local = jsonld . compact ( data , {}, options = dict ( expandContext = ctx )) set_metadata ( from_local , ** { \"rdf:description\" : \"all of the things we can expand from the notebook metadata.\" }) display ( from_local ) when the post is published we can condense the notation. remote = \"https://raw.githubusercontent.com/tonyfast/tonyfast/main/tonyfast/xxii/oct/2022-10-29-metadata-formatter.ipynb\" ; remote if ACTIVE : from_remote = jsonld . compact ( remote , {}, options = dict ( expandContext = ctx ) ) display ( from_remote ) this notebook is certified to have metadata things we capture \u00a4 in this proof of concept we don't capture much, but we do expose machinery to test this concept further and extend. we capture: kernel session id which can verify the outputs are generated in the same session each cell id that makes it possible link back to the source cells. some python variable information. things we can capture in the graph. \u00a4 annotations are type to id mappings. we could trace function calls with could encode imports we could capture variable assignment","title":"inferring linked data from <pre>IPython</pre> run times."},{"location":"xxii/oct/2022-10-29-metadata-formatter.html#inferring-linked-data-from-ipython-run-times","text":"automatically exporting rdf data during interactive computing. we'll modify IPython s display formatter to include a system that describes python objects as json linked data. from functools import singledispatch , singledispatchmethod import types , gc , re , sys from IPython.core.formatters import DisplayFormatter , BaseFormatter , catch_format_error , JSONFormatter from traitlets import ObjectName , Unicode , Instance , List , Any from IPython import get_ipython from pathlib import Path TYPE , ID , GRAPH , CONTAINER , NEST , CONTEXT = \"@type @id @graph @container @nest @context\" . split () MAIN = __name__ == \"__main__\" ACTIVE = \"__file__\" not in locals () and MAIN shell = get_ipython () a MetadataFormatter for including linked data in IPython reprs. this class carries machinery to generate: ids for python types and objects as urns MetadataFormatter.get_id linked data representations of python objects with MetadataFormatter.get_graph MetadataFormatter.for_type, MetadataFormatter.get_id.register, MetadataFormatter.get_graph.register extra the expression of the linked data graphs.","title":"inferring linked data from IPython run times."},{"location":"xxii/oct/2022-10-29-metadata-formatter.html#custom-metadata-formatter","text":"class MetadataFormatter ( BaseFormatter ): graph , format_type , = List (), Unicode ( 'application/ld+json' ) _return_type , print_method = ( list , dict ), ObjectName ( '_repr_metadata_' ) @singledispatchmethod def get_id ( self , object ): return next ( self . get_object ( object ), None ) @singledispatchmethod def get_graph ( self , object ): data = { TYPE : self . get_id ( type ( object ))} id = self . get_id ( object ) if id : data . setdefault ( ID , id ) if isinstance ( id , list ): return [{ ID : x , ** data } for x in id ] return data def get_object ( self , object , filter = None ): if isinstance ( filter , str ): filter = re . compile ( filter ) for referrer in ( x for x in gc . get_referrers ( object ) if isinstance ( x , dict )): yield from self . get_object_from_ns ( referrer , object , filter = filter ) def get_object_from_ns ( self , ns , object , filter = None ): weakref = ns . get ( \"__weakref__\" ) parent = None if weakref : parent = self . get_id ( weakref . __objclass__ ) else : parent = ns . get ( \"__module__\" , ns . get ( \"__name__\" )) if parent : parent += \":\" if not parent and ns is sys . modules : return object . __name__ for k in ( k for k , v in ns . items () if v is object and not k . startswith ( \"_\" )): name = F \" { parent or 'noparent' } # { k } \" if filter is not None and not filter . match ( name ): continue yield name def get_session_cell_id ( self ): data = get_ipython () . kernel . get_parent () return data [ \"metadata\" ][ \"cellId\" ], data [ \"header\" ][ \"session\" ] def set_metadata ( self , object = None , ** kwargs ): ids = dict ( zip (( \"cell:id\" , \"session:id\" ), self . get_session_cell_id ())) node = {} if object is None else self . get_graph ( object ) if isinstance ( node , dict ): node = [ node ] for node in node : self . graph . append ({ ** ids , ** node , ** kwargs }) def __call__ ( self , object ): explicit = super () . __call__ ( object ) if explicit : if isinstance ( explicit , dict ): self . set_metadata ( ** explicit ) else : for e in explicit : self . set_metadata ( ** w ) else : self . set_metadata ( object ) try : return self . graph [:] finally : self . graph . clear ()","title":"custom metadata formatter"},{"location":"xxii/oct/2022-10-29-metadata-formatter.html#custom-display-formatter","text":"the LinkedDataFormatter customizes how IPython s normal DisplayFormatter expresses metadata. class LinkedDataFormatter ( DisplayFormatter ): metadata_formatter = Instance ( MetadataFormatter , args = ()) def format ( self , object , include = None , exclude = None ): data , meta = super () . format ( object , include , exclude ) g = self . metadata_formatter ( object ) if g : meta [ GRAPH ] = g return data , meta def load_ipython_extension ( shell = get_ipython ()): shell . display_formatter = LinkedDataFormatter ( ** shell . display_formatter . _trait_values ) shell . user_ns [ \"set_metadata\" ] = shell . display_formatter . metadata_formatter . set_metadata def unload_ipython_extension ( shell = get_ipython ()): shell . display_formatter = DisplayFormatter ( ** shell . display_formatter . _trait_values ) extend how the graph is generated for tuples and strings as examples. @MetadataFormatter . get_graph . register ( tuple ) def get_graph_tuple ( self , object ): return list ( map ( self . get_graph , object )) register a different id for modules. we use their namespaces for expansion later. @MetadataFormatter . get_id . register ( types . ModuleType ) def get_name ( self , object ): return object . __name__ activate the display formatter ACTIVE and load_ipython_extension ()","title":"custom display formatter"},{"location":"xxii/oct/2022-10-29-metadata-formatter.html#some-data-for-the-graph","text":"","title":"some data for the graph"},{"location":"xxii/oct/2022-10-29-metadata-formatter.html#dataframes","text":"create a custom graph expression for pandas.DataFrame s import pandas if ACTIVE : shell . display_formatter . metadata_formatter . get_graph . register ( pandas . DataFrame )( lambda s , x : { ID : s . get_id ( x ), TYPE : s . get_id ( type ( x )), \"pandas.DataFrame:shape\" : list ( x . shape )}) if ACTIVE : import pandas df = pandas . DataFrame () display (( df , pandas , pandas . DataFrame ))","title":"dataframes"},{"location":"xxii/oct/2022-10-29-metadata-formatter.html#string-or-url","text":"if there is a url hidden in a string we can elevate that as metadata thereby linked it to a cell. for example, this work revists https://nbviewer.org/gist/tonyfast/16d3bc82d69890949212b46040bd86e1 so we'll include that in the graph. @MetadataFormatter . get_id . register ( str ) def get_graph_str ( self , object ): from urllib.parse import urlparse parsed = urlparse ( object ) if parsed . scheme : return object \"https://nbviewer.org/gist/tonyfast/16d3bc82d69890949212b46040bd86e1\"","title":"string or url"},{"location":"xxii/oct/2022-10-29-metadata-formatter.html#looking-at-the-metadata-graph","text":"our choice of \"@id\" and \"@type\" are jsonld conventions. through these conventions we can surface the metadata by creating a jsonld context. we can ensure a consistent structure of the notebook and thereby context. ctx = { \"cells\" : { ID : \"nb:cell\" , CONTAINER : \"@list\" , CONTEXT : { \"outputs\" : { CONTEXT : { \"metadata\" : NEST }, ID : \"cell:metadata\" }, \"id\" : { ID : \"cell:id\" , TYPE : ID }, \"cell_type\" : \"cell:type\" , \"metadata\" : { ID : \"cell:metadata\" , CONTAINER : GRAPH , CONTEXT : { \"tags\" : \"rdf:name\" } }, } }, \"@version\" : 1.1 } if ACTIVE : file = Path ( \"2022-10-29-metadata-formatter.ipynb\" ) data = __import__ ( \"json\" ) . loads ( file . read_text ()) if ACTIVE : from pyld import jsonld from IPython.display import JSON from_local = jsonld . compact ( data , {}, options = dict ( expandContext = ctx )) set_metadata ( from_local , ** { \"rdf:description\" : \"all of the things we can expand from the notebook metadata.\" }) display ( from_local ) when the post is published we can condense the notation. remote = \"https://raw.githubusercontent.com/tonyfast/tonyfast/main/tonyfast/xxii/oct/2022-10-29-metadata-formatter.ipynb\" ; remote if ACTIVE : from_remote = jsonld . compact ( remote , {}, options = dict ( expandContext = ctx ) ) display ( from_remote ) this notebook is certified to have metadata","title":"looking at the metadata graph"},{"location":"xxii/oct/2022-10-29-metadata-formatter.html#things-we-capture","text":"in this proof of concept we don't capture much, but we do expose machinery to test this concept further and extend. we capture: kernel session id which can verify the outputs are generated in the same session each cell id that makes it possible link back to the source cells. some python variable information.","title":"things we capture"},{"location":"xxii/oct/2022-10-29-metadata-formatter.html#things-we-can-capture-in-the-graph","text":"annotations are type to id mappings. we could trace function calls with could encode imports we could capture variable assignment","title":"things we can capture in the graph."},{"location":"xxii/oct/2022-11-17-assignment-expression-display.html","text":"using assignment expressions to display and assign in IPython \u00a4 not all notebok users are aware that there are different implicit display conditions that can be configured with IPython with the ast_node_interactivity option shell = get_ipython () by default, shell.ast_node_interactivity displays the last expressions shell . ast_node_interactivity the other 5 options follow the code below shell . traits ()[ \"ast_node_interactivity\" ] . values sometimes when i am debugging i want to store a variable and display it at the same. the IPython approach set shell.ast_node_interactivity = \"last_expr_or_assign\" . admittedly, i never choose this because i don't want it all the time, just while debugging. what i do instead is set the variable and append an implicit display at the end. my_variable = 42 ; my_variable i've shifted from this approach to using assignment expressions with make more sense. ( my_variable := 42 ) it feels lispy and i like it. it feels good with some widgets too. ( button := __import__ ( \"ipywidgets\" ) . Button ( description = \"a butt\" ))","title":"using assignment expressions to display and assign in <pre>IPython</pre>"},{"location":"xxii/oct/2022-11-17-assignment-expression-display.html#using-assignment-expressions-to-display-and-assign-in-ipython","text":"not all notebok users are aware that there are different implicit display conditions that can be configured with IPython with the ast_node_interactivity option shell = get_ipython () by default, shell.ast_node_interactivity displays the last expressions shell . ast_node_interactivity the other 5 options follow the code below shell . traits ()[ \"ast_node_interactivity\" ] . values sometimes when i am debugging i want to store a variable and display it at the same. the IPython approach set shell.ast_node_interactivity = \"last_expr_or_assign\" . admittedly, i never choose this because i don't want it all the time, just while debugging. what i do instead is set the variable and append an implicit display at the end. my_variable = 42 ; my_variable i've shifted from this approach to using assignment expressions with make more sense. ( my_variable := 42 ) it feels lispy and i like it. it feels good with some widgets too. ( button := __import__ ( \"ipywidgets\" ) . Button ( description = \"a butt\" ))","title":"using assignment expressions to display and assign in IPython"},{"location":"xxii/oct/colormap-dataframes/2021-10-11-colorizing.html","text":"what do we learn when we make colormaps from dataframes \u00a4 import pandas , numpy , toolz.curried as toolz we'll begin with a small linear domain of values between 0..1. domain = numpy . linspace ( 0 , 1 , 11 ) we'll focus on primary color components to start. from the primary components we can make a lot of colors. we combine the domain with empty values, to create triplets that will become our color. triples = list ( map ( numpy . array , zip ( domain , domain * 0 , domain * 0 ))) we'll start with making red , with a focus on rgb values, as a series of triples between 0..255. red = pandas . Series ( triples ) . apply ( numpy . array ) . mul ( 255 ) red def rgb ( x ): \"format a pandas cells\" ; return \"font-size: 0px; background-color: rgb(\" + \", \" . join ( map ( str , x ) ) + \");\" why are we calling this value red ? because we can explicitly the format for each cell and (255, 0, 0) is completely red. red . to_frame ( \"red\" ) . T . applymap ( rgb ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 10 red font-size: 0px; background-color: rgb(0.0, 0.0... font-size: 0px; background-color: rgb(25.5, 0.... font-size: 0px; background-color: rgb(51.0, 0.... font-size: 0px; background-color: rgb(76.50000... font-size: 0px; background-color: rgb(102.0, 0... font-size: 0px; background-color: rgb(127.5, 0... font-size: 0px; background-color: rgb(153.0000... font-size: 0px; background-color: rgb(178.5000... font-size: 0px; background-color: rgb(204.0, 0... font-size: 0px; background-color: rgb(229.5, 0... font-size: 0px; background-color: rgb(255.0, 0... with a small change to code in the cell above, we can indeed colorize our table. red . to_frame ( \"red\" ) . T . style . applymap ( rgb ) #T_28aad_row0_col0 { font-size: 0px; background-color: rgb(0.0, 0.0, 0.0); } #T_28aad_row0_col1 { font-size: 0px; background-color: rgb(25.5, 0.0, 0.0); } #T_28aad_row0_col2 { font-size: 0px; background-color: rgb(51.0, 0.0, 0.0); } #T_28aad_row0_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 0.0, 0.0); } #T_28aad_row0_col4 { font-size: 0px; background-color: rgb(102.0, 0.0, 0.0); } #T_28aad_row0_col5 { font-size: 0px; background-color: rgb(127.5, 0.0, 0.0); } #T_28aad_row0_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 0.0, 0.0); } #T_28aad_row0_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 0.0, 0.0); } #T_28aad_row0_col8 { font-size: 0px; background-color: rgb(204.0, 0.0, 0.0); } #T_28aad_row0_col9 { font-size: 0px; background-color: rgb(229.5, 0.0, 0.0); } #T_28aad_row0_col10 { font-size: 0px; background-color: rgb(255.0, 0.0, 0.0); } 0 1 2 3 4 5 6 7 8 9 10 red [0. 0. 0.] [25.5 0. 0. ] [51. 0. 0.] [76.5 0. 0. ] [102. 0. 0.] [127.5 0. 0. ] [153. 0. 0.] [178.5 0. 0. ] [204. 0. 0.] [229.5 0. 0. ] [255. 0. 0.] green and blue are a shift of the indices in the triple. @toolz . curry def shift ( n , x ): return numpy . concatenate ([ x [ n :], x [: n ]]) with shift(1) for blue, and shift(2) for green we can construct a composite dataframe with all of our rgb colorvalues ... colors = \"red blue green\" . split () df = pandas . concat ({ c : red . apply ( shift ( i )) . rename ( c ) for i , c in enumerate ( colors ) }, axis = 1 ) . T ; df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 10 red [0.0, 0.0, 0.0] [25.5, 0.0, 0.0] [51.0, 0.0, 0.0] [76.50000000000001, 0.0, 0.0] [102.0, 0.0, 0.0] [127.5, 0.0, 0.0] [153.00000000000003, 0.0, 0.0] [178.50000000000003, 0.0, 0.0] [204.0, 0.0, 0.0] [229.5, 0.0, 0.0] [255.0, 0.0, 0.0] blue [0.0, 0.0, 0.0] [0.0, 0.0, 25.5] [0.0, 0.0, 51.0] [0.0, 0.0, 76.50000000000001] [0.0, 0.0, 102.0] [0.0, 0.0, 127.5] [0.0, 0.0, 153.00000000000003] [0.0, 0.0, 178.50000000000003] [0.0, 0.0, 204.0] [0.0, 0.0, 229.5] [0.0, 0.0, 255.0] green [0.0, 0.0, 0.0] [0.0, 25.5, 0.0] [0.0, 51.0, 0.0] [0.0, 76.50000000000001, 0.0] [0.0, 102.0, 0.0] [0.0, 127.5, 0.0] [0.0, 153.00000000000003, 0.0] [0.0, 178.50000000000003, 0.0] [0.0, 204.0, 0.0] [0.0, 229.5, 0.0] [0.0, 255.0, 0.0] that may be stylized using the df.style feature df . style . applymap ( rgb ) #T_dd8aa_row0_col0, #T_dd8aa_row1_col0, #T_dd8aa_row2_col0 { font-size: 0px; background-color: rgb(0.0, 0.0, 0.0); } #T_dd8aa_row0_col1 { font-size: 0px; background-color: rgb(25.5, 0.0, 0.0); } #T_dd8aa_row0_col2 { font-size: 0px; background-color: rgb(51.0, 0.0, 0.0); } #T_dd8aa_row0_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 0.0, 0.0); } #T_dd8aa_row0_col4 { font-size: 0px; background-color: rgb(102.0, 0.0, 0.0); } #T_dd8aa_row0_col5 { font-size: 0px; background-color: rgb(127.5, 0.0, 0.0); } #T_dd8aa_row0_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 0.0, 0.0); } #T_dd8aa_row0_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 0.0, 0.0); } #T_dd8aa_row0_col8 { font-size: 0px; background-color: rgb(204.0, 0.0, 0.0); } #T_dd8aa_row0_col9 { font-size: 0px; background-color: rgb(229.5, 0.0, 0.0); } #T_dd8aa_row0_col10 { font-size: 0px; background-color: rgb(255.0, 0.0, 0.0); } #T_dd8aa_row1_col1 { font-size: 0px; background-color: rgb(0.0, 0.0, 25.5); } #T_dd8aa_row1_col2 { font-size: 0px; background-color: rgb(0.0, 0.0, 51.0); } #T_dd8aa_row1_col3 { font-size: 0px; background-color: rgb(0.0, 0.0, 76.50000000000001); } #T_dd8aa_row1_col4 { font-size: 0px; background-color: rgb(0.0, 0.0, 102.0); } #T_dd8aa_row1_col5 { font-size: 0px; background-color: rgb(0.0, 0.0, 127.5); } #T_dd8aa_row1_col6 { font-size: 0px; background-color: rgb(0.0, 0.0, 153.00000000000003); } #T_dd8aa_row1_col7 { font-size: 0px; background-color: rgb(0.0, 0.0, 178.50000000000003); } #T_dd8aa_row1_col8 { font-size: 0px; background-color: rgb(0.0, 0.0, 204.0); } #T_dd8aa_row1_col9 { font-size: 0px; background-color: rgb(0.0, 0.0, 229.5); } #T_dd8aa_row1_col10 { font-size: 0px; background-color: rgb(0.0, 0.0, 255.0); } #T_dd8aa_row2_col1 { font-size: 0px; background-color: rgb(0.0, 25.5, 0.0); } #T_dd8aa_row2_col2 { font-size: 0px; background-color: rgb(0.0, 51.0, 0.0); } #T_dd8aa_row2_col3 { font-size: 0px; background-color: rgb(0.0, 76.50000000000001, 0.0); } #T_dd8aa_row2_col4 { font-size: 0px; background-color: rgb(0.0, 102.0, 0.0); } #T_dd8aa_row2_col5 { font-size: 0px; background-color: rgb(0.0, 127.5, 0.0); } #T_dd8aa_row2_col6 { font-size: 0px; background-color: rgb(0.0, 153.00000000000003, 0.0); } #T_dd8aa_row2_col7 { font-size: 0px; background-color: rgb(0.0, 178.50000000000003, 0.0); } #T_dd8aa_row2_col8 { font-size: 0px; background-color: rgb(0.0, 204.0, 0.0); } #T_dd8aa_row2_col9 { font-size: 0px; background-color: rgb(0.0, 229.5, 0.0); } #T_dd8aa_row2_col10 { font-size: 0px; background-color: rgb(0.0, 255.0, 0.0); } 0 1 2 3 4 5 6 7 8 9 10 red [0. 0. 0.] [25.5 0. 0. ] [51. 0. 0.] [76.5 0. 0. ] [102. 0. 0.] [127.5 0. 0. ] [153. 0. 0.] [178.5 0. 0. ] [204. 0. 0.] [229.5 0. 0. ] [255. 0. 0.] blue [0. 0. 0.] [ 0. 0. 25.5] [ 0. 0. 51.] [ 0. 0. 76.5] [ 0. 0. 102.] [ 0. 0. 127.5] [ 0. 0. 153.] [ 0. 0. 178.5] [ 0. 0. 204.] [ 0. 0. 229.5] [ 0. 0. 255.] green [0. 0. 0.] [ 0. 25.5 0. ] [ 0. 51. 0.] [ 0. 76.5 0. ] [ 0. 102. 0.] [ 0. 127.5 0. ] [ 0. 153. 0.] [ 0. 178.5 0. ] [ 0. 204. 0.] [ 0. 229.5 0. ] [ 0. 255. 0.] from our df containing a primary palette we can build a secondary palette. secondary = pandas . concat ([ df . loc [ \"red\" ] . add ( df . loc [ \"blue\" ]) . rename ( \"violet\" ), df . loc [ \"red\" ] . add ( df . loc [ \"green\" ]) . rename ( \"yellow\" ), df . loc [ \"blue\" ] . add ( df . loc [ \"green\" ]) . rename ( \"cyan\" ) ], axis = 1 ) . T ; secondary . style . applymap ( rgb ) #T_7fb02_row0_col0, #T_7fb02_row1_col0, #T_7fb02_row2_col0 { font-size: 0px; background-color: rgb(0.0, 0.0, 0.0); } #T_7fb02_row0_col1 { font-size: 0px; background-color: rgb(25.5, 0.0, 25.5); } #T_7fb02_row0_col2 { font-size: 0px; background-color: rgb(51.0, 0.0, 51.0); } #T_7fb02_row0_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 0.0, 76.50000000000001); } #T_7fb02_row0_col4 { font-size: 0px; background-color: rgb(102.0, 0.0, 102.0); } #T_7fb02_row0_col5 { font-size: 0px; background-color: rgb(127.5, 0.0, 127.5); } #T_7fb02_row0_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 0.0, 153.00000000000003); } #T_7fb02_row0_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 0.0, 178.50000000000003); } #T_7fb02_row0_col8 { font-size: 0px; background-color: rgb(204.0, 0.0, 204.0); } #T_7fb02_row0_col9 { font-size: 0px; background-color: rgb(229.5, 0.0, 229.5); } #T_7fb02_row0_col10 { font-size: 0px; background-color: rgb(255.0, 0.0, 255.0); } #T_7fb02_row1_col1 { font-size: 0px; background-color: rgb(25.5, 25.5, 0.0); } #T_7fb02_row1_col2 { font-size: 0px; background-color: rgb(51.0, 51.0, 0.0); } #T_7fb02_row1_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 76.50000000000001, 0.0); } #T_7fb02_row1_col4 { font-size: 0px; background-color: rgb(102.0, 102.0, 0.0); } #T_7fb02_row1_col5 { font-size: 0px; background-color: rgb(127.5, 127.5, 0.0); } #T_7fb02_row1_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 153.00000000000003, 0.0); } #T_7fb02_row1_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 178.50000000000003, 0.0); } #T_7fb02_row1_col8 { font-size: 0px; background-color: rgb(204.0, 204.0, 0.0); } #T_7fb02_row1_col9 { font-size: 0px; background-color: rgb(229.5, 229.5, 0.0); } #T_7fb02_row1_col10 { font-size: 0px; background-color: rgb(255.0, 255.0, 0.0); } #T_7fb02_row2_col1 { font-size: 0px; background-color: rgb(0.0, 25.5, 25.5); } #T_7fb02_row2_col2 { font-size: 0px; background-color: rgb(0.0, 51.0, 51.0); } #T_7fb02_row2_col3 { font-size: 0px; background-color: rgb(0.0, 76.50000000000001, 76.50000000000001); } #T_7fb02_row2_col4 { font-size: 0px; background-color: rgb(0.0, 102.0, 102.0); } #T_7fb02_row2_col5 { font-size: 0px; background-color: rgb(0.0, 127.5, 127.5); } #T_7fb02_row2_col6 { font-size: 0px; background-color: rgb(0.0, 153.00000000000003, 153.00000000000003); } #T_7fb02_row2_col7 { font-size: 0px; background-color: rgb(0.0, 178.50000000000003, 178.50000000000003); } #T_7fb02_row2_col8 { font-size: 0px; background-color: rgb(0.0, 204.0, 204.0); } #T_7fb02_row2_col9 { font-size: 0px; background-color: rgb(0.0, 229.5, 229.5); } #T_7fb02_row2_col10 { font-size: 0px; background-color: rgb(0.0, 255.0, 255.0); } 0 1 2 3 4 5 6 7 8 9 10 violet [0. 0. 0.] [25.5 0. 25.5] [51. 0. 51.] [76.5 0. 76.5] [102. 0. 102.] [127.5 0. 127.5] [153. 0. 153.] [178.5 0. 178.5] [204. 0. 204.] [229.5 0. 229.5] [255. 0. 255.] yellow [0. 0. 0.] [25.5 25.5 0. ] [51. 51. 0.] [76.5 76.5 0. ] [102. 102. 0.] [127.5 127.5 0. ] [153. 153. 0.] [178.5 178.5 0. ] [204. 204. 0.] [229.5 229.5 0. ] [255. 255. 0.] cyan [0. 0. 0.] [ 0. 25.5 25.5] [ 0. 51. 51.] [ 0. 76.5 76.5] [ 0. 102. 102.] [ 0. 127.5 127.5] [ 0. 153. 153.] [ 0. 178.5 178.5] [ 0. 204. 204.] [ 0. 229.5 229.5] [ 0. 255. 255.] they can all be recombined together on their respective order. all = pandas . concat ([ df , secondary ]) . loc [ \"red violet blue cyan green yellow\" . split ()] all . style . applymap ( rgb ) #T_5b15c_row0_col0, #T_5b15c_row1_col0, #T_5b15c_row2_col0, #T_5b15c_row3_col0, #T_5b15c_row4_col0, #T_5b15c_row5_col0 { font-size: 0px; background-color: rgb(0.0, 0.0, 0.0); } #T_5b15c_row0_col1 { font-size: 0px; background-color: rgb(25.5, 0.0, 0.0); } #T_5b15c_row0_col2 { font-size: 0px; background-color: rgb(51.0, 0.0, 0.0); } #T_5b15c_row0_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 0.0, 0.0); } #T_5b15c_row0_col4 { font-size: 0px; background-color: rgb(102.0, 0.0, 0.0); } #T_5b15c_row0_col5 { font-size: 0px; background-color: rgb(127.5, 0.0, 0.0); } #T_5b15c_row0_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 0.0, 0.0); } #T_5b15c_row0_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 0.0, 0.0); } #T_5b15c_row0_col8 { font-size: 0px; background-color: rgb(204.0, 0.0, 0.0); } #T_5b15c_row0_col9 { font-size: 0px; background-color: rgb(229.5, 0.0, 0.0); } #T_5b15c_row0_col10 { font-size: 0px; background-color: rgb(255.0, 0.0, 0.0); } #T_5b15c_row1_col1 { font-size: 0px; background-color: rgb(25.5, 0.0, 25.5); } #T_5b15c_row1_col2 { font-size: 0px; background-color: rgb(51.0, 0.0, 51.0); } #T_5b15c_row1_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 0.0, 76.50000000000001); } #T_5b15c_row1_col4 { font-size: 0px; background-color: rgb(102.0, 0.0, 102.0); } #T_5b15c_row1_col5 { font-size: 0px; background-color: rgb(127.5, 0.0, 127.5); } #T_5b15c_row1_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 0.0, 153.00000000000003); } #T_5b15c_row1_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 0.0, 178.50000000000003); } #T_5b15c_row1_col8 { font-size: 0px; background-color: rgb(204.0, 0.0, 204.0); } #T_5b15c_row1_col9 { font-size: 0px; background-color: rgb(229.5, 0.0, 229.5); } #T_5b15c_row1_col10 { font-size: 0px; background-color: rgb(255.0, 0.0, 255.0); } #T_5b15c_row2_col1 { font-size: 0px; background-color: rgb(0.0, 0.0, 25.5); } #T_5b15c_row2_col2 { font-size: 0px; background-color: rgb(0.0, 0.0, 51.0); } #T_5b15c_row2_col3 { font-size: 0px; background-color: rgb(0.0, 0.0, 76.50000000000001); } #T_5b15c_row2_col4 { font-size: 0px; background-color: rgb(0.0, 0.0, 102.0); } #T_5b15c_row2_col5 { font-size: 0px; background-color: rgb(0.0, 0.0, 127.5); } #T_5b15c_row2_col6 { font-size: 0px; background-color: rgb(0.0, 0.0, 153.00000000000003); } #T_5b15c_row2_col7 { font-size: 0px; background-color: rgb(0.0, 0.0, 178.50000000000003); } #T_5b15c_row2_col8 { font-size: 0px; background-color: rgb(0.0, 0.0, 204.0); } #T_5b15c_row2_col9 { font-size: 0px; background-color: rgb(0.0, 0.0, 229.5); } #T_5b15c_row2_col10 { font-size: 0px; background-color: rgb(0.0, 0.0, 255.0); } #T_5b15c_row3_col1 { font-size: 0px; background-color: rgb(0.0, 25.5, 25.5); } #T_5b15c_row3_col2 { font-size: 0px; background-color: rgb(0.0, 51.0, 51.0); } #T_5b15c_row3_col3 { font-size: 0px; background-color: rgb(0.0, 76.50000000000001, 76.50000000000001); } #T_5b15c_row3_col4 { font-size: 0px; background-color: rgb(0.0, 102.0, 102.0); } #T_5b15c_row3_col5 { font-size: 0px; background-color: rgb(0.0, 127.5, 127.5); } #T_5b15c_row3_col6 { font-size: 0px; background-color: rgb(0.0, 153.00000000000003, 153.00000000000003); } #T_5b15c_row3_col7 { font-size: 0px; background-color: rgb(0.0, 178.50000000000003, 178.50000000000003); } #T_5b15c_row3_col8 { font-size: 0px; background-color: rgb(0.0, 204.0, 204.0); } #T_5b15c_row3_col9 { font-size: 0px; background-color: rgb(0.0, 229.5, 229.5); } #T_5b15c_row3_col10 { font-size: 0px; background-color: rgb(0.0, 255.0, 255.0); } #T_5b15c_row4_col1 { font-size: 0px; background-color: rgb(0.0, 25.5, 0.0); } #T_5b15c_row4_col2 { font-size: 0px; background-color: rgb(0.0, 51.0, 0.0); } #T_5b15c_row4_col3 { font-size: 0px; background-color: rgb(0.0, 76.50000000000001, 0.0); } #T_5b15c_row4_col4 { font-size: 0px; background-color: rgb(0.0, 102.0, 0.0); } #T_5b15c_row4_col5 { font-size: 0px; background-color: rgb(0.0, 127.5, 0.0); } #T_5b15c_row4_col6 { font-size: 0px; background-color: rgb(0.0, 153.00000000000003, 0.0); } #T_5b15c_row4_col7 { font-size: 0px; background-color: rgb(0.0, 178.50000000000003, 0.0); } #T_5b15c_row4_col8 { font-size: 0px; background-color: rgb(0.0, 204.0, 0.0); } #T_5b15c_row4_col9 { font-size: 0px; background-color: rgb(0.0, 229.5, 0.0); } #T_5b15c_row4_col10 { font-size: 0px; background-color: rgb(0.0, 255.0, 0.0); } #T_5b15c_row5_col1 { font-size: 0px; background-color: rgb(25.5, 25.5, 0.0); } #T_5b15c_row5_col2 { font-size: 0px; background-color: rgb(51.0, 51.0, 0.0); } #T_5b15c_row5_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 76.50000000000001, 0.0); } #T_5b15c_row5_col4 { font-size: 0px; background-color: rgb(102.0, 102.0, 0.0); } #T_5b15c_row5_col5 { font-size: 0px; background-color: rgb(127.5, 127.5, 0.0); } #T_5b15c_row5_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 153.00000000000003, 0.0); } #T_5b15c_row5_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 178.50000000000003, 0.0); } #T_5b15c_row5_col8 { font-size: 0px; background-color: rgb(204.0, 204.0, 0.0); } #T_5b15c_row5_col9 { font-size: 0px; background-color: rgb(229.5, 229.5, 0.0); } #T_5b15c_row5_col10 { font-size: 0px; background-color: rgb(255.0, 255.0, 0.0); } 0 1 2 3 4 5 6 7 8 9 10 red [0. 0. 0.] [25.5 0. 0. ] [51. 0. 0.] [76.5 0. 0. ] [102. 0. 0.] [127.5 0. 0. ] [153. 0. 0.] [178.5 0. 0. ] [204. 0. 0.] [229.5 0. 0. ] [255. 0. 0.] violet [0. 0. 0.] [25.5 0. 25.5] [51. 0. 51.] [76.5 0. 76.5] [102. 0. 102.] [127.5 0. 127.5] [153. 0. 153.] [178.5 0. 178.5] [204. 0. 204.] [229.5 0. 229.5] [255. 0. 255.] blue [0. 0. 0.] [ 0. 0. 25.5] [ 0. 0. 51.] [ 0. 0. 76.5] [ 0. 0. 102.] [ 0. 0. 127.5] [ 0. 0. 153.] [ 0. 0. 178.5] [ 0. 0. 204.] [ 0. 0. 229.5] [ 0. 0. 255.] cyan [0. 0. 0.] [ 0. 25.5 25.5] [ 0. 51. 51.] [ 0. 76.5 76.5] [ 0. 102. 102.] [ 0. 127.5 127.5] [ 0. 153. 153.] [ 0. 178.5 178.5] [ 0. 204. 204.] [ 0. 229.5 229.5] [ 0. 255. 255.] green [0. 0. 0.] [ 0. 25.5 0. ] [ 0. 51. 0.] [ 0. 76.5 0. ] [ 0. 102. 0.] [ 0. 127.5 0. ] [ 0. 153. 0.] [ 0. 178.5 0. ] [ 0. 204. 0.] [ 0. 229.5 0. ] [ 0. 255. 0.] yellow [0. 0. 0.] [25.5 25.5 0. ] [51. 51. 0.] [76.5 76.5 0. ] [102. 102. 0.] [127.5 127.5 0. ] [153. 153. 0.] [178.5 178.5 0. ] [204. 204. 0.] [229.5 229.5 0. ] [255. 255. 0.]","title":"what do we learn when we <em>make colormaps from dataframes</em><!-- TEASER_END -->"},{"location":"xxii/oct/colormap-dataframes/2021-10-11-colorizing.html#what-do-we-learn-when-we-make-colormaps-from-dataframes","text":"import pandas , numpy , toolz.curried as toolz we'll begin with a small linear domain of values between 0..1. domain = numpy . linspace ( 0 , 1 , 11 ) we'll focus on primary color components to start. from the primary components we can make a lot of colors. we combine the domain with empty values, to create triplets that will become our color. triples = list ( map ( numpy . array , zip ( domain , domain * 0 , domain * 0 ))) we'll start with making red , with a focus on rgb values, as a series of triples between 0..255. red = pandas . Series ( triples ) . apply ( numpy . array ) . mul ( 255 ) red def rgb ( x ): \"format a pandas cells\" ; return \"font-size: 0px; background-color: rgb(\" + \", \" . join ( map ( str , x ) ) + \");\" why are we calling this value red ? because we can explicitly the format for each cell and (255, 0, 0) is completely red. red . to_frame ( \"red\" ) . T . applymap ( rgb ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 10 red font-size: 0px; background-color: rgb(0.0, 0.0... font-size: 0px; background-color: rgb(25.5, 0.... font-size: 0px; background-color: rgb(51.0, 0.... font-size: 0px; background-color: rgb(76.50000... font-size: 0px; background-color: rgb(102.0, 0... font-size: 0px; background-color: rgb(127.5, 0... font-size: 0px; background-color: rgb(153.0000... font-size: 0px; background-color: rgb(178.5000... font-size: 0px; background-color: rgb(204.0, 0... font-size: 0px; background-color: rgb(229.5, 0... font-size: 0px; background-color: rgb(255.0, 0... with a small change to code in the cell above, we can indeed colorize our table. red . to_frame ( \"red\" ) . T . style . applymap ( rgb ) #T_28aad_row0_col0 { font-size: 0px; background-color: rgb(0.0, 0.0, 0.0); } #T_28aad_row0_col1 { font-size: 0px; background-color: rgb(25.5, 0.0, 0.0); } #T_28aad_row0_col2 { font-size: 0px; background-color: rgb(51.0, 0.0, 0.0); } #T_28aad_row0_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 0.0, 0.0); } #T_28aad_row0_col4 { font-size: 0px; background-color: rgb(102.0, 0.0, 0.0); } #T_28aad_row0_col5 { font-size: 0px; background-color: rgb(127.5, 0.0, 0.0); } #T_28aad_row0_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 0.0, 0.0); } #T_28aad_row0_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 0.0, 0.0); } #T_28aad_row0_col8 { font-size: 0px; background-color: rgb(204.0, 0.0, 0.0); } #T_28aad_row0_col9 { font-size: 0px; background-color: rgb(229.5, 0.0, 0.0); } #T_28aad_row0_col10 { font-size: 0px; background-color: rgb(255.0, 0.0, 0.0); } 0 1 2 3 4 5 6 7 8 9 10 red [0. 0. 0.] [25.5 0. 0. ] [51. 0. 0.] [76.5 0. 0. ] [102. 0. 0.] [127.5 0. 0. ] [153. 0. 0.] [178.5 0. 0. ] [204. 0. 0.] [229.5 0. 0. ] [255. 0. 0.] green and blue are a shift of the indices in the triple. @toolz . curry def shift ( n , x ): return numpy . concatenate ([ x [ n :], x [: n ]]) with shift(1) for blue, and shift(2) for green we can construct a composite dataframe with all of our rgb colorvalues ... colors = \"red blue green\" . split () df = pandas . concat ({ c : red . apply ( shift ( i )) . rename ( c ) for i , c in enumerate ( colors ) }, axis = 1 ) . T ; df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 10 red [0.0, 0.0, 0.0] [25.5, 0.0, 0.0] [51.0, 0.0, 0.0] [76.50000000000001, 0.0, 0.0] [102.0, 0.0, 0.0] [127.5, 0.0, 0.0] [153.00000000000003, 0.0, 0.0] [178.50000000000003, 0.0, 0.0] [204.0, 0.0, 0.0] [229.5, 0.0, 0.0] [255.0, 0.0, 0.0] blue [0.0, 0.0, 0.0] [0.0, 0.0, 25.5] [0.0, 0.0, 51.0] [0.0, 0.0, 76.50000000000001] [0.0, 0.0, 102.0] [0.0, 0.0, 127.5] [0.0, 0.0, 153.00000000000003] [0.0, 0.0, 178.50000000000003] [0.0, 0.0, 204.0] [0.0, 0.0, 229.5] [0.0, 0.0, 255.0] green [0.0, 0.0, 0.0] [0.0, 25.5, 0.0] [0.0, 51.0, 0.0] [0.0, 76.50000000000001, 0.0] [0.0, 102.0, 0.0] [0.0, 127.5, 0.0] [0.0, 153.00000000000003, 0.0] [0.0, 178.50000000000003, 0.0] [0.0, 204.0, 0.0] [0.0, 229.5, 0.0] [0.0, 255.0, 0.0] that may be stylized using the df.style feature df . style . applymap ( rgb ) #T_dd8aa_row0_col0, #T_dd8aa_row1_col0, #T_dd8aa_row2_col0 { font-size: 0px; background-color: rgb(0.0, 0.0, 0.0); } #T_dd8aa_row0_col1 { font-size: 0px; background-color: rgb(25.5, 0.0, 0.0); } #T_dd8aa_row0_col2 { font-size: 0px; background-color: rgb(51.0, 0.0, 0.0); } #T_dd8aa_row0_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 0.0, 0.0); } #T_dd8aa_row0_col4 { font-size: 0px; background-color: rgb(102.0, 0.0, 0.0); } #T_dd8aa_row0_col5 { font-size: 0px; background-color: rgb(127.5, 0.0, 0.0); } #T_dd8aa_row0_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 0.0, 0.0); } #T_dd8aa_row0_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 0.0, 0.0); } #T_dd8aa_row0_col8 { font-size: 0px; background-color: rgb(204.0, 0.0, 0.0); } #T_dd8aa_row0_col9 { font-size: 0px; background-color: rgb(229.5, 0.0, 0.0); } #T_dd8aa_row0_col10 { font-size: 0px; background-color: rgb(255.0, 0.0, 0.0); } #T_dd8aa_row1_col1 { font-size: 0px; background-color: rgb(0.0, 0.0, 25.5); } #T_dd8aa_row1_col2 { font-size: 0px; background-color: rgb(0.0, 0.0, 51.0); } #T_dd8aa_row1_col3 { font-size: 0px; background-color: rgb(0.0, 0.0, 76.50000000000001); } #T_dd8aa_row1_col4 { font-size: 0px; background-color: rgb(0.0, 0.0, 102.0); } #T_dd8aa_row1_col5 { font-size: 0px; background-color: rgb(0.0, 0.0, 127.5); } #T_dd8aa_row1_col6 { font-size: 0px; background-color: rgb(0.0, 0.0, 153.00000000000003); } #T_dd8aa_row1_col7 { font-size: 0px; background-color: rgb(0.0, 0.0, 178.50000000000003); } #T_dd8aa_row1_col8 { font-size: 0px; background-color: rgb(0.0, 0.0, 204.0); } #T_dd8aa_row1_col9 { font-size: 0px; background-color: rgb(0.0, 0.0, 229.5); } #T_dd8aa_row1_col10 { font-size: 0px; background-color: rgb(0.0, 0.0, 255.0); } #T_dd8aa_row2_col1 { font-size: 0px; background-color: rgb(0.0, 25.5, 0.0); } #T_dd8aa_row2_col2 { font-size: 0px; background-color: rgb(0.0, 51.0, 0.0); } #T_dd8aa_row2_col3 { font-size: 0px; background-color: rgb(0.0, 76.50000000000001, 0.0); } #T_dd8aa_row2_col4 { font-size: 0px; background-color: rgb(0.0, 102.0, 0.0); } #T_dd8aa_row2_col5 { font-size: 0px; background-color: rgb(0.0, 127.5, 0.0); } #T_dd8aa_row2_col6 { font-size: 0px; background-color: rgb(0.0, 153.00000000000003, 0.0); } #T_dd8aa_row2_col7 { font-size: 0px; background-color: rgb(0.0, 178.50000000000003, 0.0); } #T_dd8aa_row2_col8 { font-size: 0px; background-color: rgb(0.0, 204.0, 0.0); } #T_dd8aa_row2_col9 { font-size: 0px; background-color: rgb(0.0, 229.5, 0.0); } #T_dd8aa_row2_col10 { font-size: 0px; background-color: rgb(0.0, 255.0, 0.0); } 0 1 2 3 4 5 6 7 8 9 10 red [0. 0. 0.] [25.5 0. 0. ] [51. 0. 0.] [76.5 0. 0. ] [102. 0. 0.] [127.5 0. 0. ] [153. 0. 0.] [178.5 0. 0. ] [204. 0. 0.] [229.5 0. 0. ] [255. 0. 0.] blue [0. 0. 0.] [ 0. 0. 25.5] [ 0. 0. 51.] [ 0. 0. 76.5] [ 0. 0. 102.] [ 0. 0. 127.5] [ 0. 0. 153.] [ 0. 0. 178.5] [ 0. 0. 204.] [ 0. 0. 229.5] [ 0. 0. 255.] green [0. 0. 0.] [ 0. 25.5 0. ] [ 0. 51. 0.] [ 0. 76.5 0. ] [ 0. 102. 0.] [ 0. 127.5 0. ] [ 0. 153. 0.] [ 0. 178.5 0. ] [ 0. 204. 0.] [ 0. 229.5 0. ] [ 0. 255. 0.] from our df containing a primary palette we can build a secondary palette. secondary = pandas . concat ([ df . loc [ \"red\" ] . add ( df . loc [ \"blue\" ]) . rename ( \"violet\" ), df . loc [ \"red\" ] . add ( df . loc [ \"green\" ]) . rename ( \"yellow\" ), df . loc [ \"blue\" ] . add ( df . loc [ \"green\" ]) . rename ( \"cyan\" ) ], axis = 1 ) . T ; secondary . style . applymap ( rgb ) #T_7fb02_row0_col0, #T_7fb02_row1_col0, #T_7fb02_row2_col0 { font-size: 0px; background-color: rgb(0.0, 0.0, 0.0); } #T_7fb02_row0_col1 { font-size: 0px; background-color: rgb(25.5, 0.0, 25.5); } #T_7fb02_row0_col2 { font-size: 0px; background-color: rgb(51.0, 0.0, 51.0); } #T_7fb02_row0_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 0.0, 76.50000000000001); } #T_7fb02_row0_col4 { font-size: 0px; background-color: rgb(102.0, 0.0, 102.0); } #T_7fb02_row0_col5 { font-size: 0px; background-color: rgb(127.5, 0.0, 127.5); } #T_7fb02_row0_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 0.0, 153.00000000000003); } #T_7fb02_row0_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 0.0, 178.50000000000003); } #T_7fb02_row0_col8 { font-size: 0px; background-color: rgb(204.0, 0.0, 204.0); } #T_7fb02_row0_col9 { font-size: 0px; background-color: rgb(229.5, 0.0, 229.5); } #T_7fb02_row0_col10 { font-size: 0px; background-color: rgb(255.0, 0.0, 255.0); } #T_7fb02_row1_col1 { font-size: 0px; background-color: rgb(25.5, 25.5, 0.0); } #T_7fb02_row1_col2 { font-size: 0px; background-color: rgb(51.0, 51.0, 0.0); } #T_7fb02_row1_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 76.50000000000001, 0.0); } #T_7fb02_row1_col4 { font-size: 0px; background-color: rgb(102.0, 102.0, 0.0); } #T_7fb02_row1_col5 { font-size: 0px; background-color: rgb(127.5, 127.5, 0.0); } #T_7fb02_row1_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 153.00000000000003, 0.0); } #T_7fb02_row1_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 178.50000000000003, 0.0); } #T_7fb02_row1_col8 { font-size: 0px; background-color: rgb(204.0, 204.0, 0.0); } #T_7fb02_row1_col9 { font-size: 0px; background-color: rgb(229.5, 229.5, 0.0); } #T_7fb02_row1_col10 { font-size: 0px; background-color: rgb(255.0, 255.0, 0.0); } #T_7fb02_row2_col1 { font-size: 0px; background-color: rgb(0.0, 25.5, 25.5); } #T_7fb02_row2_col2 { font-size: 0px; background-color: rgb(0.0, 51.0, 51.0); } #T_7fb02_row2_col3 { font-size: 0px; background-color: rgb(0.0, 76.50000000000001, 76.50000000000001); } #T_7fb02_row2_col4 { font-size: 0px; background-color: rgb(0.0, 102.0, 102.0); } #T_7fb02_row2_col5 { font-size: 0px; background-color: rgb(0.0, 127.5, 127.5); } #T_7fb02_row2_col6 { font-size: 0px; background-color: rgb(0.0, 153.00000000000003, 153.00000000000003); } #T_7fb02_row2_col7 { font-size: 0px; background-color: rgb(0.0, 178.50000000000003, 178.50000000000003); } #T_7fb02_row2_col8 { font-size: 0px; background-color: rgb(0.0, 204.0, 204.0); } #T_7fb02_row2_col9 { font-size: 0px; background-color: rgb(0.0, 229.5, 229.5); } #T_7fb02_row2_col10 { font-size: 0px; background-color: rgb(0.0, 255.0, 255.0); } 0 1 2 3 4 5 6 7 8 9 10 violet [0. 0. 0.] [25.5 0. 25.5] [51. 0. 51.] [76.5 0. 76.5] [102. 0. 102.] [127.5 0. 127.5] [153. 0. 153.] [178.5 0. 178.5] [204. 0. 204.] [229.5 0. 229.5] [255. 0. 255.] yellow [0. 0. 0.] [25.5 25.5 0. ] [51. 51. 0.] [76.5 76.5 0. ] [102. 102. 0.] [127.5 127.5 0. ] [153. 153. 0.] [178.5 178.5 0. ] [204. 204. 0.] [229.5 229.5 0. ] [255. 255. 0.] cyan [0. 0. 0.] [ 0. 25.5 25.5] [ 0. 51. 51.] [ 0. 76.5 76.5] [ 0. 102. 102.] [ 0. 127.5 127.5] [ 0. 153. 153.] [ 0. 178.5 178.5] [ 0. 204. 204.] [ 0. 229.5 229.5] [ 0. 255. 255.] they can all be recombined together on their respective order. all = pandas . concat ([ df , secondary ]) . loc [ \"red violet blue cyan green yellow\" . split ()] all . style . applymap ( rgb ) #T_5b15c_row0_col0, #T_5b15c_row1_col0, #T_5b15c_row2_col0, #T_5b15c_row3_col0, #T_5b15c_row4_col0, #T_5b15c_row5_col0 { font-size: 0px; background-color: rgb(0.0, 0.0, 0.0); } #T_5b15c_row0_col1 { font-size: 0px; background-color: rgb(25.5, 0.0, 0.0); } #T_5b15c_row0_col2 { font-size: 0px; background-color: rgb(51.0, 0.0, 0.0); } #T_5b15c_row0_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 0.0, 0.0); } #T_5b15c_row0_col4 { font-size: 0px; background-color: rgb(102.0, 0.0, 0.0); } #T_5b15c_row0_col5 { font-size: 0px; background-color: rgb(127.5, 0.0, 0.0); } #T_5b15c_row0_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 0.0, 0.0); } #T_5b15c_row0_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 0.0, 0.0); } #T_5b15c_row0_col8 { font-size: 0px; background-color: rgb(204.0, 0.0, 0.0); } #T_5b15c_row0_col9 { font-size: 0px; background-color: rgb(229.5, 0.0, 0.0); } #T_5b15c_row0_col10 { font-size: 0px; background-color: rgb(255.0, 0.0, 0.0); } #T_5b15c_row1_col1 { font-size: 0px; background-color: rgb(25.5, 0.0, 25.5); } #T_5b15c_row1_col2 { font-size: 0px; background-color: rgb(51.0, 0.0, 51.0); } #T_5b15c_row1_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 0.0, 76.50000000000001); } #T_5b15c_row1_col4 { font-size: 0px; background-color: rgb(102.0, 0.0, 102.0); } #T_5b15c_row1_col5 { font-size: 0px; background-color: rgb(127.5, 0.0, 127.5); } #T_5b15c_row1_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 0.0, 153.00000000000003); } #T_5b15c_row1_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 0.0, 178.50000000000003); } #T_5b15c_row1_col8 { font-size: 0px; background-color: rgb(204.0, 0.0, 204.0); } #T_5b15c_row1_col9 { font-size: 0px; background-color: rgb(229.5, 0.0, 229.5); } #T_5b15c_row1_col10 { font-size: 0px; background-color: rgb(255.0, 0.0, 255.0); } #T_5b15c_row2_col1 { font-size: 0px; background-color: rgb(0.0, 0.0, 25.5); } #T_5b15c_row2_col2 { font-size: 0px; background-color: rgb(0.0, 0.0, 51.0); } #T_5b15c_row2_col3 { font-size: 0px; background-color: rgb(0.0, 0.0, 76.50000000000001); } #T_5b15c_row2_col4 { font-size: 0px; background-color: rgb(0.0, 0.0, 102.0); } #T_5b15c_row2_col5 { font-size: 0px; background-color: rgb(0.0, 0.0, 127.5); } #T_5b15c_row2_col6 { font-size: 0px; background-color: rgb(0.0, 0.0, 153.00000000000003); } #T_5b15c_row2_col7 { font-size: 0px; background-color: rgb(0.0, 0.0, 178.50000000000003); } #T_5b15c_row2_col8 { font-size: 0px; background-color: rgb(0.0, 0.0, 204.0); } #T_5b15c_row2_col9 { font-size: 0px; background-color: rgb(0.0, 0.0, 229.5); } #T_5b15c_row2_col10 { font-size: 0px; background-color: rgb(0.0, 0.0, 255.0); } #T_5b15c_row3_col1 { font-size: 0px; background-color: rgb(0.0, 25.5, 25.5); } #T_5b15c_row3_col2 { font-size: 0px; background-color: rgb(0.0, 51.0, 51.0); } #T_5b15c_row3_col3 { font-size: 0px; background-color: rgb(0.0, 76.50000000000001, 76.50000000000001); } #T_5b15c_row3_col4 { font-size: 0px; background-color: rgb(0.0, 102.0, 102.0); } #T_5b15c_row3_col5 { font-size: 0px; background-color: rgb(0.0, 127.5, 127.5); } #T_5b15c_row3_col6 { font-size: 0px; background-color: rgb(0.0, 153.00000000000003, 153.00000000000003); } #T_5b15c_row3_col7 { font-size: 0px; background-color: rgb(0.0, 178.50000000000003, 178.50000000000003); } #T_5b15c_row3_col8 { font-size: 0px; background-color: rgb(0.0, 204.0, 204.0); } #T_5b15c_row3_col9 { font-size: 0px; background-color: rgb(0.0, 229.5, 229.5); } #T_5b15c_row3_col10 { font-size: 0px; background-color: rgb(0.0, 255.0, 255.0); } #T_5b15c_row4_col1 { font-size: 0px; background-color: rgb(0.0, 25.5, 0.0); } #T_5b15c_row4_col2 { font-size: 0px; background-color: rgb(0.0, 51.0, 0.0); } #T_5b15c_row4_col3 { font-size: 0px; background-color: rgb(0.0, 76.50000000000001, 0.0); } #T_5b15c_row4_col4 { font-size: 0px; background-color: rgb(0.0, 102.0, 0.0); } #T_5b15c_row4_col5 { font-size: 0px; background-color: rgb(0.0, 127.5, 0.0); } #T_5b15c_row4_col6 { font-size: 0px; background-color: rgb(0.0, 153.00000000000003, 0.0); } #T_5b15c_row4_col7 { font-size: 0px; background-color: rgb(0.0, 178.50000000000003, 0.0); } #T_5b15c_row4_col8 { font-size: 0px; background-color: rgb(0.0, 204.0, 0.0); } #T_5b15c_row4_col9 { font-size: 0px; background-color: rgb(0.0, 229.5, 0.0); } #T_5b15c_row4_col10 { font-size: 0px; background-color: rgb(0.0, 255.0, 0.0); } #T_5b15c_row5_col1 { font-size: 0px; background-color: rgb(25.5, 25.5, 0.0); } #T_5b15c_row5_col2 { font-size: 0px; background-color: rgb(51.0, 51.0, 0.0); } #T_5b15c_row5_col3 { font-size: 0px; background-color: rgb(76.50000000000001, 76.50000000000001, 0.0); } #T_5b15c_row5_col4 { font-size: 0px; background-color: rgb(102.0, 102.0, 0.0); } #T_5b15c_row5_col5 { font-size: 0px; background-color: rgb(127.5, 127.5, 0.0); } #T_5b15c_row5_col6 { font-size: 0px; background-color: rgb(153.00000000000003, 153.00000000000003, 0.0); } #T_5b15c_row5_col7 { font-size: 0px; background-color: rgb(178.50000000000003, 178.50000000000003, 0.0); } #T_5b15c_row5_col8 { font-size: 0px; background-color: rgb(204.0, 204.0, 0.0); } #T_5b15c_row5_col9 { font-size: 0px; background-color: rgb(229.5, 229.5, 0.0); } #T_5b15c_row5_col10 { font-size: 0px; background-color: rgb(255.0, 255.0, 0.0); } 0 1 2 3 4 5 6 7 8 9 10 red [0. 0. 0.] [25.5 0. 0. ] [51. 0. 0.] [76.5 0. 0. ] [102. 0. 0.] [127.5 0. 0. ] [153. 0. 0.] [178.5 0. 0. ] [204. 0. 0.] [229.5 0. 0. ] [255. 0. 0.] violet [0. 0. 0.] [25.5 0. 25.5] [51. 0. 51.] [76.5 0. 76.5] [102. 0. 102.] [127.5 0. 127.5] [153. 0. 153.] [178.5 0. 178.5] [204. 0. 204.] [229.5 0. 229.5] [255. 0. 255.] blue [0. 0. 0.] [ 0. 0. 25.5] [ 0. 0. 51.] [ 0. 0. 76.5] [ 0. 0. 102.] [ 0. 0. 127.5] [ 0. 0. 153.] [ 0. 0. 178.5] [ 0. 0. 204.] [ 0. 0. 229.5] [ 0. 0. 255.] cyan [0. 0. 0.] [ 0. 25.5 25.5] [ 0. 51. 51.] [ 0. 76.5 76.5] [ 0. 102. 102.] [ 0. 127.5 127.5] [ 0. 153. 153.] [ 0. 178.5 178.5] [ 0. 204. 204.] [ 0. 229.5 229.5] [ 0. 255. 255.] green [0. 0. 0.] [ 0. 25.5 0. ] [ 0. 51. 0.] [ 0. 76.5 0. ] [ 0. 102. 0.] [ 0. 127.5 0. ] [ 0. 153. 0.] [ 0. 178.5 0. ] [ 0. 204. 0.] [ 0. 229.5 0. ] [ 0. 255. 0.] yellow [0. 0. 0.] [25.5 25.5 0. ] [51. 51. 0.] [76.5 76.5 0. ] [102. 102. 0.] [127.5 127.5 0. ] [153. 153. 0.] [178.5 178.5 0. ] [204. 204. 0.] [229.5 229.5 0. ] [255. 255. 0.]","title":"what do we learn when we make colormaps from dataframes"},{"location":"xxiii/2023-01-01-strawberry-server.html","text":"WIP: strawberry jupyter server \u00a4 a work in progress of a graphql schema for jupyter server. https://strawberry.rocks/docs/general/schema-basics # %pip install 'strawberry-graphql[debug-server]' from jupyter_server.services.contents.largefilemanager import LargeFileManager from pathlib import Path contents = LargeFileManager ( root_dir = str ( Path ( \"~\" ) . expanduser ())) import strawberry as S , typing as T , strawberry.cli , enum from pathlib import Path from datetime import datetime DICT = S . scalar ( T . NewType ( \"DICT\" , object ), description = \"a dictionary\" , serialize = lambda v : v , parse_value = lambda v : v , ) @strawberry . enum class ContentType ( enum . Enum ): FILE = \"file\" NOTEBOOK = \"notebook\" DIRECTORY = \"directory\" @S . type class Base : name : str path : str last_modified : datetime created : datetime format : str mimetype : T . Optional [ str ] size : T . Optional [ int ] writable : bool type : ContentType Cells = T . Union [ DICT ] @S . type class NbFormat : cells : list [ Cells ] metadata : DICT @S . type class Notebook ( Base ): content : DICT @S . type class File ( Base ): content : str @S . type class Directory ( Base ): content : T . Annotated [ \"Kinds\" , \"__main__\" ] Kinds = T . Union [ Notebook , File , Directory ] @S . type class Query : @S . field def search ( self , dir : str = \"\" , kind : T . Optional [ ContentType ] = None ) -> Kinds : return Directory ( ** contents . get ( dir , )) schema = strawberry . Schema ( Query ) from fastapi import FastAPI from strawberry.fastapi import GraphQLRouter graphql_app = GraphQLRouter ( schema ) app = FastAPI () app . include_router ( graphql_app , prefix = \"/graphql\" ) if ( I := \"__file__\" not in locals ()): from IPython.display import IFrame import uvicorn , asyncio __import__ ( \"nest_asyncio\" ) . apply () config = uvicorn . Config ( app ) if \"server\" in locals (): asyncio . ensure_future ( server . shutdown ()) server = uvicorn . Server ( config = config ) asyncio . ensure_future ( server . serve ()) INFO: Shutting down INFO: Started server process [288580] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Waiting for application shutdown. INFO: Application shutdown complete. I and display ( IFrame ( \"http://127.0.0.1:8000/graphql\" , * \"100% 600\" . split ())) --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In [8], line 1 ----> 1 I and display(IFrame( \" http://127.0.0.1:8000/graphql \" , * \" 100 % 600 \" . split())) NameError : name 'I' is not defined asyncio.ensure_future(server.shutdown())","title":"WIP: strawberry jupyter server"},{"location":"xxiii/2023-01-01-strawberry-server.html#wip-strawberry-jupyter-server","text":"a work in progress of a graphql schema for jupyter server. https://strawberry.rocks/docs/general/schema-basics # %pip install 'strawberry-graphql[debug-server]' from jupyter_server.services.contents.largefilemanager import LargeFileManager from pathlib import Path contents = LargeFileManager ( root_dir = str ( Path ( \"~\" ) . expanduser ())) import strawberry as S , typing as T , strawberry.cli , enum from pathlib import Path from datetime import datetime DICT = S . scalar ( T . NewType ( \"DICT\" , object ), description = \"a dictionary\" , serialize = lambda v : v , parse_value = lambda v : v , ) @strawberry . enum class ContentType ( enum . Enum ): FILE = \"file\" NOTEBOOK = \"notebook\" DIRECTORY = \"directory\" @S . type class Base : name : str path : str last_modified : datetime created : datetime format : str mimetype : T . Optional [ str ] size : T . Optional [ int ] writable : bool type : ContentType Cells = T . Union [ DICT ] @S . type class NbFormat : cells : list [ Cells ] metadata : DICT @S . type class Notebook ( Base ): content : DICT @S . type class File ( Base ): content : str @S . type class Directory ( Base ): content : T . Annotated [ \"Kinds\" , \"__main__\" ] Kinds = T . Union [ Notebook , File , Directory ] @S . type class Query : @S . field def search ( self , dir : str = \"\" , kind : T . Optional [ ContentType ] = None ) -> Kinds : return Directory ( ** contents . get ( dir , )) schema = strawberry . Schema ( Query ) from fastapi import FastAPI from strawberry.fastapi import GraphQLRouter graphql_app = GraphQLRouter ( schema ) app = FastAPI () app . include_router ( graphql_app , prefix = \"/graphql\" ) if ( I := \"__file__\" not in locals ()): from IPython.display import IFrame import uvicorn , asyncio __import__ ( \"nest_asyncio\" ) . apply () config = uvicorn . Config ( app ) if \"server\" in locals (): asyncio . ensure_future ( server . shutdown ()) server = uvicorn . Server ( config = config ) asyncio . ensure_future ( server . serve ()) INFO: Shutting down INFO: Started server process [288580] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Waiting for application shutdown. INFO: Application shutdown complete. I and display ( IFrame ( \"http://127.0.0.1:8000/graphql\" , * \"100% 600\" . split ())) --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In [8], line 1 ----> 1 I and display(IFrame( \" http://127.0.0.1:8000/graphql \" , * \" 100 % 600 \" . split())) NameError : name 'I' is not defined asyncio.ensure_future(server.shutdown())","title":"WIP: strawberry jupyter server"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html","text":"rendering dataframes for screen readers with pandas \u00a4 in the document we'll explore what it takes to make pandas.DataFrame s accessible. we'll follow Paul J Adam's instructions for making Simple Data Tables . %reload_ext pidgy import pandas.io.formats.style, bs4, pytest shell.weave.reactive = False shell.weave.use_async = False soup = lambda x: bs4.BeautifulSoup(x, features=\"html.parser\") the basic dataframe df we use for explanation in this document. A B C index 0 nan nan nan 1 nan nan nan (df := pandas.DataFrame( columns=pandas.Index(list(\"ABC\")), index=pandas.Index(range(2), name=\"index\")) ).style.set_caption(\"the basic dataframe <var>df</var> we use for explanation in this document. \") df is a basic pandas.DataFrame because: it has ONE row level it has ONE column level we do not need to consider the contents of the cells for the recommendations we are implementing. applying best practices to pandas \u00a4 1. Title of data table is inside the <caption> element. \u00a4 the caption is dependent on the data and effects the visual appearance of the table. it is context dependent up to the author to supply. def assert_has_caption(object): caption = soup(object).select_one(\"table caption\") assert caption and caption.string.strip(), \"table is missing a <caption>\" # with pytest.raises(AssertionError): assert_has_caption(df._repr_html_()) the caption can be set using the pandas.DataFrame.style.set_caption method. the result is a pandas.io.formats.style.Styler object that let's us modify how the instance is displayed. set_caption user the pandas.DataFrame.style attribute to set a caption def set_caption(df, caption) -> pandas.io.formats.style.Styler: return df.style.set_caption(caption) after we have a styler we are working with a subset of pandas operations. the styler should be the last stop for the data. the <table> below demonstrates how the <caption> appears on a captioned pandas.DataFrame . a value-less dataframe with columns and row indexes A B C index 0 nan nan nan 1 nan nan nan assert_has_caption( captioned := set_caption(df, \"a value-less dataframe with columns and row indexes\")._repr_html_() ); pandas.io.formats.style.Styler \u00a4 pandas.io.formats.style.Styler gives a different html representation that _repr_html_ assert df.style.to_html() != df.to_html() 2. Column headers are inside <th scope=\"col\"> elements. \u00a4 the scope property improves navigation for screen readers when used properly. def assert_has_col_scope(object, selector=\"thead tr th\"): for basic frames, all the <th> tags in <thead> should have scope=\"col\" assert all(th.attrs.get(\"scope\") in {\"col\", \"colgroup\"} for th in soup(object).select(selector)), \"<th> is missing `scope='col'`\" # with pytest.raises(AssertionError): assert_has_col_scope(captioned) def set_col_scope(object, selector=\"thead tr th\"): set_col_scope automatically remediates missing columns scope s for th in (object := soup(object)).select(selector): th.attrs.setdefault(\"scope\", \"col\") return str(object) these scope has no visual effect, however we can use assert_has_col_scope to verify the scope is correct. assert_has_col_scope(col_scoped := set_col_scope(captioned)); 3. Row headers are inside <th scope=\"row\"> elements. \u00a4 similar to scope=\"col\" , the <th> elements in the body require scope=\"row\" ; basically every <th> needs the scope property. def assert_has_row_scope(object, selector=\"tbody tr th\"): assert all(th.attrs.get(\"scope\") in {\"row\", \"rowgroup\"} for th in soup(object).select(selector)),\\ \"<th> is missing `scope='col'`\" # with pytest.raises(AssertionError): assert_has_row_scope(col_scoped) def set_row_scope(object, selector=\"tbody tr th, tfoot tr th\"): like the columns, we can deterministically add scope=\"row\" for th in (object := soup(object)).select(selector): th.attrs.setdefault(\"scope\", \"row\") return str(object) we use set_row_scope to verify the scope because, again, there aren't any visual effects to these changes. assert_has_row_scope(row_scoped := set_row_scope(col_scoped)) 4. Avoid using blank header cells. \u00a4 this instruction to yield a best practice to name the dataframe index . without a name, the index <th> will always be empty. def assert_no_blank_header(body): assert all(th.string.strip() for th in soup(body).select(\"th\")), \"there is a blank <th>\" # with pytest.raises(AssertionError): assert_no_blank_header(row_scoped) pandas requires some upstream work to satisfy this instruction. def set_squashed_th(body): set_squashed_th squashes the table column names. this method on works for the most basic dataframes. table = soup(body) tr = bs4.Tag(name=\"tr\") col, row = table.select(\"thead tr\") for top, bottom in zip(col.select(\"th\"), row.select(\"th\")): tr.append(top if top.string.strip() else bottom) thead = bs4.Tag(name=\"thead\"); thead.append(tr) table.select_one(\"thead\").replace_with(thead) return str(table) the frame below doesn't have empty <th> elements and is denser. a value-less dataframe with columns and row indexes index A B C 0 nan nan nan 1 nan nan nan assert_no_blank_header(squashed_th := set_squashed_th(row_scoped)) 5. Header cells with text abbreviations that need expansion use the title attribute with the expanded text set as the value. \u00a4 the naming of columns is a context specific screen reader feature. authors would have to add this information themselves. titles = dict(A=\"apple\", B=\"banana\", C=\"carrot\") def set_header_titles(body, titles=titles): set_header_titles is a method that includes the name of the abbreviation in the title . for th in (body := soup(body)).select(\"thead tr th\"): name = th.string.strip() if name in titles: th.attrs[\"title\"] = titles[name] return str(body) a real dataset would make more sense in this example. titled = set_header_titles(squashed_th, titles) def strip_class_ids(body): pandas adds classes and ids to elements that for the sake of this discussion are superfluous. for e in (body := soup(body)).select(\"td, th\"): e.attrs.pop(\"id\", None) e.attrs.pop(\"class\", None) return str(body) final = strip_class_ids(titled) inconsistencies in labelled indexes \u00a4 everything goes to hell with named_column which as a column name. named_column = df.copy() named_column.columns.name = \"letters\" column and index names \u00a4 consider the case of df2 where the df2.columns is named and df2.index is not. the named_column dataframe with a column name letters A B C index 0 nan nan nan 1 nan nan nan screenreader visitors may struggle to interpret the meaning of \"letters\" relative to the index. ensurely a proper experience for screen readers will require extra markup to group the ` with the columns. column name and no index name \u00a4 named_column_no_index = named_column.copy() named_column_no_index.index.name = None the named_column_no_index dataframe with a column name and without an index name letters A B C 0 nan nan nan 1 nan nan nan in this conformation, it is possible for a screen reader to misinterpet letters as the name of the index column. when the column index is named, like letters , the entry should be <th scope=\"row\"> . it this example we can see how instructions in 2 and 3 differ. conclusions \u00a4 for basic dataframes, two practices can be enforced without knowledge of the data: Column headers are inside elements. Row headers are inside elements. the abbreviations and caption are context specific and require knowledge of the data: Title of data table is inside the element. Header cells with text abbreviations that need expansion use the title attribute with the expanded text set as the value. with pandas<=1.4.2 , it is hard to avoid black header cells without some significant effort. Avoid using blank header cells. some conventions we can extract from this study is: treat the df.index as a column that needs to be named if an index is superfluous then remove it. this can be down with the styler df.style.hide(axis=0) final frame \u00a4 our final dataframe has: <caption> <th scope=\"col\"> <th scope=\"row\"> no empty <th> <th title> for abbreviations a value-less dataframe with columns and row indexes index A B C 0 nan nan nan 1 nan nan nan final html source \u00a4 < style type = \"text/css\" > </ style > < table id = \"T_595b7\" > < caption > a value-less dataframe with columns and row indexes </ caption > < thead >< tr >< th scope = \"col\" > index </ th >< th scope = \"col\" title = \"apple\" > A </ th >< th scope = \"col\" title = \"banana\" > B </ th >< th scope = \"col\" title = \"carrot\" > C </ th ></ tr ></ thead > < tbody > < tr > < th scope = \"row\" > 0 </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > < tr > < th scope = \"row\" > 1 </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > </ tbody > </ table > about the scope attribute \u00a4 The scope attribute specifies whether a header cell is a header for a column, row, or group of columns or rows. The scope attribute makes table navigation much easier for screen reader users, provided that it is used correctly. Incorrectly used, scope can make table navigation much harder and less efficient. links \u00a4 https://pandas.pydata.org/docs/user_guide/style.html https://pauljadam.com/demos/data-tables.html https://www.w3.org/WAI/tutorials/tables/ https://developer.mozilla.org/en-US/docs/Learn/HTML/Tables/Advanced {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"rendering dataframes for screen readers with <pre>pandas</pre>"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#rendering-dataframes-for-screen-readers-with-pandas","text":"in the document we'll explore what it takes to make pandas.DataFrame s accessible. we'll follow Paul J Adam's instructions for making Simple Data Tables . %reload_ext pidgy import pandas.io.formats.style, bs4, pytest shell.weave.reactive = False shell.weave.use_async = False soup = lambda x: bs4.BeautifulSoup(x, features=\"html.parser\") the basic dataframe df we use for explanation in this document. A B C index 0 nan nan nan 1 nan nan nan (df := pandas.DataFrame( columns=pandas.Index(list(\"ABC\")), index=pandas.Index(range(2), name=\"index\")) ).style.set_caption(\"the basic dataframe <var>df</var> we use for explanation in this document. \") df is a basic pandas.DataFrame because: it has ONE row level it has ONE column level we do not need to consider the contents of the cells for the recommendations we are implementing.","title":"rendering dataframes for screen readers with pandas"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#applying-best-practices-to-pandas","text":"","title":"applying best practices  to pandas"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#1-title-of-data-table-is-inside-the-caption-element","text":"the caption is dependent on the data and effects the visual appearance of the table. it is context dependent up to the author to supply. def assert_has_caption(object): caption = soup(object).select_one(\"table caption\") assert caption and caption.string.strip(), \"table is missing a <caption>\" # with pytest.raises(AssertionError): assert_has_caption(df._repr_html_()) the caption can be set using the pandas.DataFrame.style.set_caption method. the result is a pandas.io.formats.style.Styler object that let's us modify how the instance is displayed. set_caption user the pandas.DataFrame.style attribute to set a caption def set_caption(df, caption) -> pandas.io.formats.style.Styler: return df.style.set_caption(caption) after we have a styler we are working with a subset of pandas operations. the styler should be the last stop for the data. the <table> below demonstrates how the <caption> appears on a captioned pandas.DataFrame . a value-less dataframe with columns and row indexes A B C index 0 nan nan nan 1 nan nan nan assert_has_caption( captioned := set_caption(df, \"a value-less dataframe with columns and row indexes\")._repr_html_() );","title":"1. Title of data table is inside the &lt;caption&gt; element."},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#pandasioformatsstylestyler","text":"pandas.io.formats.style.Styler gives a different html representation that _repr_html_ assert df.style.to_html() != df.to_html()","title":"pandas.io.formats.style.Styler"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#2-column-headers-are-inside-th-scopecol-elements","text":"the scope property improves navigation for screen readers when used properly. def assert_has_col_scope(object, selector=\"thead tr th\"): for basic frames, all the <th> tags in <thead> should have scope=\"col\" assert all(th.attrs.get(\"scope\") in {\"col\", \"colgroup\"} for th in soup(object).select(selector)), \"<th> is missing `scope='col'`\" # with pytest.raises(AssertionError): assert_has_col_scope(captioned) def set_col_scope(object, selector=\"thead tr th\"): set_col_scope automatically remediates missing columns scope s for th in (object := soup(object)).select(selector): th.attrs.setdefault(\"scope\", \"col\") return str(object) these scope has no visual effect, however we can use assert_has_col_scope to verify the scope is correct. assert_has_col_scope(col_scoped := set_col_scope(captioned));","title":"2. Column headers are inside &lt;th scope=\"col\"&gt; elements."},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#3-row-headers-are-inside-th-scoperow-elements","text":"similar to scope=\"col\" , the <th> elements in the body require scope=\"row\" ; basically every <th> needs the scope property. def assert_has_row_scope(object, selector=\"tbody tr th\"): assert all(th.attrs.get(\"scope\") in {\"row\", \"rowgroup\"} for th in soup(object).select(selector)),\\ \"<th> is missing `scope='col'`\" # with pytest.raises(AssertionError): assert_has_row_scope(col_scoped) def set_row_scope(object, selector=\"tbody tr th, tfoot tr th\"): like the columns, we can deterministically add scope=\"row\" for th in (object := soup(object)).select(selector): th.attrs.setdefault(\"scope\", \"row\") return str(object) we use set_row_scope to verify the scope because, again, there aren't any visual effects to these changes. assert_has_row_scope(row_scoped := set_row_scope(col_scoped))","title":"3. Row headers are inside &lt;th scope=\"row\"&gt; elements."},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#4-avoid-using-blank-header-cells","text":"this instruction to yield a best practice to name the dataframe index . without a name, the index <th> will always be empty. def assert_no_blank_header(body): assert all(th.string.strip() for th in soup(body).select(\"th\")), \"there is a blank <th>\" # with pytest.raises(AssertionError): assert_no_blank_header(row_scoped) pandas requires some upstream work to satisfy this instruction. def set_squashed_th(body): set_squashed_th squashes the table column names. this method on works for the most basic dataframes. table = soup(body) tr = bs4.Tag(name=\"tr\") col, row = table.select(\"thead tr\") for top, bottom in zip(col.select(\"th\"), row.select(\"th\")): tr.append(top if top.string.strip() else bottom) thead = bs4.Tag(name=\"thead\"); thead.append(tr) table.select_one(\"thead\").replace_with(thead) return str(table) the frame below doesn't have empty <th> elements and is denser. a value-less dataframe with columns and row indexes index A B C 0 nan nan nan 1 nan nan nan assert_no_blank_header(squashed_th := set_squashed_th(row_scoped))","title":"4. Avoid using blank header cells."},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#5-header-cells-with-text-abbreviations-that-need-expansion-use-the-title-attribute-with-the-expanded-text-set-as-the-value","text":"the naming of columns is a context specific screen reader feature. authors would have to add this information themselves. titles = dict(A=\"apple\", B=\"banana\", C=\"carrot\") def set_header_titles(body, titles=titles): set_header_titles is a method that includes the name of the abbreviation in the title . for th in (body := soup(body)).select(\"thead tr th\"): name = th.string.strip() if name in titles: th.attrs[\"title\"] = titles[name] return str(body) a real dataset would make more sense in this example. titled = set_header_titles(squashed_th, titles) def strip_class_ids(body): pandas adds classes and ids to elements that for the sake of this discussion are superfluous. for e in (body := soup(body)).select(\"td, th\"): e.attrs.pop(\"id\", None) e.attrs.pop(\"class\", None) return str(body) final = strip_class_ids(titled)","title":"5. Header cells with text abbreviations that need expansion use the title attribute with the expanded text set as the value."},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#inconsistencies-in-labelled-indexes","text":"everything goes to hell with named_column which as a column name. named_column = df.copy() named_column.columns.name = \"letters\"","title":"inconsistencies in labelled indexes"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#column-and-index-names","text":"consider the case of df2 where the df2.columns is named and df2.index is not. the named_column dataframe with a column name letters A B C index 0 nan nan nan 1 nan nan nan screenreader visitors may struggle to interpret the meaning of \"letters\" relative to the index. ensurely a proper experience for screen readers will require extra markup to group the ` with the columns.","title":"column and index names"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#column-name-and-no-index-name","text":"named_column_no_index = named_column.copy() named_column_no_index.index.name = None the named_column_no_index dataframe with a column name and without an index name letters A B C 0 nan nan nan 1 nan nan nan in this conformation, it is possible for a screen reader to misinterpet letters as the name of the index column. when the column index is named, like letters , the entry should be <th scope=\"row\"> . it this example we can see how instructions in 2 and 3 differ.","title":"column name and no index name"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#conclusions","text":"for basic dataframes, two practices can be enforced without knowledge of the data: Column headers are inside elements. Row headers are inside elements. the abbreviations and caption are context specific and require knowledge of the data: Title of data table is inside the element. Header cells with text abbreviations that need expansion use the title attribute with the expanded text set as the value. with pandas<=1.4.2 , it is hard to avoid black header cells without some significant effort. Avoid using blank header cells. some conventions we can extract from this study is: treat the df.index as a column that needs to be named if an index is superfluous then remove it. this can be down with the styler df.style.hide(axis=0)","title":"conclusions"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#final-frame","text":"our final dataframe has: <caption> <th scope=\"col\"> <th scope=\"row\"> no empty <th> <th title> for abbreviations a value-less dataframe with columns and row indexes index A B C 0 nan nan nan 1 nan nan nan","title":"final frame"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#final-html-source","text":"< style type = \"text/css\" > </ style > < table id = \"T_595b7\" > < caption > a value-less dataframe with columns and row indexes </ caption > < thead >< tr >< th scope = \"col\" > index </ th >< th scope = \"col\" title = \"apple\" > A </ th >< th scope = \"col\" title = \"banana\" > B </ th >< th scope = \"col\" title = \"carrot\" > C </ th ></ tr ></ thead > < tbody > < tr > < th scope = \"row\" > 0 </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > < tr > < th scope = \"row\" > 1 </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > </ tbody > </ table >","title":"final html source"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#about-the-scope-attribute","text":"The scope attribute specifies whether a header cell is a header for a column, row, or group of columns or rows. The scope attribute makes table navigation much easier for screen reader users, provided that it is used correctly. Incorrectly used, scope can make table navigation much harder and less efficient.","title":"about the scope attribute"},{"location":"xxiii/2023-01-02-accessible-dataframes-basic-indexes.html#links","text":"https://pandas.pydata.org/docs/user_guide/style.html https://pauljadam.com/demos/data-tables.html https://www.w3.org/WAI/tutorials/tables/ https://developer.mozilla.org/en-US/docs/Learn/HTML/Tables/Advanced {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"links"},{"location":"xxiii/2023-01-04-tfoot-dataframe.html","text":"using tfoot in dataframes for more information \u00a4 while exploring techniques that make dataframes better for screen readers. i found resources on the tfoot element. i rarely see it used it with dataframes. this work explores what a footer description might look like. import functions that make dataframes more useful on screen readers with __import__('midgy.loader').loader.Markdown(extensions=[\".ipynb\"]): from tonyfast.xxiii.__accessible_dataframes_basic_indexes import ( df, soup, set_caption, set_col_scope, set_row_scope, set_squashed_th, set_header_titles, strip_class_ids ) set tfoot \u00a4 <tfoot> is meant to contain supplementary <table> information. def set_tfoot(df, caption=None): set_tfoot sets the footer as df.describe tfoot = soup(strip_class_ids(df.describe().style.to_html())).select_one(\"tbody\") tfoot.name = \"tfoot\" with_footer = soup(df.style.set_caption(caption).to_html()) with_footer.select_one(\"table\").append(tfoot) return str(with_footer) formatting the final dataframe \u00a4 def format_df(df, caption=None): return strip_class_ids(set_header_titles(set_squashed_th( set_row_scope(set_col_scope(set_tfoot(df, caption=caption)))))) final_table = format_df(df, \"a value-less dataframe with thead, tbody and tfoot defined. tfoot are statistical descriptions of the data.\") import tests from another work. with __import__('midgy.loader').loader.Markdown(extensions=[\".ipynb\"]): from tonyfast.xxiii.__accessible_dataframes_basic_indexes import ( assert_has_col_scope, assert_has_row_scope, assert_no_blank_header, assert_has_caption ) verifying that we pass our tests \u00a4 def test_basic_table_accessibility(html): test for a caption, scope attributes, and populated headers assert_has_caption(html) assert_has_col_scope(html) assert_has_row_scope(html) assert_no_blank_header(html) the final_table \u00a4 this message means our table abides. woo! INTERACTIVE and print(\"this message means our table abides. woo!\") a value-less dataframe with thead, tbody and tfoot defined. tfoot are statistical descriptions of the data. index A B C 0 nan nan nan 1 nan nan nan count 0 0 0 unique 0 0 0 top nan nan nan freq nan nan nan remediated pandas.DataFrame < style type = \"text/css\" > </ style > < table id = \"T_6ae36\" > < caption > a value-less dataframe with thead, tbody and tfoot defined. tfoot are statistical descriptions of the data. </ caption > < thead >< tr >< th scope = \"col\" > index </ th >< th scope = \"col\" title = \"apple\" > A </ th >< th scope = \"col\" title = \"banana\" > B </ th >< th scope = \"col\" title = \"carrot\" > C </ th ></ tr ></ thead > < tbody > < tr > < th scope = \"row\" > 0 </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > < tr > < th scope = \"row\" > 1 </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > </ tbody > < tfoot > < tr > < th scope = \"row\" > count </ th > < td > 0 </ td > < td > 0 </ td > < td > 0 </ td > </ tr > < tr > < th scope = \"row\" > unique </ th > < td > 0 </ td > < td > 0 </ td > < td > 0 </ td > </ tr > < tr > < th scope = \"row\" > top </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > < tr > < th scope = \"row\" > freq </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > </ tfoot ></ table >","title":"using <pre>tfoot</pre> in dataframes for more information"},{"location":"xxiii/2023-01-04-tfoot-dataframe.html#using-tfoot-in-dataframes-for-more-information","text":"while exploring techniques that make dataframes better for screen readers. i found resources on the tfoot element. i rarely see it used it with dataframes. this work explores what a footer description might look like. import functions that make dataframes more useful on screen readers with __import__('midgy.loader').loader.Markdown(extensions=[\".ipynb\"]): from tonyfast.xxiii.__accessible_dataframes_basic_indexes import ( df, soup, set_caption, set_col_scope, set_row_scope, set_squashed_th, set_header_titles, strip_class_ids )","title":"using tfoot in dataframes for more information"},{"location":"xxiii/2023-01-04-tfoot-dataframe.html#set-tfoot","text":"<tfoot> is meant to contain supplementary <table> information. def set_tfoot(df, caption=None): set_tfoot sets the footer as df.describe tfoot = soup(strip_class_ids(df.describe().style.to_html())).select_one(\"tbody\") tfoot.name = \"tfoot\" with_footer = soup(df.style.set_caption(caption).to_html()) with_footer.select_one(\"table\").append(tfoot) return str(with_footer)","title":"set tfoot"},{"location":"xxiii/2023-01-04-tfoot-dataframe.html#formatting-the-final-dataframe","text":"def format_df(df, caption=None): return strip_class_ids(set_header_titles(set_squashed_th( set_row_scope(set_col_scope(set_tfoot(df, caption=caption)))))) final_table = format_df(df, \"a value-less dataframe with thead, tbody and tfoot defined. tfoot are statistical descriptions of the data.\") import tests from another work. with __import__('midgy.loader').loader.Markdown(extensions=[\".ipynb\"]): from tonyfast.xxiii.__accessible_dataframes_basic_indexes import ( assert_has_col_scope, assert_has_row_scope, assert_no_blank_header, assert_has_caption )","title":"formatting the final dataframe"},{"location":"xxiii/2023-01-04-tfoot-dataframe.html#verifying-that-we-pass-our-tests","text":"def test_basic_table_accessibility(html): test for a caption, scope attributes, and populated headers assert_has_caption(html) assert_has_col_scope(html) assert_has_row_scope(html) assert_no_blank_header(html)","title":"verifying that we pass our tests"},{"location":"xxiii/2023-01-04-tfoot-dataframe.html#the-final_table","text":"this message means our table abides. woo! INTERACTIVE and print(\"this message means our table abides. woo!\") a value-less dataframe with thead, tbody and tfoot defined. tfoot are statistical descriptions of the data. index A B C 0 nan nan nan 1 nan nan nan count 0 0 0 unique 0 0 0 top nan nan nan freq nan nan nan remediated pandas.DataFrame < style type = \"text/css\" > </ style > < table id = \"T_6ae36\" > < caption > a value-less dataframe with thead, tbody and tfoot defined. tfoot are statistical descriptions of the data. </ caption > < thead >< tr >< th scope = \"col\" > index </ th >< th scope = \"col\" title = \"apple\" > A </ th >< th scope = \"col\" title = \"banana\" > B </ th >< th scope = \"col\" title = \"carrot\" > C </ th ></ tr ></ thead > < tbody > < tr > < th scope = \"row\" > 0 </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > < tr > < th scope = \"row\" > 1 </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > </ tbody > < tfoot > < tr > < th scope = \"row\" > count </ th > < td > 0 </ td > < td > 0 </ td > < td > 0 </ td > </ tr > < tr > < th scope = \"row\" > unique </ th > < td > 0 </ td > < td > 0 </ td > < td > 0 </ td > </ tr > < tr > < th scope = \"row\" > top </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > < tr > < th scope = \"row\" > freq </ th > < td > nan </ td > < td > nan </ td > < td > nan </ td > </ tr > </ tfoot ></ table >","title":"the final_table"},{"location":"xxiii/2023-01-04-using-diagrams.html","text":"diagrams in jupyter and mkdocs \u00a4 mermaid has emerged as a community standard for diagrams. we can use it in three places relative to this content: mermaid can be used on github mermaid can be used in jupyterlab with jupyterlab-markup mermaid can be used with mkdocs-material from the root page, using beautifulsoup4 as an intermediary, we can discover that mermaid supports 13 chart syntaxes. soup = bs4 . BeautifulSoup (( root := requests . get ( \"https://mermaid.js.org/syntax/classDiagram.html\" )) . text , \"html.parser\" ) kinds = set ( x . attrs [ \"href\" ] for x in soup . select ( \"a\" ) if x . attrs [ \"href\" ] . startswith (( \"/syntax\" ,))) extracting the syntaxes to build one master chart. baseurl = root . url . rsplit ( \"/\" , 2 )[ 0 ] syntaxes = { baseurl + x for x in kinds } responses = { x : requests . get ( x ) for x in syntaxes } def strip_front_matter ( x ): fence = ( '---' ,) if x . startswith ( fence ): y = None for line in x . splitlines ( 1 )[ 1 :]: if y is None : if line . startswith ( fence ): y = \"\" else : y += line return y return x all the diagram styles collection programmatically \u00a4 Mindmap \u00a4 mindmap root((mindmap)) Origins Long history ::icon(fa fa-book) Popularisation British popular psychology author Tony Buzan Research On effectiveness<br/>and features On Automatic creation Uses Creative techniques Strategic planning Argument mapping Tools Pen and paper Mermaid C4 Diagrams \u00a4 C4Context title System Context diagram for Internet Banking System Enterprise_Boundary(b0, \"BankBoundary0\") { Person(customerA, \"Banking Customer A\", \"A customer of the bank, with personal bank accounts.\") Person(customerB, \"Banking Customer B\") Person_Ext(customerC, \"Banking Customer C\", \"desc\") Person(customerD, \"Banking Customer D\", \"A customer of the bank, <br/> with personal bank accounts.\") System(SystemAA, \"Internet Banking System\", \"Allows customers to view information about their bank accounts, and make payments.\") Enterprise_Boundary(b1, \"BankBoundary\") { SystemDb_Ext(SystemE, \"Mainframe Banking System\", \"Stores all of the core banking information about customers, accounts, transactions, etc.\") System_Boundary(b2, \"BankBoundary2\") { System(SystemA, \"Banking System A\") System(SystemB, \"Banking System B\", \"A system of the bank, with personal bank accounts. next line.\") } System_Ext(SystemC, \"E-mail system\", \"The internal Microsoft Exchange e-mail system.\") SystemDb(SystemD, \"Banking System D Database\", \"A system of the bank, with personal bank accounts.\") Boundary(b3, \"BankBoundary3\", \"boundary\") { SystemQueue(SystemF, \"Banking System F Queue\", \"A system of the bank.\") SystemQueue_Ext(SystemG, \"Banking System G Queue\", \"A system of the bank, with personal bank accounts.\") } } } BiRel(customerA, SystemAA, \"Uses\") BiRel(SystemAA, SystemE, \"Uses\") Rel(SystemAA, SystemC, \"Sends e-mails\", \"SMTP\") Rel(SystemC, customerA, \"Sends e-mails to\") UpdateElementStyle(customerA, $fontColor=\"red\", $bgColor=\"grey\", $borderColor=\"red\") UpdateRelStyle(customerA, SystemAA, $textColor=\"blue\", $lineColor=\"blue\", $offsetX=\"5\") UpdateRelStyle(SystemAA, SystemE, $textColor=\"blue\", $lineColor=\"blue\", $offsetY=\"-10\") UpdateRelStyle(SystemAA, SystemC, $textColor=\"blue\", $lineColor=\"blue\", $offsetY=\"-40\", $offsetX=\"-50\") UpdateRelStyle(SystemC, customerA, $textColor=\"red\", $lineColor=\"red\", $offsetX=\"-50\", $offsetY=\"20\") UpdateLayoutConfig($c4ShapeInRow=\"3\", $c4BoundaryInRow=\"1\") Examples \u00a4 pie title NETFLIX \"Time spent looking for movie\" : 90 \"Time spent watching it\" : 10 User Journey Diagram \u00a4 journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 5: Me Sequence diagrams \u00a4 sequenceDiagram Alice->>John: Hello John, how are you? John-->>Alice: Great! Alice-)John: See you later! Requirement Diagram \u00a4 requirementDiagram requirement test_req { id: 1 text: the test text. risk: high verifymethod: test } element test_entity { type: simulation } test_entity - satisfies -> test_req Flowcharts - Basic Syntax \u00a4 flowchart LR id Entity Relationship Diagrams \u00a4 erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses Pie chart diagrams \u00a4 pie title Pets adopted by volunteers \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 State diagrams \u00a4 stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*] Gitgraph Diagrams \u00a4 gitGraph commit commit branch develop checkout develop commit commit checkout main merge develop commit commit Class diagrams \u00a4 classDiagram note \"From Duck till Zebra\" Animal <|-- Duck note for Duck \"can fly\\ncan swim\\ncan dive\\ncan help in debugging\" Animal <|-- Fish Animal <|-- Zebra Animal : +int age Animal : +String gender Animal: +isMammal() Animal: +mate() class Duck{ +String beakColor +swim() +quack() } class Fish{ -int sizeInFeet -canEat() } class Zebra{ +bool is_wild +run() } Gantt diagrams \u00a4 gantt title A Gantt Diagram dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d the following demos are not working, but 10/13 demos in one place ain't bad. C4 Diagrams Class diagrams (these work the mkdocs version) Mindmap why did mermaid win? \u00a4 i <3 graphviz and i'm curius why mermaid one. graphviz is over 30 years old and written in c, but maybe the javascript was enough to win. does wasm change the game? i hope to explore this question further when i read the graphviz authors take . appendix \u00a4 weird thing: i was only able to get mermaid to work when the title for the document did not have mermaid in it.","title":"diagrams in jupyter and mkdocs"},{"location":"xxiii/2023-01-04-using-diagrams.html#diagrams-in-jupyter-and-mkdocs","text":"mermaid has emerged as a community standard for diagrams. we can use it in three places relative to this content: mermaid can be used on github mermaid can be used in jupyterlab with jupyterlab-markup mermaid can be used with mkdocs-material from the root page, using beautifulsoup4 as an intermediary, we can discover that mermaid supports 13 chart syntaxes. soup = bs4 . BeautifulSoup (( root := requests . get ( \"https://mermaid.js.org/syntax/classDiagram.html\" )) . text , \"html.parser\" ) kinds = set ( x . attrs [ \"href\" ] for x in soup . select ( \"a\" ) if x . attrs [ \"href\" ] . startswith (( \"/syntax\" ,))) extracting the syntaxes to build one master chart. baseurl = root . url . rsplit ( \"/\" , 2 )[ 0 ] syntaxes = { baseurl + x for x in kinds } responses = { x : requests . get ( x ) for x in syntaxes } def strip_front_matter ( x ): fence = ( '---' ,) if x . startswith ( fence ): y = None for line in x . splitlines ( 1 )[ 1 :]: if y is None : if line . startswith ( fence ): y = \"\" else : y += line return y return x","title":"diagrams in jupyter and mkdocs"},{"location":"xxiii/2023-01-04-using-diagrams.html#all-the-diagram-styles-collection-programmatically","text":"","title":"all the diagram styles collection programmatically"},{"location":"xxiii/2023-01-04-using-diagrams.html#mindmap","text":"mindmap root((mindmap)) Origins Long history ::icon(fa fa-book) Popularisation British popular psychology author Tony Buzan Research On effectiveness<br/>and features On Automatic creation Uses Creative techniques Strategic planning Argument mapping Tools Pen and paper Mermaid","title":"Mindmap"},{"location":"xxiii/2023-01-04-using-diagrams.html#c4-diagrams","text":"C4Context title System Context diagram for Internet Banking System Enterprise_Boundary(b0, \"BankBoundary0\") { Person(customerA, \"Banking Customer A\", \"A customer of the bank, with personal bank accounts.\") Person(customerB, \"Banking Customer B\") Person_Ext(customerC, \"Banking Customer C\", \"desc\") Person(customerD, \"Banking Customer D\", \"A customer of the bank, <br/> with personal bank accounts.\") System(SystemAA, \"Internet Banking System\", \"Allows customers to view information about their bank accounts, and make payments.\") Enterprise_Boundary(b1, \"BankBoundary\") { SystemDb_Ext(SystemE, \"Mainframe Banking System\", \"Stores all of the core banking information about customers, accounts, transactions, etc.\") System_Boundary(b2, \"BankBoundary2\") { System(SystemA, \"Banking System A\") System(SystemB, \"Banking System B\", \"A system of the bank, with personal bank accounts. next line.\") } System_Ext(SystemC, \"E-mail system\", \"The internal Microsoft Exchange e-mail system.\") SystemDb(SystemD, \"Banking System D Database\", \"A system of the bank, with personal bank accounts.\") Boundary(b3, \"BankBoundary3\", \"boundary\") { SystemQueue(SystemF, \"Banking System F Queue\", \"A system of the bank.\") SystemQueue_Ext(SystemG, \"Banking System G Queue\", \"A system of the bank, with personal bank accounts.\") } } } BiRel(customerA, SystemAA, \"Uses\") BiRel(SystemAA, SystemE, \"Uses\") Rel(SystemAA, SystemC, \"Sends e-mails\", \"SMTP\") Rel(SystemC, customerA, \"Sends e-mails to\") UpdateElementStyle(customerA, $fontColor=\"red\", $bgColor=\"grey\", $borderColor=\"red\") UpdateRelStyle(customerA, SystemAA, $textColor=\"blue\", $lineColor=\"blue\", $offsetX=\"5\") UpdateRelStyle(SystemAA, SystemE, $textColor=\"blue\", $lineColor=\"blue\", $offsetY=\"-10\") UpdateRelStyle(SystemAA, SystemC, $textColor=\"blue\", $lineColor=\"blue\", $offsetY=\"-40\", $offsetX=\"-50\") UpdateRelStyle(SystemC, customerA, $textColor=\"red\", $lineColor=\"red\", $offsetX=\"-50\", $offsetY=\"20\") UpdateLayoutConfig($c4ShapeInRow=\"3\", $c4BoundaryInRow=\"1\")","title":"C4 Diagrams"},{"location":"xxiii/2023-01-04-using-diagrams.html#examples","text":"pie title NETFLIX \"Time spent looking for movie\" : 90 \"Time spent watching it\" : 10","title":"Examples"},{"location":"xxiii/2023-01-04-using-diagrams.html#user-journey-diagram","text":"journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 5: Me","title":"User Journey Diagram"},{"location":"xxiii/2023-01-04-using-diagrams.html#sequence-diagrams","text":"sequenceDiagram Alice->>John: Hello John, how are you? John-->>Alice: Great! Alice-)John: See you later!","title":"Sequence diagrams"},{"location":"xxiii/2023-01-04-using-diagrams.html#requirement-diagram","text":"requirementDiagram requirement test_req { id: 1 text: the test text. risk: high verifymethod: test } element test_entity { type: simulation } test_entity - satisfies -> test_req","title":"Requirement Diagram"},{"location":"xxiii/2023-01-04-using-diagrams.html#flowcharts-basic-syntax","text":"flowchart LR id","title":"Flowcharts - Basic Syntax"},{"location":"xxiii/2023-01-04-using-diagrams.html#entity-relationship-diagrams","text":"erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses","title":"Entity Relationship Diagrams"},{"location":"xxiii/2023-01-04-using-diagrams.html#pie-chart-diagrams","text":"pie title Pets adopted by volunteers \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15","title":"Pie chart diagrams"},{"location":"xxiii/2023-01-04-using-diagrams.html#state-diagrams","text":"stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*]","title":"State diagrams"},{"location":"xxiii/2023-01-04-using-diagrams.html#gitgraph-diagrams","text":"gitGraph commit commit branch develop checkout develop commit commit checkout main merge develop commit commit","title":"Gitgraph Diagrams"},{"location":"xxiii/2023-01-04-using-diagrams.html#class-diagrams","text":"classDiagram note \"From Duck till Zebra\" Animal <|-- Duck note for Duck \"can fly\\ncan swim\\ncan dive\\ncan help in debugging\" Animal <|-- Fish Animal <|-- Zebra Animal : +int age Animal : +String gender Animal: +isMammal() Animal: +mate() class Duck{ +String beakColor +swim() +quack() } class Fish{ -int sizeInFeet -canEat() } class Zebra{ +bool is_wild +run() }","title":"Class diagrams"},{"location":"xxiii/2023-01-04-using-diagrams.html#gantt-diagrams","text":"gantt title A Gantt Diagram dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d the following demos are not working, but 10/13 demos in one place ain't bad. C4 Diagrams Class diagrams (these work the mkdocs version) Mindmap","title":"Gantt diagrams"},{"location":"xxiii/2023-01-04-using-diagrams.html#why-did-mermaid-win","text":"i <3 graphviz and i'm curius why mermaid one. graphviz is over 30 years old and written in c, but maybe the javascript was enough to win. does wasm change the game? i hope to explore this question further when i read the graphviz authors take .","title":"why did mermaid win?"},{"location":"xxiii/2023-01-04-using-diagrams.html#appendix","text":"weird thing: i was only able to get mermaid to work when the title for the document did not have mermaid in it.","title":"appendix"},{"location":"xxiii/2023-01-07-pidgy-voila.html","text":"running pidgy in voila \u00a4 this notebook demonstrates pidgy widget and voila integration. # a blank line in pidgy suppressing the woven output % reload_ext pidgy shell . weave . template_type = \"widget\" # a blank line in pidgy suppressing the woven output %reload_ext pidgy shell.weave.template_type = \"widget\" COOOKIES! \u00a4 the cookies demo is a go to demo from Bret Viktor's tanglejs . if you eat {{ cookies . value }} that you consume {{ cookies . value * calories }} calories . var element = $('#6666c058-86e0-49b8-ba7d-f7c510faf81c'); {\"model_id\": \"c91a3037a55f406d9b31ddd00b3117bc\", \"version_major\": 2, \"version_minor\": 0} calories = 50 display ( cookies := IntSlider ( 3 , 1 , description = \"COOKIES\" )) var element = $('#7982cd17-494c-4063-a80f-220cdccd6cc5'); {\"model_id\": \"c2aecbfa15fd4cc2ab29c05d0c0822a2\", \"version_major\": 2, \"version_minor\": 0} calories = 50 display(cookies := IntSlider(3, 1, description=\"COOKIES\")) ## `pidgy` widget integration out of the box , ` pidgy ` relies on ` IPython ` s markdown display , but in this document we use the ` pidgy . displays . IPyWidgetsHtml ` display shell . weave . template_cls = pidgy . displays . IPyWidgetsHtml the reactive and asynchronous ` pidgy . displays . IPyWidgetsHtml ` display passes the input through a jinja environment then it is parsed into html by ` shell . weave . markdown_renderer ` . after each exection we link undeclared variables in the jinja templates to any interactive widgets . when widgets change , the display changes as shown in the COOOKIES demo . var element = $('#9c106047-42b1-4afe-a596-8ea70178535b'); {\"model_id\": \"4ddab38140394b759f63c83bdddee5e0\", \"version_major\": 2, \"version_minor\": 0} {\"state\": {\"2d87d54548194acd955da7d1d5e0016e\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"2.0.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"4ddab38140394b759f63c83bdddee5e0\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_2d87d54548194acd955da7d1d5e0016e\", \"style\": \"IPY_MODEL_636b9b6698994f29bce0f8d4dd65a170\", \"value\": \"<h2><code>pidgy</code> widget integration</h2>\\n<p>out of the box, <code>pidgy</code> relies on <code>IPython</code>s markdown display, but in this document we use the <code>pidgy.displays.IPyWidgetsHtml</code> display</p>\\n<pre><code>shell.weave.template_cls = pidgy.displays.IPyWidgetsHtml\\n</code></pre>\\n<p>the reactive and asynchronous <code>pidgy.displays.IPyWidgetsHtml</code> display passes the input through a jinja environment then it is parsed into html by <code>shell.weave.markdown_renderer</code>.</p>\\n<p>after each exection we link undeclared variables in the jinja templates to any interactive widgets.\\nwhen widgets change, the display changes as shown in the COOOKIES demo.</p>\\n\"}}, \"61f66c8575d44bb29df2becd55d538ba\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"2.0.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"636b9b6698994f29bce0f8d4dd65a170\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"HTMLStyleModel\", \"state\": {\"description_width\": \"\", \"font_size\": null, \"text_color\": null}}, \"a761b1c1a5344ac68bd244cb208e038c\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"2.0.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"c2aecbfa15fd4cc2ab29c05d0c0822a2\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"IntSliderModel\", \"state\": {\"behavior\": \"drag-tap\", \"description\": \"COOKIES\", \"layout\": \"IPY_MODEL_61f66c8575d44bb29df2becd55d538ba\", \"min\": 1, \"style\": \"IPY_MODEL_fe3089005cb24917879753004399e720\", \"value\": 3}}, \"c91a3037a55f406d9b31ddd00b3117bc\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_a761b1c1a5344ac68bd244cb208e038c\", \"style\": \"IPY_MODEL_d2cd7a98622c4750bf63fbae65ff881a\", \"value\": \"<p>if you eat 3 that you consume 150 calories.</p>\\n\"}}, \"d2cd7a98622c4750bf63fbae65ff881a\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"HTMLStyleModel\", \"state\": {\"description_width\": \"\", \"font_size\": null, \"text_color\": null}}, \"fe3089005cb24917879753004399e720\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"SliderStyleModel\", \"state\": {\"description_width\": \"\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"running <pre>pidgy</pre> in <pre>voila</pre>"},{"location":"xxiii/2023-01-07-pidgy-voila.html#running-pidgy-in-voila","text":"this notebook demonstrates pidgy widget and voila integration. # a blank line in pidgy suppressing the woven output % reload_ext pidgy shell . weave . template_type = \"widget\" # a blank line in pidgy suppressing the woven output %reload_ext pidgy shell.weave.template_type = \"widget\"","title":"running pidgy in voila"},{"location":"xxiii/2023-01-07-pidgy-voila.html#coookies","text":"the cookies demo is a go to demo from Bret Viktor's tanglejs . if you eat {{ cookies . value }} that you consume {{ cookies . value * calories }} calories . var element = $('#6666c058-86e0-49b8-ba7d-f7c510faf81c'); {\"model_id\": \"c91a3037a55f406d9b31ddd00b3117bc\", \"version_major\": 2, \"version_minor\": 0} calories = 50 display ( cookies := IntSlider ( 3 , 1 , description = \"COOKIES\" )) var element = $('#7982cd17-494c-4063-a80f-220cdccd6cc5'); {\"model_id\": \"c2aecbfa15fd4cc2ab29c05d0c0822a2\", \"version_major\": 2, \"version_minor\": 0} calories = 50 display(cookies := IntSlider(3, 1, description=\"COOKIES\")) ## `pidgy` widget integration out of the box , ` pidgy ` relies on ` IPython ` s markdown display , but in this document we use the ` pidgy . displays . IPyWidgetsHtml ` display shell . weave . template_cls = pidgy . displays . IPyWidgetsHtml the reactive and asynchronous ` pidgy . displays . IPyWidgetsHtml ` display passes the input through a jinja environment then it is parsed into html by ` shell . weave . markdown_renderer ` . after each exection we link undeclared variables in the jinja templates to any interactive widgets . when widgets change , the display changes as shown in the COOOKIES demo . var element = $('#9c106047-42b1-4afe-a596-8ea70178535b'); {\"model_id\": \"4ddab38140394b759f63c83bdddee5e0\", \"version_major\": 2, \"version_minor\": 0} {\"state\": {\"2d87d54548194acd955da7d1d5e0016e\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"2.0.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"4ddab38140394b759f63c83bdddee5e0\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_2d87d54548194acd955da7d1d5e0016e\", \"style\": \"IPY_MODEL_636b9b6698994f29bce0f8d4dd65a170\", \"value\": \"<h2><code>pidgy</code> widget integration</h2>\\n<p>out of the box, <code>pidgy</code> relies on <code>IPython</code>s markdown display, but in this document we use the <code>pidgy.displays.IPyWidgetsHtml</code> display</p>\\n<pre><code>shell.weave.template_cls = pidgy.displays.IPyWidgetsHtml\\n</code></pre>\\n<p>the reactive and asynchronous <code>pidgy.displays.IPyWidgetsHtml</code> display passes the input through a jinja environment then it is parsed into html by <code>shell.weave.markdown_renderer</code>.</p>\\n<p>after each exection we link undeclared variables in the jinja templates to any interactive widgets.\\nwhen widgets change, the display changes as shown in the COOOKIES demo.</p>\\n\"}}, \"61f66c8575d44bb29df2becd55d538ba\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"2.0.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"636b9b6698994f29bce0f8d4dd65a170\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"HTMLStyleModel\", \"state\": {\"description_width\": \"\", \"font_size\": null, \"text_color\": null}}, \"a761b1c1a5344ac68bd244cb208e038c\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"2.0.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"c2aecbfa15fd4cc2ab29c05d0c0822a2\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"IntSliderModel\", \"state\": {\"behavior\": \"drag-tap\", \"description\": \"COOKIES\", \"layout\": \"IPY_MODEL_61f66c8575d44bb29df2becd55d538ba\", \"min\": 1, \"style\": \"IPY_MODEL_fe3089005cb24917879753004399e720\", \"value\": 3}}, \"c91a3037a55f406d9b31ddd00b3117bc\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_a761b1c1a5344ac68bd244cb208e038c\", \"style\": \"IPY_MODEL_d2cd7a98622c4750bf63fbae65ff881a\", \"value\": \"<p>if you eat 3 that you consume 150 calories.</p>\\n\"}}, \"d2cd7a98622c4750bf63fbae65ff881a\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"HTMLStyleModel\", \"state\": {\"description_width\": \"\", \"font_size\": null, \"text_color\": null}}, \"fe3089005cb24917879753004399e720\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"SliderStyleModel\", \"state\": {\"description_width\": \"\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"COOOKIES!"},{"location":"xxiii/2023-01-09-notebooks-states.html","text":"three states of python in a notebook \u00a4 most folks are familiar with notebooks in their interactive forms. in IPython , we are working in python and __name__ == \"__main__\" . for python programs, this condition is the last stop for our program, but in notebooks it is just the beginning. i like to thing we development programs and applications in reverse in notebooks. with experience, it is possible to author notebooks that operate as programs/scripts. this document demonstrates some advanced usage patterns to apply to get the most out of future notebooks. %reload_ext pidgy the three python compilation states of notebooks: an interactive state when we are authoring notebooks. a script state when a notebook is used as procedural code. a module when it is imported like any python resource we need to confer with the __name__ and __file__ properties of a module's runtime to know which state we are in. state __name__ __file__ interactive __name__ == \"__main__\" \"__file__\" not in locals() module __name__ != \"__main__\" \"__file__\" in locals() script __name__ == \"__main__\" \"__file__\" in locals() python state logic \u00a4 the three conditions are expressed in python code below MAIN, FILE = __name__ == \"__main__\", \"__file__\" in locals() INTERACTIVE = MAIN and not FILE SCRIPT = MAIN and FILE MODULE = FILE and not MAIN these states let us write multi purpose documents that can be used in multiple contexts and sharing the benefits of all of them. applying the condition \u00a4 def a_bad_ass_function(who: str = \"you\"): tells you who the bad ass is. >>> a_bad_ass_function() you are bad ass. print(F\"{who} are bad ass.\") the MAIN conditions \u00a4 INTERACTIVE notebooks and SCRIPT s both have __name__ == \"__main__\" the SCRIPT conditin \u00a4 if SCRIPT: we import the notebook and run the command line program. belowis a typer application that is only executed when the notebook is used as a script import typer typer.run(a_bad_ass_function) the INTERACTIVE condition \u00a4 if INTERACTIVE: data for demonstration is computed and displayed !midgy 2023-01-09-notebooks-states.ipynb --help !midgy 2023-01-09-notebooks-states.ipynb --who \"y'alls\" the MODULE condition \u00a4 if MODULE: we import the notebook and avoid running an command line programs. often notebooks as tests are loaded under this condition. if INTERACTIVE: import midgy.loader with midgy.loader.Markdown(extensions=[\".ipynb\"]): import __notebooks_states display(repr(__notebooks_states)) the benefits of knowing the state \u00a4 knowing the state or context of a notebook's execution allows us to write logic that makes notebooks a swiss army knife. some advanced patterns are include testing notebooks or writing command line scripts . applying doctest s \u00a4 a_bad_ass_function has a doctest in the docstring. when working interactively we might want to run these tests to verify our idea works. if INTERACTIVE: we only want to execute the doctest s in interactive mode import doctest display(doctest.testmod()) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"three states of python in a notebook"},{"location":"xxiii/2023-01-09-notebooks-states.html#three-states-of-python-in-a-notebook","text":"most folks are familiar with notebooks in their interactive forms. in IPython , we are working in python and __name__ == \"__main__\" . for python programs, this condition is the last stop for our program, but in notebooks it is just the beginning. i like to thing we development programs and applications in reverse in notebooks. with experience, it is possible to author notebooks that operate as programs/scripts. this document demonstrates some advanced usage patterns to apply to get the most out of future notebooks. %reload_ext pidgy the three python compilation states of notebooks: an interactive state when we are authoring notebooks. a script state when a notebook is used as procedural code. a module when it is imported like any python resource we need to confer with the __name__ and __file__ properties of a module's runtime to know which state we are in. state __name__ __file__ interactive __name__ == \"__main__\" \"__file__\" not in locals() module __name__ != \"__main__\" \"__file__\" in locals() script __name__ == \"__main__\" \"__file__\" in locals()","title":"three states of python in a notebook"},{"location":"xxiii/2023-01-09-notebooks-states.html#python-state-logic","text":"the three conditions are expressed in python code below MAIN, FILE = __name__ == \"__main__\", \"__file__\" in locals() INTERACTIVE = MAIN and not FILE SCRIPT = MAIN and FILE MODULE = FILE and not MAIN these states let us write multi purpose documents that can be used in multiple contexts and sharing the benefits of all of them.","title":"python state logic"},{"location":"xxiii/2023-01-09-notebooks-states.html#applying-the-condition","text":"def a_bad_ass_function(who: str = \"you\"): tells you who the bad ass is. >>> a_bad_ass_function() you are bad ass. print(F\"{who} are bad ass.\")","title":"applying the condition"},{"location":"xxiii/2023-01-09-notebooks-states.html#the-main-conditions","text":"INTERACTIVE notebooks and SCRIPT s both have __name__ == \"__main__\"","title":"the MAIN conditions"},{"location":"xxiii/2023-01-09-notebooks-states.html#the-script-conditin","text":"if SCRIPT: we import the notebook and run the command line program. belowis a typer application that is only executed when the notebook is used as a script import typer typer.run(a_bad_ass_function)","title":"the SCRIPT conditin"},{"location":"xxiii/2023-01-09-notebooks-states.html#the-interactive-condition","text":"if INTERACTIVE: data for demonstration is computed and displayed !midgy 2023-01-09-notebooks-states.ipynb --help !midgy 2023-01-09-notebooks-states.ipynb --who \"y'alls\"","title":"the INTERACTIVE condition"},{"location":"xxiii/2023-01-09-notebooks-states.html#the-module-condition","text":"if MODULE: we import the notebook and avoid running an command line programs. often notebooks as tests are loaded under this condition. if INTERACTIVE: import midgy.loader with midgy.loader.Markdown(extensions=[\".ipynb\"]): import __notebooks_states display(repr(__notebooks_states))","title":"the MODULE condition"},{"location":"xxiii/2023-01-09-notebooks-states.html#the-benefits-of-knowing-the-state","text":"knowing the state or context of a notebook's execution allows us to write logic that makes notebooks a swiss army knife. some advanced patterns are include testing notebooks or writing command line scripts .","title":"the benefits of knowing the state"},{"location":"xxiii/2023-01-09-notebooks-states.html#applying-doctests","text":"a_bad_ass_function has a doctest in the docstring. when working interactively we might want to run these tests to verify our idea works. if INTERACTIVE: we only want to execute the doctest s in interactive mode import doctest display(doctest.testmod()) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"applying doctests"},{"location":"xxiii/2023-01-10-rara-gtfo.html","text":"restart and run all or it didn't happen \u00a4 my golden rule for notebooks is Restart and run all or it didn't happen . RARA || GTFO for short. the RARA criteria is satisfied when: all of the cells are executed. conversely, there are not any unexecuted cells. all of the cells prompt numbers increase monotonically. out-of-order prompts means out-of-order execution and the possibility of hidden state. %reload_ext pidgy rara = F\" RARA \" RARA = \"__Restart and run all__\" benefits of RARA || GTFO \u00a4 a computational notebook is a literate program that composites narrative and code; it simultaneously has the qualities of literature and a program . literate programs introduce a computational qualities to documents, and the compute implied the authoring system had state . a common complaint is that literate notebook programs contain hidden states . hidden state is happens when code cells are executed out-of-order, or not executed at all. executing code cells ensures that code can be read by at least one machine; the code input worked at some point. ordered execution is an objective measure of non-hidden state 1 . in the literature, Exploration and Explanation in Computational Notebooks , a large scale analysis of notebooks, they distinguish between linear notebooks and non-linear notebooks. non-linear notebooks are most likely to have hidden state . at scale, restart and run all is a distinguishing quality of computational notebooks. The most prevalent expression of literate computing right now is the computational notebook. fernando perez - co-founder of Project Jupyter - acknowledged that notebooks are literate programs with something extra. he describes notebook authoring as literate computing where live computation is woven into the document. the notebook document is stateful, statefulness is a quality of this new media. the restart and run all criteria verifies that code inputs work and there is not hidden state. executed code in computational notebooks \u00a4 when notebooks are observed as documents, code objects are literary devices actively participating in the narrative. these code objects have two distinct states: NOT executed code un-executed reverses back into normal language. it is arbitrary symbol propping up the narrative. executed code executed code encapsulates a form of pre-formatted input code and corresponding output respresentations. the input and outputs cooperate - through text and form - to bring meaning to the coded object. NOT executed code has arbitrary meaning, trust it like you would code returned from chatgpt . on the other hand, executed code has inputs, outputs and prompts. when the prompts are populated we know a cell has been executed. when all the cells execute in order we satisfy the restart and run all criteria. Restart and run all criteria implementation \u00a4 def restart_and_run_all(file, code_cell_count=0): restart_and_run_all is violated when we iterate through the the notebook cells for i, cell in enumerate((notebook := read_file(file)).cells): if cell[\"cell_type\"] == \"code\": to discover a code cell: code_cell_count += 1 try: that has not been executed because the prompt is not an integer current_count = int(cell.execution_count) except ValueError: raise UnexecutedCell(F\"cell {i} not executed\") if code_cell_count != current_count: has cell prompts that do not align with current cell count raise OutOfOrderExecution(F\"cell {i} is executed out of order\") Restart and run all exceptions \u00a4 we assign two formal BaseException s for restart_and_run_all : OutOfOrderExection and UnexecutedCell unexecuted cells \u00a4 unexecuted cells are representations only by the preformatted text input. they lack an outputs or prompt number. without these features the code stands on its own. it reverts back to pseudocode. class UnexecutedCell(BaseException): UnexecutedCell s raises when the code cell execution counts is a non-integer. out of order execution \u00a4 there are some tools that are designed to be reactive or executed out of order. the RARA criterion does not dismiss these cases rather it could be a starting state. we may suspect hidden state class OutOfOrderExection: OutOfOrderExection raises when code cell execution counts are not monotonically increasing. def read_file(file): if not isinstance(file, str): file = file.__file__ with open(file) as f: return nbformat.v4.reads(f.read()) extra preservatives \u00a4 notebooks are swiss army knives documents; they have application as tests, modules, scripts and documents. the restart and run all criteria combined with the different cell execution states : INTERACTIVE , SCRIPT , or MODULE . assert restart_and_run_all(\"2023-01-09-notebooks-states.ipynb\") is None,\\ errors are raised when failing, and i don't want to demostrate failure here when we assert that a previous notebook will restart and run all we become slightly more confident about the code in that document. impacts of Restart and run all -ability \u00a4 Restart and run all and reusability \u00a4 restart and run all is critical to reusing python notebooks as modules, tests or scripts . there are a few ways of testing notebooks , and none of these methods suceed without notebooks that restart and run all. restart and run all as a practice will encourage you to write notebooks you'll be able to return to. RARA conflicts \u00a4 too many ideas \u00a4 it is easy to put too many units of thought into a notebook . this struggle can be striking when sticking to the restart and run all principle. when we write documents about single units of thought we are treading in formal testing territory. compute intensive operations \u00a4 a push against restart and run all is having large datasets or other compute intensive operations. we apply the CREAM principle and cache our compute intensive tasks. in these situations, authors will benefit data engineering there work to avoid costly repetitive tasks; you'll save your future self time. \ud83d\udce3 write notebooks that endure \u00a4 RARA || GTFO helps us trust our's and other's notebooks more. we can be slightly more confident in future uses of that work. RARA . i'm cheering for y'all. i see y'all beating the snot out of every line of code. let's make these efforts endure. remember to tell you friends Restart and run all or GTFO . devils \ud83d\ude08 share \u00a4 Restart and run all roots \u00a4 paco nathan was the first person i heard this concept from. at Jupyter Day 2016 , at the time, he was using notebooks as medium for immersive professional publications. these oriole notebooks were educational tools that embedded the teacher and the runtime together. there were some beautiful videos produced with some big names like:... paco presented what his team learned when producing teaching videos with notebooks . focus on a concise \"unit of thought\" invest the time and editorial effort to create a good introduction keep your narrative simple and reasonably linear \"chunk\" both the text and the code into understandable parts alternate between text, code, output, further links, etc. leverage markdown by providing interesting links for background, deep-dive, etc. code cells should not be long, < 10 lines code cells must show that they've run, producing at least some output load data from the container, not the network clear all output then \"Run All\" -- or it didn't happen video narratives: there's text, and there's subtext... pause after each \"beat\" -- smile, breathe, allow people to follow you and there, clear as day, in the tenth bullet, we find clear all output then \"Run All\" -- or it didn't happen . i love that this pattern was discovered when producing video content. it means that Restart and run all is critical to communicating our ideas with notebooks as the substrate. {\"state\": {}, \"version_major\": 2, \"version_minor\": 0} it is possible that inputs were tampered with, but let us assume the best intentions. \u21a9","title":"restart and run all or it didn't happen"},{"location":"xxiii/2023-01-10-rara-gtfo.html#restart-and-run-all-or-it-didnt-happen","text":"my golden rule for notebooks is Restart and run all or it didn't happen . RARA || GTFO for short. the RARA criteria is satisfied when: all of the cells are executed. conversely, there are not any unexecuted cells. all of the cells prompt numbers increase monotonically. out-of-order prompts means out-of-order execution and the possibility of hidden state. %reload_ext pidgy rara = F\" RARA \" RARA = \"__Restart and run all__\"","title":"restart and run all or it didn't happen"},{"location":"xxiii/2023-01-10-rara-gtfo.html#benefits-of-raragtfo","text":"a computational notebook is a literate program that composites narrative and code; it simultaneously has the qualities of literature and a program . literate programs introduce a computational qualities to documents, and the compute implied the authoring system had state . a common complaint is that literate notebook programs contain hidden states . hidden state is happens when code cells are executed out-of-order, or not executed at all. executing code cells ensures that code can be read by at least one machine; the code input worked at some point. ordered execution is an objective measure of non-hidden state 1 . in the literature, Exploration and Explanation in Computational Notebooks , a large scale analysis of notebooks, they distinguish between linear notebooks and non-linear notebooks. non-linear notebooks are most likely to have hidden state . at scale, restart and run all is a distinguishing quality of computational notebooks. The most prevalent expression of literate computing right now is the computational notebook. fernando perez - co-founder of Project Jupyter - acknowledged that notebooks are literate programs with something extra. he describes notebook authoring as literate computing where live computation is woven into the document. the notebook document is stateful, statefulness is a quality of this new media. the restart and run all criteria verifies that code inputs work and there is not hidden state.","title":"benefits of RARA||GTFO"},{"location":"xxiii/2023-01-10-rara-gtfo.html#executed-code-in-computational-notebooks","text":"when notebooks are observed as documents, code objects are literary devices actively participating in the narrative. these code objects have two distinct states: NOT executed code un-executed reverses back into normal language. it is arbitrary symbol propping up the narrative. executed code executed code encapsulates a form of pre-formatted input code and corresponding output respresentations. the input and outputs cooperate - through text and form - to bring meaning to the coded object. NOT executed code has arbitrary meaning, trust it like you would code returned from chatgpt . on the other hand, executed code has inputs, outputs and prompts. when the prompts are populated we know a cell has been executed. when all the cells execute in order we satisfy the restart and run all criteria.","title":"executed code in computational notebooks"},{"location":"xxiii/2023-01-10-rara-gtfo.html#restart-and-run-all-criteria-implementation","text":"def restart_and_run_all(file, code_cell_count=0): restart_and_run_all is violated when we iterate through the the notebook cells for i, cell in enumerate((notebook := read_file(file)).cells): if cell[\"cell_type\"] == \"code\": to discover a code cell: code_cell_count += 1 try: that has not been executed because the prompt is not an integer current_count = int(cell.execution_count) except ValueError: raise UnexecutedCell(F\"cell {i} not executed\") if code_cell_count != current_count: has cell prompts that do not align with current cell count raise OutOfOrderExecution(F\"cell {i} is executed out of order\")","title":"Restart and run all criteria implementation"},{"location":"xxiii/2023-01-10-rara-gtfo.html#restart-and-run-all-exceptions","text":"we assign two formal BaseException s for restart_and_run_all : OutOfOrderExection and UnexecutedCell","title":"Restart and run all exceptions"},{"location":"xxiii/2023-01-10-rara-gtfo.html#unexecuted-cells","text":"unexecuted cells are representations only by the preformatted text input. they lack an outputs or prompt number. without these features the code stands on its own. it reverts back to pseudocode. class UnexecutedCell(BaseException): UnexecutedCell s raises when the code cell execution counts is a non-integer.","title":"unexecuted cells"},{"location":"xxiii/2023-01-10-rara-gtfo.html#out-of-order-execution","text":"there are some tools that are designed to be reactive or executed out of order. the RARA criterion does not dismiss these cases rather it could be a starting state. we may suspect hidden state class OutOfOrderExection: OutOfOrderExection raises when code cell execution counts are not monotonically increasing. def read_file(file): if not isinstance(file, str): file = file.__file__ with open(file) as f: return nbformat.v4.reads(f.read())","title":"out of order execution"},{"location":"xxiii/2023-01-10-rara-gtfo.html#extra-preservatives","text":"notebooks are swiss army knives documents; they have application as tests, modules, scripts and documents. the restart and run all criteria combined with the different cell execution states : INTERACTIVE , SCRIPT , or MODULE . assert restart_and_run_all(\"2023-01-09-notebooks-states.ipynb\") is None,\\ errors are raised when failing, and i don't want to demostrate failure here when we assert that a previous notebook will restart and run all we become slightly more confident about the code in that document.","title":"extra preservatives"},{"location":"xxiii/2023-01-10-rara-gtfo.html#impacts-of-restart-and-run-all-ability","text":"","title":"impacts of Restart and run all-ability"},{"location":"xxiii/2023-01-10-rara-gtfo.html#restart-and-run-all-and-reusability","text":"restart and run all is critical to reusing python notebooks as modules, tests or scripts . there are a few ways of testing notebooks , and none of these methods suceed without notebooks that restart and run all. restart and run all as a practice will encourage you to write notebooks you'll be able to return to.","title":"Restart and run all and reusability"},{"location":"xxiii/2023-01-10-rara-gtfo.html#rara-conflicts","text":"","title":"RARA conflicts"},{"location":"xxiii/2023-01-10-rara-gtfo.html#too-many-ideas","text":"it is easy to put too many units of thought into a notebook . this struggle can be striking when sticking to the restart and run all principle. when we write documents about single units of thought we are treading in formal testing territory.","title":"too many ideas"},{"location":"xxiii/2023-01-10-rara-gtfo.html#compute-intensive-operations","text":"a push against restart and run all is having large datasets or other compute intensive operations. we apply the CREAM principle and cache our compute intensive tasks. in these situations, authors will benefit data engineering there work to avoid costly repetitive tasks; you'll save your future self time.","title":"compute intensive operations"},{"location":"xxiii/2023-01-10-rara-gtfo.html#write-notebooks-that-endure","text":"RARA || GTFO helps us trust our's and other's notebooks more. we can be slightly more confident in future uses of that work. RARA . i'm cheering for y'all. i see y'all beating the snot out of every line of code. let's make these efforts endure. remember to tell you friends Restart and run all or GTFO .","title":"\ud83d\udce3 write notebooks that endure"},{"location":"xxiii/2023-01-10-rara-gtfo.html#devils-share","text":"","title":"devils \ud83d\ude08 share"},{"location":"xxiii/2023-01-10-rara-gtfo.html#restart-and-run-all-roots","text":"paco nathan was the first person i heard this concept from. at Jupyter Day 2016 , at the time, he was using notebooks as medium for immersive professional publications. these oriole notebooks were educational tools that embedded the teacher and the runtime together. there were some beautiful videos produced with some big names like:... paco presented what his team learned when producing teaching videos with notebooks . focus on a concise \"unit of thought\" invest the time and editorial effort to create a good introduction keep your narrative simple and reasonably linear \"chunk\" both the text and the code into understandable parts alternate between text, code, output, further links, etc. leverage markdown by providing interesting links for background, deep-dive, etc. code cells should not be long, < 10 lines code cells must show that they've run, producing at least some output load data from the container, not the network clear all output then \"Run All\" -- or it didn't happen video narratives: there's text, and there's subtext... pause after each \"beat\" -- smile, breathe, allow people to follow you and there, clear as day, in the tenth bullet, we find clear all output then \"Run All\" -- or it didn't happen . i love that this pattern was discovered when producing video content. it means that Restart and run all is critical to communicating our ideas with notebooks as the substrate. {\"state\": {}, \"version_major\": 2, \"version_minor\": 0} it is possible that inputs were tampered with, but let us assume the best intentions. \u21a9","title":"Restart and run all roots"},{"location":"xxiii/2023-01-11-duckdb-search.html","text":"full text search for notebooks and files using duckdb \u00a4 duckdb is great for moderate sized data. maybe it would be good for searching notebooks. i know pandas so we are going to use pandas to load in our data reads files load contents in the nbformat create the table on a in memory duckdb at full text search the columns search the source import pandas , duckdb , functools search is our database goal \u00a4 the use of the search is demonstrated at the end of the document def search ( q ) -> pandas . DataFrame : return ( get_db () . execute ( F \"\"\" SELECT * FROM ( SELECT *, fts_main_cells.match_bm25(path, ' { q } ', fields:='source') AS score FROM cells ) WHERE score IS NOT NULL ORDER BY score DESC; \"\"\" )) . df () https://duckdb.org/docs/extensions/full_text_search @functools . lru_cache # this makes our function a singleton def get_db () -> duckdb . DuckDBPyConnection : con = duckdb . connect () con . execute ( \"CREATE TABLE cells AS SELECT * FROM sources\" ) con . execute ( \"INSERT INTO cells SELECT * FROM sources\" ) con . execute ( \"\"\"PRAGMA create_fts_index('cells', 'path', 'source');\"\"\" ) return con create a shape of the cells that duckdb can use. we ignore metadata, attachments and outputs. def get_fts_sources ( cells ): sources = cells . drop ( columns = [ \"metadata\" , \"attachments\" , \"outputs\" ]) sources . source = sources . source . str . join ( \"\" ) sources = sources . set_index ( sources . index . map ( compose_left ( map ( str ), \"#/cells/\" . join )) . rename ( \"path\" )) . reset_index () sources . execution_count = sources . execution_count . fillna ( - 1 ) return sources load all the documents in as cells \u00a4 def get_cells ( docs ): return ( docs [ \"cells\" ] . apply ( compose_left ( enumerate , list ) ) . explode () . apply ( pandas . Series ) . rename ( columns = { 0 : \"cell_ct\" , 1 : \"cell\" }) . set_index ( \"cell_ct\" , append = True )[ \"cell\" ] . apply ( pandas . Series ) ) get_files creates our first dataframes def get_files ( dir ) -> pandas . DataFrame : files = pandas . DataFrame ( index = pandas . Index ( iter_files ( dir ), name = \"file\" )) return files . assign ( suffix = files . index . map ( operator . attrgetter ( \"suffix\" ))) get_markdown_file reads a markdown file as a markdown notebook cell. def get_markdown_file ( md ): import nbformat return nbformat . v4 . new_notebook ( cells = [ nbformat . v4 . new_markdown_cell ( md )]) def get_docs ( files : pandas . DataFrame ) -> pandas . DataFrame : files = files . assign ( text = files . index . map ( pathlib . Path . read_text )) return pandas . concat ([ files [ files . suffix . eq ( \".ipynb\" )] . text . apply ( compose_left ( orjson . loads , pandas . Series )), files [ files . suffix . eq ( \".md\" )] . text . apply ( compose_left ( get_markdown_file , pandas . Series )), ]) def get_cells_frame ( dir ): return get_cells ( get_docs ( get_files ( dir ))) iter_files finds files matching an include pattern, and not matching an exclude pattern def iter_files ( dir = None , exclude = \".nox \\n .ipynb_checkpoints \\n \" , include = \"*.md \\n *.ipynb\" ): import pathspec exclude_spec = pathspec . PathSpec . from_lines ( pathspec . GitIgnorePattern , exclude . splitlines ()) include_spec = pathspec . PathSpec . from_lines ( pathspec . GitIgnorePattern , include . splitlines ()) dir = pathlib . Path ( dir or pathlib . Path . cwd ()) for f in dir . iterdir (): if f . is_dir (): if not exclude_spec . match_file ( f ): yield from iter_files ( f ) if f . is_file (): if include_spec . match_file ( f ): if not exclude_spec . match_file ( f ): yield f iter_files uses a pattern i like where pathspec defines the files included and excluded. sometimes include/exclude logic can be confusing. the .gitignore convention is adopted to rely on that and point someone else's docs. using our search function \u00a4 import pathspec , dataclasses , orjson , pathlib ; from toolz.curried import * initialize the pandas.DataFrame so duckdb can use it. our table in this work is cells initialize the duckdb tables from pandas \u00a4 https://duckdb.org/docs/guides/python/import_pandas.html if ( I := \"__file__\" not in locals ()): sources = get_fts_sources ( get_cells_frame ( \"..\" )) display ( get_db () . execute ( \"DESCRIBE cells\" ) . df ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column_name column_type null key default extra 0 path VARCHAR YES NaN NaN NaN 1 cell_type VARCHAR YES NaN NaN NaN 2 id VARCHAR YES NaN NaN NaN 3 source VARCHAR YES NaN NaN NaN 4 execution_count DOUBLE YES NaN NaN NaN sample searches \u00a4 I and display ( search ( \"pandas\" ) . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } path cell_type id source execution_count score 0 ../xxii/oct/2022-10-29-metadata-formatter.ipyn... code e3a8b43e-aaeb-4b7a-9ccb-4485ae0689a0 if ACTIVE:\\n import pandas\\n ... 8.0 1.718594 1 ../xxii/oct/2022-10-29-metadata-formatter.ipyn... code e3a8b43e-aaeb-4b7a-9ccb-4485ae0689a0 if ACTIVE:\\n import pandas\\n ... 8.0 1.718594 2 ../xxiii/2023-01-02-accessible-dataframes-basi... code 401913ff-534f-4659-aec5-0784b1f1f34c (df := pandas.DataFrame(\\n columns=... 2.0 1.679535 3 ../xxiii/2023-01-11-accessible-dataframes-comp... code 401913ff-534f-4659-aec5-0784b1f1f34c (df := pandas.DataFrame(\\n columns=... 2.0 1.679535 4 ../xxiii/2023-01-02-accessible-dataframes-basi... code 401913ff-534f-4659-aec5-0784b1f1f34c (df := pandas.DataFrame(\\n columns=... 2.0 1.679535 I and display ( search ( \"toolz\" ) . head ( 4 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } path cell_type id source execution_count score 0 ../xxii/oct/colormap-dataframes/2021-10-11-col... code 391cab50-209e-4843-8bea-0405f6734e6f import pandas, numpy, toolz.curried as toolz 1.0 3.776350 1 ../xxii/oct/colormap-dataframes/2021-10-11-col... code 391cab50-209e-4843-8bea-0405f6734e6f import pandas, numpy, toolz.curried as toolz 1.0 3.776350 2 ../xxiii/2023-01-11-duckdb-search.ipynb#/cells/20 code d3a6ca2a-7b1d-4f0a-a86c-9345913468c0 import pathspec, dataclasses, orjson, path... -1.0 3.340617 3 ../xxiii/2023-01-11-duckdb-search.ipynb#/cells/26 code 589d3fb6-a4bb-434f-a06e-05d57fe57f09 I and display(search(\"toolz\").head(4)) -1.0 3.340617 {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"full text search for notebooks and files using duckdb"},{"location":"xxiii/2023-01-11-duckdb-search.html#full-text-search-for-notebooks-and-files-using-duckdb","text":"duckdb is great for moderate sized data. maybe it would be good for searching notebooks. i know pandas so we are going to use pandas to load in our data reads files load contents in the nbformat create the table on a in memory duckdb at full text search the columns search the source import pandas , duckdb , functools","title":"full text search for notebooks and files using duckdb"},{"location":"xxiii/2023-01-11-duckdb-search.html#search-is-our-database-goal","text":"the use of the search is demonstrated at the end of the document def search ( q ) -> pandas . DataFrame : return ( get_db () . execute ( F \"\"\" SELECT * FROM ( SELECT *, fts_main_cells.match_bm25(path, ' { q } ', fields:='source') AS score FROM cells ) WHERE score IS NOT NULL ORDER BY score DESC; \"\"\" )) . df () https://duckdb.org/docs/extensions/full_text_search @functools . lru_cache # this makes our function a singleton def get_db () -> duckdb . DuckDBPyConnection : con = duckdb . connect () con . execute ( \"CREATE TABLE cells AS SELECT * FROM sources\" ) con . execute ( \"INSERT INTO cells SELECT * FROM sources\" ) con . execute ( \"\"\"PRAGMA create_fts_index('cells', 'path', 'source');\"\"\" ) return con create a shape of the cells that duckdb can use. we ignore metadata, attachments and outputs. def get_fts_sources ( cells ): sources = cells . drop ( columns = [ \"metadata\" , \"attachments\" , \"outputs\" ]) sources . source = sources . source . str . join ( \"\" ) sources = sources . set_index ( sources . index . map ( compose_left ( map ( str ), \"#/cells/\" . join )) . rename ( \"path\" )) . reset_index () sources . execution_count = sources . execution_count . fillna ( - 1 ) return sources","title":"search is our database goal"},{"location":"xxiii/2023-01-11-duckdb-search.html#load-all-the-documents-in-as-cells","text":"def get_cells ( docs ): return ( docs [ \"cells\" ] . apply ( compose_left ( enumerate , list ) ) . explode () . apply ( pandas . Series ) . rename ( columns = { 0 : \"cell_ct\" , 1 : \"cell\" }) . set_index ( \"cell_ct\" , append = True )[ \"cell\" ] . apply ( pandas . Series ) ) get_files creates our first dataframes def get_files ( dir ) -> pandas . DataFrame : files = pandas . DataFrame ( index = pandas . Index ( iter_files ( dir ), name = \"file\" )) return files . assign ( suffix = files . index . map ( operator . attrgetter ( \"suffix\" ))) get_markdown_file reads a markdown file as a markdown notebook cell. def get_markdown_file ( md ): import nbformat return nbformat . v4 . new_notebook ( cells = [ nbformat . v4 . new_markdown_cell ( md )]) def get_docs ( files : pandas . DataFrame ) -> pandas . DataFrame : files = files . assign ( text = files . index . map ( pathlib . Path . read_text )) return pandas . concat ([ files [ files . suffix . eq ( \".ipynb\" )] . text . apply ( compose_left ( orjson . loads , pandas . Series )), files [ files . suffix . eq ( \".md\" )] . text . apply ( compose_left ( get_markdown_file , pandas . Series )), ]) def get_cells_frame ( dir ): return get_cells ( get_docs ( get_files ( dir ))) iter_files finds files matching an include pattern, and not matching an exclude pattern def iter_files ( dir = None , exclude = \".nox \\n .ipynb_checkpoints \\n \" , include = \"*.md \\n *.ipynb\" ): import pathspec exclude_spec = pathspec . PathSpec . from_lines ( pathspec . GitIgnorePattern , exclude . splitlines ()) include_spec = pathspec . PathSpec . from_lines ( pathspec . GitIgnorePattern , include . splitlines ()) dir = pathlib . Path ( dir or pathlib . Path . cwd ()) for f in dir . iterdir (): if f . is_dir (): if not exclude_spec . match_file ( f ): yield from iter_files ( f ) if f . is_file (): if include_spec . match_file ( f ): if not exclude_spec . match_file ( f ): yield f iter_files uses a pattern i like where pathspec defines the files included and excluded. sometimes include/exclude logic can be confusing. the .gitignore convention is adopted to rely on that and point someone else's docs.","title":"load all the documents in as cells"},{"location":"xxiii/2023-01-11-duckdb-search.html#using-our-search-function","text":"import pathspec , dataclasses , orjson , pathlib ; from toolz.curried import * initialize the pandas.DataFrame so duckdb can use it. our table in this work is cells","title":"using our search function"},{"location":"xxiii/2023-01-11-duckdb-search.html#initialize-the-duckdb-tables-from-pandas","text":"https://duckdb.org/docs/guides/python/import_pandas.html if ( I := \"__file__\" not in locals ()): sources = get_fts_sources ( get_cells_frame ( \"..\" )) display ( get_db () . execute ( \"DESCRIBE cells\" ) . df ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column_name column_type null key default extra 0 path VARCHAR YES NaN NaN NaN 1 cell_type VARCHAR YES NaN NaN NaN 2 id VARCHAR YES NaN NaN NaN 3 source VARCHAR YES NaN NaN NaN 4 execution_count DOUBLE YES NaN NaN NaN","title":"initialize the duckdb tables from pandas"},{"location":"xxiii/2023-01-11-duckdb-search.html#sample-searches","text":"I and display ( search ( \"pandas\" ) . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } path cell_type id source execution_count score 0 ../xxii/oct/2022-10-29-metadata-formatter.ipyn... code e3a8b43e-aaeb-4b7a-9ccb-4485ae0689a0 if ACTIVE:\\n import pandas\\n ... 8.0 1.718594 1 ../xxii/oct/2022-10-29-metadata-formatter.ipyn... code e3a8b43e-aaeb-4b7a-9ccb-4485ae0689a0 if ACTIVE:\\n import pandas\\n ... 8.0 1.718594 2 ../xxiii/2023-01-02-accessible-dataframes-basi... code 401913ff-534f-4659-aec5-0784b1f1f34c (df := pandas.DataFrame(\\n columns=... 2.0 1.679535 3 ../xxiii/2023-01-11-accessible-dataframes-comp... code 401913ff-534f-4659-aec5-0784b1f1f34c (df := pandas.DataFrame(\\n columns=... 2.0 1.679535 4 ../xxiii/2023-01-02-accessible-dataframes-basi... code 401913ff-534f-4659-aec5-0784b1f1f34c (df := pandas.DataFrame(\\n columns=... 2.0 1.679535 I and display ( search ( \"toolz\" ) . head ( 4 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } path cell_type id source execution_count score 0 ../xxii/oct/colormap-dataframes/2021-10-11-col... code 391cab50-209e-4843-8bea-0405f6734e6f import pandas, numpy, toolz.curried as toolz 1.0 3.776350 1 ../xxii/oct/colormap-dataframes/2021-10-11-col... code 391cab50-209e-4843-8bea-0405f6734e6f import pandas, numpy, toolz.curried as toolz 1.0 3.776350 2 ../xxiii/2023-01-11-duckdb-search.ipynb#/cells/20 code d3a6ca2a-7b1d-4f0a-a86c-9345913468c0 import pathspec, dataclasses, orjson, path... -1.0 3.340617 3 ../xxiii/2023-01-11-duckdb-search.ipynb#/cells/26 code 589d3fb6-a4bb-434f-a06e-05d57fe57f09 I and display(search(\"toolz\").head(4)) -1.0 3.340617 {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"sample searches"},{"location":"xxiii/2023-01-11-extensible-metadata.html","text":"extensible metadata in the notebook format \u00a4 this document is the minimum schema to make metadata extensible. it makes clearer distinctions between the notebook, cell and display metadata. new metadata schema are combined using the allOf key that references schema from other places. %reload_ext pidgy from midgy.utils import *; import tomli the extended schema is written below in toml. schema = strip_fence | tomli.loads | \\ toml description = \"extensible metadata for the notebook schema.\" [\"$defs\".notebook-meta] \"$anchor\" = \"notebook-meta\" allOf = [ {\"$ref\" = \"kernelspec.json\"} ] [\"$defs\".code-meta] \"$anchor\" = \"code-meta\" allOf = [ {\"$ref\" = \"#slides\"} ] [\"$defs\".display-meta] \"$anchor\" = \"display-meta\" allOf = [ {\"$ref\" = \"#images\"}, {\"$ref\" = \"#json\"} ] [\"$defs\".slides] \"$anchor\" = \"slides\" properties.slides.enum = [\"slide\", \"subslide\", \"fragment\"] [properties] metadata.\"$ref\" = \"#notebook-meta\" [properties.cells.items.properties.metadata] \"$ref\" = \"#cell-meta\" [properties.cells.items.properties.outputs.items.properties.metadata] outputs.items.properties.metadata.\"$ref\" = \"#display-meta\" it is shown in json below. {'$defs': {'code-meta': {'$anchor': 'code-meta', 'allOf': [{'$ref': '#slides'}]}, 'display-meta': {'$anchor': 'display-meta', 'allOf': [{'$ref': '#images'}, {'$ref': '#json'}]}, 'notebook-meta': {'$anchor': 'notebook-meta', 'allOf': [{'$ref': 'kernelspec.json'}]}, 'slides': {'$anchor': 'slides', 'properties': {'slides': {'enum': ['slide', 'subslide', 'fragment']}}}}, 'description': 'extensible metadata for the notebook schema.', 'properties': {'cells': {'items': {'properties': {'metadata': {'$ref': '#cell-meta'}, 'outputs': {'items': {'properties': {'metadata': {'outputs': {'items': {'properties': {'metadata': {'$ref': '#display-meta'}}}}}}}}}}}, 'metadata': {'$ref': '#notebook-meta'}}} pprint.pprint(schema) JSON(schema, expanded=False) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"extensible metadata in the notebook format"},{"location":"xxiii/2023-01-11-extensible-metadata.html#extensible-metadata-in-the-notebook-format","text":"this document is the minimum schema to make metadata extensible. it makes clearer distinctions between the notebook, cell and display metadata. new metadata schema are combined using the allOf key that references schema from other places. %reload_ext pidgy from midgy.utils import *; import tomli the extended schema is written below in toml. schema = strip_fence | tomli.loads | \\ toml description = \"extensible metadata for the notebook schema.\" [\"$defs\".notebook-meta] \"$anchor\" = \"notebook-meta\" allOf = [ {\"$ref\" = \"kernelspec.json\"} ] [\"$defs\".code-meta] \"$anchor\" = \"code-meta\" allOf = [ {\"$ref\" = \"#slides\"} ] [\"$defs\".display-meta] \"$anchor\" = \"display-meta\" allOf = [ {\"$ref\" = \"#images\"}, {\"$ref\" = \"#json\"} ] [\"$defs\".slides] \"$anchor\" = \"slides\" properties.slides.enum = [\"slide\", \"subslide\", \"fragment\"] [properties] metadata.\"$ref\" = \"#notebook-meta\" [properties.cells.items.properties.metadata] \"$ref\" = \"#cell-meta\" [properties.cells.items.properties.outputs.items.properties.metadata] outputs.items.properties.metadata.\"$ref\" = \"#display-meta\" it is shown in json below. {'$defs': {'code-meta': {'$anchor': 'code-meta', 'allOf': [{'$ref': '#slides'}]}, 'display-meta': {'$anchor': 'display-meta', 'allOf': [{'$ref': '#images'}, {'$ref': '#json'}]}, 'notebook-meta': {'$anchor': 'notebook-meta', 'allOf': [{'$ref': 'kernelspec.json'}]}, 'slides': {'$anchor': 'slides', 'properties': {'slides': {'enum': ['slide', 'subslide', 'fragment']}}}}, 'description': 'extensible metadata for the notebook schema.', 'properties': {'cells': {'items': {'properties': {'metadata': {'$ref': '#cell-meta'}, 'outputs': {'items': {'properties': {'metadata': {'outputs': {'items': {'properties': {'metadata': {'$ref': '#display-meta'}}}}}}}}}}}, 'metadata': {'$ref': '#notebook-meta'}}} pprint.pprint(schema) JSON(schema, expanded=False) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"extensible metadata in the notebook format"},{"location":"xxiii/2023-01-12-giscus-comments.html","text":"adding giscus comments to this blog \u00a4 i've wanted comments for a bit. i didn't realize what a can of worms this decision was. during the day i implemented: disqus after finding that mkdocs-material deprecated it. https://github.com/squidfunk/mkdocs-material/pull/3329 then i tried utterances , but i guess that is old hat. now we have giscus because it is successor of utterances. https://yihui.org/en/2022/12/disqus-to-giscus/ the diff \u00a4 the new comment system was added by overriding the commments.html partial. if I := \"__file__\" not in locals (): ! cd ../.. && git log -- name - only -- pretty = oneline 8 f808b9208e8226b68a243882b5738258d1a6c72 .. c184b6d10b3405b2b0e227b21dc37c5285800214 the comments.html file \u00a4 the snippet in generated by the giscus app . everything needs to be filled out! if I : import pathlib , IPython display ( IPython . display . Code ( pathlib . Path ( \"../../overrides/partials/comments.html\" ) . read_text (), language = \"html\" )) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"adding giscus comments to this blog"},{"location":"xxiii/2023-01-12-giscus-comments.html#adding-giscus-comments-to-this-blog","text":"i've wanted comments for a bit. i didn't realize what a can of worms this decision was. during the day i implemented: disqus after finding that mkdocs-material deprecated it. https://github.com/squidfunk/mkdocs-material/pull/3329 then i tried utterances , but i guess that is old hat. now we have giscus because it is successor of utterances. https://yihui.org/en/2022/12/disqus-to-giscus/","title":"adding giscus comments to this blog"},{"location":"xxiii/2023-01-12-giscus-comments.html#the-diff","text":"the new comment system was added by overriding the commments.html partial. if I := \"__file__\" not in locals (): ! cd ../.. && git log -- name - only -- pretty = oneline 8 f808b9208e8226b68a243882b5738258d1a6c72 .. c184b6d10b3405b2b0e227b21dc37c5285800214","title":"the diff"},{"location":"xxiii/2023-01-12-giscus-comments.html#the-commentshtml-file","text":"the snippet in generated by the giscus app . everything needs to be filled out! if I : import pathlib , IPython display ( IPython . display . Code ( pathlib . Path ( \"../../overrides/partials/comments.html\" ) . read_text (), language = \"html\" )) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"the comments.html file"},{"location":"xxiii/2023-01-12-tree-sitter.html","text":"getting start with tree sitter \u00a4 installing tree sitter for python \u00a4 def task_setup_tree_sitter (): import tree_sitter , pathlib , shutil target = pathlib . Path ( \"vendor/tree-sitter-python/.git/HEAD\" ) yield dict ( name = \"clone\" , actions = [ \"git clone https://github.com/tree-sitter/tree-sitter-python vendor/tree-sitter-python --depth 1\" ], targets = [ target ], uptodate = [ target . exists ], clean = [ \"rm -rf vendor\" ] ) yield dict ( name = \"compile\" , actions = [( tree_sitter . Language . build_library , ( 'build/my-languages.so' , [ 'vendor/tree-sitter-python' ]))], file_dep = [ target ], targets = [ \"build/my-languages.so\" ], clean = [ \"rm build/my-languages.so\" ] ) if I := __name__ == \"__main__\" : % reload_ext doit % doit setup_tree_sitter -- setup_tree_sitter:clone -- setup_tree_sitter:compile loading a bunch of python code \u00a4 we have some nice dataframes in a prior post that we'll use for demonstration with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxiii.__duckdb_search import * if I := \"__file__\" not in locals (): cells = get_cells_frame ( \"..\" ) cells . source = cells . source . apply ( \"\" . join ) some of the cells might have pidgy syntax so lets sort that otu. if I : import midgy cells = cells . source . str . contains ( \"%(re)?load_ext\\s+(pidgy)\" ) . groupby ( \"file\" ) . any () . rename ( \"pidgy\" ) . pipe ( cells . join ) cells . loc [ cells [ cells . pidgy ] . index , \"source\" ] = cells [ cells . pidgy ] . source . apply ( midgy . Python () . render ) /tmp/ipykernel_994317/2050936253.py:3: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract. cells = cells.source.str.contains(\"%(re)?load_ext\\s+(pidgy)\").groupby(\"file\").any().rename(\"pidgy\").pipe(cells.join) tree sitting parser \u00a4 if I : import tree_sitter parser = tree_sitter . Parser () parser . set_language ( language := tree_sitter . Language ( \"build/my-languages.so\" , \"python\" )) display ( parser ) tree sitting parsed \u00a4 if I : sitter = cells . source . apply ( compose_left ( str . encode , parser . parse )) sexp = sitter . apply ( compose_left ( operator . attrgetter ( \"root_node\" ), operator . methodcaller ( \"sexp\" ))) display ( sexp . to_frame ( \"s-expression\" )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } s-expression file cell_ct ../regexs.ipynb 0 (module (comment)) 1 (module (import_statement name: (dotted_name (... 2 (module (comment) (expression_statement (compa... 3 (module (expression_statement (assignment left... 4 (module) ... ... ... ../xxii/2022-12-23-mkdocs-plugin.ipynb 8 (module (ERROR (identifier) (identifier) (stri... 9 (module (expression_statement (augmented_assig... ../xxiii/vendor/tree-sitter-python/README.md 0 (module (expression_statement (binary_operator... ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md 0 (module (comment) (ERROR (identifier) (identif... ../README.md 0 (module (ERROR (UNEXPECTED '-')) (expression_s... 866 rows \u00d7 1 columns {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"getting start with tree sitter"},{"location":"xxiii/2023-01-12-tree-sitter.html#getting-start-with-tree-sitter","text":"","title":"getting start with tree sitter"},{"location":"xxiii/2023-01-12-tree-sitter.html#installing-tree-sitter-for-python","text":"def task_setup_tree_sitter (): import tree_sitter , pathlib , shutil target = pathlib . Path ( \"vendor/tree-sitter-python/.git/HEAD\" ) yield dict ( name = \"clone\" , actions = [ \"git clone https://github.com/tree-sitter/tree-sitter-python vendor/tree-sitter-python --depth 1\" ], targets = [ target ], uptodate = [ target . exists ], clean = [ \"rm -rf vendor\" ] ) yield dict ( name = \"compile\" , actions = [( tree_sitter . Language . build_library , ( 'build/my-languages.so' , [ 'vendor/tree-sitter-python' ]))], file_dep = [ target ], targets = [ \"build/my-languages.so\" ], clean = [ \"rm build/my-languages.so\" ] ) if I := __name__ == \"__main__\" : % reload_ext doit % doit setup_tree_sitter -- setup_tree_sitter:clone -- setup_tree_sitter:compile","title":"installing tree sitter for python"},{"location":"xxiii/2023-01-12-tree-sitter.html#loading-a-bunch-of-python-code","text":"we have some nice dataframes in a prior post that we'll use for demonstration with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxiii.__duckdb_search import * if I := \"__file__\" not in locals (): cells = get_cells_frame ( \"..\" ) cells . source = cells . source . apply ( \"\" . join ) some of the cells might have pidgy syntax so lets sort that otu. if I : import midgy cells = cells . source . str . contains ( \"%(re)?load_ext\\s+(pidgy)\" ) . groupby ( \"file\" ) . any () . rename ( \"pidgy\" ) . pipe ( cells . join ) cells . loc [ cells [ cells . pidgy ] . index , \"source\" ] = cells [ cells . pidgy ] . source . apply ( midgy . Python () . render ) /tmp/ipykernel_994317/2050936253.py:3: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract. cells = cells.source.str.contains(\"%(re)?load_ext\\s+(pidgy)\").groupby(\"file\").any().rename(\"pidgy\").pipe(cells.join)","title":"loading a bunch of python code"},{"location":"xxiii/2023-01-12-tree-sitter.html#tree-sitting-parser","text":"if I : import tree_sitter parser = tree_sitter . Parser () parser . set_language ( language := tree_sitter . Language ( \"build/my-languages.so\" , \"python\" )) display ( parser )","title":"tree sitting parser"},{"location":"xxiii/2023-01-12-tree-sitter.html#tree-sitting-parsed","text":"if I : sitter = cells . source . apply ( compose_left ( str . encode , parser . parse )) sexp = sitter . apply ( compose_left ( operator . attrgetter ( \"root_node\" ), operator . methodcaller ( \"sexp\" ))) display ( sexp . to_frame ( \"s-expression\" )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } s-expression file cell_ct ../regexs.ipynb 0 (module (comment)) 1 (module (import_statement name: (dotted_name (... 2 (module (comment) (expression_statement (compa... 3 (module (expression_statement (assignment left... 4 (module) ... ... ... ../xxii/2022-12-23-mkdocs-plugin.ipynb 8 (module (ERROR (identifier) (identifier) (stri... 9 (module (expression_statement (augmented_assig... ../xxiii/vendor/tree-sitter-python/README.md 0 (module (expression_statement (binary_operator... ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md 0 (module (comment) (ERROR (identifier) (identif... ../README.md 0 (module (ERROR (UNEXPECTED '-')) (expression_s... 866 rows \u00d7 1 columns {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"tree sitting parsed"},{"location":"xxiii/2023-01-14-links-in-markdown-tokens.html","text":"where do all the good links go? \u00a4 extractling links and definitions from markdown using markdown_it tokens def get_exporter ( key = \"mkdocs\" , ** kw ): with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxii.__markdownish_notebook import template , HEAD , replace_attachments , PidgyExporter kw . setdefault ( \"template_file\" , key ) exporter = PidgyExporter ( ** kw ) exporter . environment . filters . setdefault ( \"attachment\" , replace_attachments ) from jinja2 import DictLoader for loader in exporter . environment . loader . loaders : if isinstance ( loader , DictLoader ): loader . mapping [ key ] = template loader . mapping [ \"HEAD\" ] = HEAD break return exporter with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxiii.__duckdb_search import * from tonyfast.xxii.__markdownish_notebook import PidgyExporter , template from midgy import Python import nbformat from markdown_it.tree import SyntaxTreeNode @dataclasses . dataclass class Finder : dir : str = \"..\" include : str = \"*.ipynb \\n *.md\" exclude : str = \".ipynb_checkpoints\" def get_files_stats ( self , path ): stat = path . stat () return dict ( path = path , suffix = path . suffix , created_at = stat . st_ctime , modified_at = stat . st_mtime , size = stat . st_size ) def get_files ( self ) -> list [ dict ]: return list ( map ( self . get_files_stats , iter_files ( self . dir , self . include , self . exclude ))) def __iter__ ( self ): yield from self . get_files () def to_frame ( self , updated_from = None ): df = pandas . DataFrame ( self ) . set_index ( \"path\" ) if updated_from is not None : return df [ df . modified_at . ne ( updated_from . modified_at )] return df def to_dask ( self ): from dask.dataframe import from_pandas return from_pandas ( df := self . to_frame (), npartitions = len ( df )) order = dict ([( \"cells\" , \"O\" ), ( \"metadata\" , \"O\" ), ( \"nbformat\" , int ), ( \"nbformat_minor\" , int )]) ( ddf := Finder () . to_dask () ) ddf = ddf . assign ( loader = ddf . suffix . apply ({ \".md\" : get_markdown_file , \".ipynb\" : nbformat . v4 . reads } . get , meta = ( \"loader\" , \"O\" ))) ddf = ddf . assign ( data = ddf . apply ( lambda s : s . loader ( s . name . read_text ()), axis = 1 , meta = ( \"data\" , \"O\" )) ) ddf = ddf . assign ( md = ddf . data . apply ( compose_left ( get_exporter () . from_notebook_node , first ), meta = ( \"md\" , \"O\" ))) ddf = ddf . assign ( tokens = ddf . md . apply ( Python () . parse , meta = ( \"tokens\" , \"O\" ))) ddf Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } suffix created_at modified_at size loader data md tokens npartitions=50 ../xxii/2022-11-12-async-import.ipynb object float64 float64 int64 object object object object ../xxii/2022-11-12-pluggy-experiments.ipynb ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md ... ... ... ... ... ... ... ... ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md ... ... ... ... ... ... ... ... Dask Name: assign, 12 graph layers how many tokens are there? \u00a4 s = ddf . tokens . apply ( compose_left ( SyntaxTreeNode , operator . methodcaller ( \"walk\" ), list ), meta = ( \"token\" , \"O\" )) . explode () s . apply ( operator . attrgetter ( \"type\" ), meta = ( \"type\" , \"O\" )) . value_counts () . compute () all the links \u00a4 links = s [ s . apply ( compose_left ( operator . attrgetter ( \"type\" ), \"link image definition\" . split () . __contains__ ), meta = ( \"link\" , bool )) ] . compute () links . apply ( compose_left ( operator . attrgetter ( \"attrs\" , \"meta\" ), merge , pandas . Series )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } href title id url label src alt path ../xxii/2022-11-12-async-import.ipynb https://gist.github.com/Rich-Harris/0b6f317657... NaN NaN NaN NaN NaN NaN ../xxii/2022-11-12-async-import.ipynb https://docs.python.org/3/library/ast.html#ast... NaN NaN NaN NaN NaN NaN ../xxii/2022-11-12-pluggy-experiments.ipynb https://pluggy.readthedocs.io/ NaN NaN NaN NaN NaN NaN ../xxii/2022-11-17-assignment-expression-display.ipynb https://peps.python.org/pep-0572/ NaN NaN NaN NaN NaN NaN ../xxii/2022-11-23-better-dask-shape.ipynb oct/2022-10-05-dask-search.ipynb NaN NaN NaN NaN NaN NaN ... ... ... ... ... ... ... ... ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md NaN LANGUAGE FUNC https://docs.rs/tree-sitter-python/*/tree_sitt... language func NaN NaN ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md NaN PARSER https://docs.rs/tree-sitter/*/tree_sitter/stru... Parser NaN NaN ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md NaN TREE-SITTER https://tree-sitter.github.io/ tree-sitter NaN NaN ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md NaN TREE-SITTER CRATE https://crates.io/crates/tree-sitter tree-sitter crate NaN NaN ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md NaN TREE-SITTER DISCUSSIONS https://github.com/tree-sitter/tree-sitter/dis... tree-sitter discussions NaN NaN 159 rows \u00d7 7 columns {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"where do all the good links go?"},{"location":"xxiii/2023-01-14-links-in-markdown-tokens.html#where-do-all-the-good-links-go","text":"extractling links and definitions from markdown using markdown_it tokens def get_exporter ( key = \"mkdocs\" , ** kw ): with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxii.__markdownish_notebook import template , HEAD , replace_attachments , PidgyExporter kw . setdefault ( \"template_file\" , key ) exporter = PidgyExporter ( ** kw ) exporter . environment . filters . setdefault ( \"attachment\" , replace_attachments ) from jinja2 import DictLoader for loader in exporter . environment . loader . loaders : if isinstance ( loader , DictLoader ): loader . mapping [ key ] = template loader . mapping [ \"HEAD\" ] = HEAD break return exporter with __import__ ( \"importnb\" ) . Notebook (): from tonyfast.xxiii.__duckdb_search import * from tonyfast.xxii.__markdownish_notebook import PidgyExporter , template from midgy import Python import nbformat from markdown_it.tree import SyntaxTreeNode @dataclasses . dataclass class Finder : dir : str = \"..\" include : str = \"*.ipynb \\n *.md\" exclude : str = \".ipynb_checkpoints\" def get_files_stats ( self , path ): stat = path . stat () return dict ( path = path , suffix = path . suffix , created_at = stat . st_ctime , modified_at = stat . st_mtime , size = stat . st_size ) def get_files ( self ) -> list [ dict ]: return list ( map ( self . get_files_stats , iter_files ( self . dir , self . include , self . exclude ))) def __iter__ ( self ): yield from self . get_files () def to_frame ( self , updated_from = None ): df = pandas . DataFrame ( self ) . set_index ( \"path\" ) if updated_from is not None : return df [ df . modified_at . ne ( updated_from . modified_at )] return df def to_dask ( self ): from dask.dataframe import from_pandas return from_pandas ( df := self . to_frame (), npartitions = len ( df )) order = dict ([( \"cells\" , \"O\" ), ( \"metadata\" , \"O\" ), ( \"nbformat\" , int ), ( \"nbformat_minor\" , int )]) ( ddf := Finder () . to_dask () ) ddf = ddf . assign ( loader = ddf . suffix . apply ({ \".md\" : get_markdown_file , \".ipynb\" : nbformat . v4 . reads } . get , meta = ( \"loader\" , \"O\" ))) ddf = ddf . assign ( data = ddf . apply ( lambda s : s . loader ( s . name . read_text ()), axis = 1 , meta = ( \"data\" , \"O\" )) ) ddf = ddf . assign ( md = ddf . data . apply ( compose_left ( get_exporter () . from_notebook_node , first ), meta = ( \"md\" , \"O\" ))) ddf = ddf . assign ( tokens = ddf . md . apply ( Python () . parse , meta = ( \"tokens\" , \"O\" ))) ddf Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } suffix created_at modified_at size loader data md tokens npartitions=50 ../xxii/2022-11-12-async-import.ipynb object float64 float64 int64 object object object object ../xxii/2022-11-12-pluggy-experiments.ipynb ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md ... ... ... ... ... ... ... ... ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md ... ... ... ... ... ... ... ... Dask Name: assign, 12 graph layers","title":"where do all the good links go?"},{"location":"xxiii/2023-01-14-links-in-markdown-tokens.html#how-many-tokens-are-there","text":"s = ddf . tokens . apply ( compose_left ( SyntaxTreeNode , operator . methodcaller ( \"walk\" ), list ), meta = ( \"token\" , \"O\" )) . explode () s . apply ( operator . attrgetter ( \"type\" ), meta = ( \"type\" , \"O\" )) . value_counts () . compute ()","title":"how many tokens are there?"},{"location":"xxiii/2023-01-14-links-in-markdown-tokens.html#all-the-links","text":"links = s [ s . apply ( compose_left ( operator . attrgetter ( \"type\" ), \"link image definition\" . split () . __contains__ ), meta = ( \"link\" , bool )) ] . compute () links . apply ( compose_left ( operator . attrgetter ( \"attrs\" , \"meta\" ), merge , pandas . Series )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } href title id url label src alt path ../xxii/2022-11-12-async-import.ipynb https://gist.github.com/Rich-Harris/0b6f317657... NaN NaN NaN NaN NaN NaN ../xxii/2022-11-12-async-import.ipynb https://docs.python.org/3/library/ast.html#ast... NaN NaN NaN NaN NaN NaN ../xxii/2022-11-12-pluggy-experiments.ipynb https://pluggy.readthedocs.io/ NaN NaN NaN NaN NaN NaN ../xxii/2022-11-17-assignment-expression-display.ipynb https://peps.python.org/pep-0572/ NaN NaN NaN NaN NaN NaN ../xxii/2022-11-23-better-dask-shape.ipynb oct/2022-10-05-dask-search.ipynb NaN NaN NaN NaN NaN NaN ... ... ... ... ... ... ... ... ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md NaN LANGUAGE FUNC https://docs.rs/tree-sitter-python/*/tree_sitt... language func NaN NaN ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md NaN PARSER https://docs.rs/tree-sitter/*/tree_sitter/stru... Parser NaN NaN ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md NaN TREE-SITTER https://tree-sitter.github.io/ tree-sitter NaN NaN ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md NaN TREE-SITTER CRATE https://crates.io/crates/tree-sitter tree-sitter crate NaN NaN ../xxiii/vendor/tree-sitter-python/bindings/rust/README.md NaN TREE-SITTER DISCUSSIONS https://github.com/tree-sitter/tree-sitter/dis... tree-sitter discussions NaN NaN 159 rows \u00d7 7 columns {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"all the links"},{"location":"xxiii/2023-01-16-inspector-hack.html","text":"customizing the IPython contextual help \u00a4 an original feature of pidgy is the use jupyter s contextual help to provide wysiwyg experience. as author's write pidgy they reveal a preview of the output and explore variables in the workspace. when source is found show help when inspection is found show the found results when is not found show the markdown preview the inspector uses jupyter s rich display system. we can send html and markdown to the contextual help. we create an ipython extension that modifies the IPython contextual help using rich displays. we are going to create a wsyiwyg for %reload_ext pidgy import pandas for any of this to work need to enable the html pager shell.enable_html_pager shell.enable_html_pager = True do_inspect will be our new contextual help completer. it provides completion when: there is no content to inspect there was no result found you found the bangs def do_inspect(self, cell, cursor_pos, detail_level=1, omit_sections=(), *, cache={}): post = post_bangs(cell, get_bangs(cell, cursor_pos)) if post: data = _do_inspect(\"\", 0, detail_level=0, omit_sections=()) data[\"data\"]= post data[\"found\"] = True else: data = _do_inspect(cell, cursor_pos, detail_level=0, omit_sections=()) if not data[\"found\"]: data.update(inspect_weave(cell, cursor_pos)) return data def inspect_weave(code, cursor_pos, cache={}): data = dict(found=True) if not code.strip(): data[\"data\"] = {\"text/markdown\": help} else: tokens = cache.get(code) line, offset= lineno_at_cursor(code, cursor_pos) if tokens is None: cache.clear() tokens = cache[code] = shell.tangle.parse(code) where = get_md_pointer(tokens, line, offset) data[\"data\"] = {\"text/markdown\": F\"\"\" {\"/\".join(where)} \\n\\n{code}\"\"\"} return data this is a hack! we'll hold the original inspect function and monkey patch it. locals().setdefault(\"_do_inspect\", shell.kernel.do_inspect) if I := (\"__file__\" not in locals()): load_ipython_extension(shell) line positions and tokenization \u00a4 it seemed important to have line and column numbers that align exactly with the code mirror line numbers and the line/column number in the bottom inspector ribbon. for extra confidence when writing pidgy we include the markdown block token the cursor is within. def lineno_at_cursor(cell, cursor_pos=0): offset = 0 for i, line in enumerate(cell.splitlines(True)): next_offset = offset + len(line) if not line.endswith('\\n'): next_offset += 1 if next_offset > cursor_pos: break offset = next_offset col = cursor_pos-offset return i, col def get_md_pointer(tokens, line, offset): where = [F\"L{line+1}:{offset+1}\"] for node in tokens: if node.type == \"root\": continue if node.map: if line < node.map[0]: break elif node.map[0] <= line < node.map[1]: where.append(node.type) return where the bangs \u00a4 the bangs give extra interactivity. when inspecting code a group of exclamation !!! points with template the woven source, or with enough emphasism !!!!!! we'll actually run code. consider these easter eggs cause i want them. def get_bangs(cell, cursor_pos): if cell[cursor_pos-1] == \"!\": l, r = cell[:cursor_pos], cell[cursor_pos:] return len(l) - len(l.rstrip(\"!\")) + len(r) - len(r.strip(\"!\")) return 0 def post_bangs(cell, bangs): if bangs>=6: result = shell.run_cell(cell, store_history=False, silent=True) error = result.error_before_exec or result.error_in_exec if error: return {\"text/markdown\": F\"\"\"```pycon\\n{\"\".join( traceback.format_exception(type(error), error, error.__traceback__) )}\\n```\"\"\"} else: shell.weave.update() if bangs >=3: result = shell.environment.from_string(cell, None, pidgy.environment.IPythonTemplate) return {\"text/markdown\": result.render()} the opportunity in the misses \u00a4 the contextual help renders on keypress when a result is found. when we use the normal inspection. we find that there is a lot of time where the contextual help is not found. for example, when we process all the code inputs in this document the inspector finds information 36.0% of the time, meaning 64.0% opportunity!! def measure(x): yield from (shell.kernel.do_inspect(x, i) for i in range(len(x))) shell.kernel.do_inspect = _do_inspect inspection = pandas.Series(In).apply(measure).apply(list).explode().dropna().apply(pandas.Series) load_ipython_extension(shell) a use is the blank screen is to provide a help interface for people to learn our new tool. with links to good docs. help =\\ pidgy is markdown/python literate programming language. indented code and fenced code are executed documentation \u00a4 jinja2 documentation markdown documentation python documentation including the current markdown token \u00a4 sometimes when writing pidgy its possible to lose your you position. we include the line number and token the cursor is in an accidental outcome is this hack adds contextual help to markdown cells. its good. {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"customizing the <pre>IPython</pre> contextual help"},{"location":"xxiii/2023-01-16-inspector-hack.html#customizing-the-ipython-contextual-help","text":"an original feature of pidgy is the use jupyter s contextual help to provide wysiwyg experience. as author's write pidgy they reveal a preview of the output and explore variables in the workspace. when source is found show help when inspection is found show the found results when is not found show the markdown preview the inspector uses jupyter s rich display system. we can send html and markdown to the contextual help. we create an ipython extension that modifies the IPython contextual help using rich displays. we are going to create a wsyiwyg for %reload_ext pidgy import pandas for any of this to work need to enable the html pager shell.enable_html_pager shell.enable_html_pager = True do_inspect will be our new contextual help completer. it provides completion when: there is no content to inspect there was no result found you found the bangs def do_inspect(self, cell, cursor_pos, detail_level=1, omit_sections=(), *, cache={}): post = post_bangs(cell, get_bangs(cell, cursor_pos)) if post: data = _do_inspect(\"\", 0, detail_level=0, omit_sections=()) data[\"data\"]= post data[\"found\"] = True else: data = _do_inspect(cell, cursor_pos, detail_level=0, omit_sections=()) if not data[\"found\"]: data.update(inspect_weave(cell, cursor_pos)) return data def inspect_weave(code, cursor_pos, cache={}): data = dict(found=True) if not code.strip(): data[\"data\"] = {\"text/markdown\": help} else: tokens = cache.get(code) line, offset= lineno_at_cursor(code, cursor_pos) if tokens is None: cache.clear() tokens = cache[code] = shell.tangle.parse(code) where = get_md_pointer(tokens, line, offset) data[\"data\"] = {\"text/markdown\": F\"\"\" {\"/\".join(where)} \\n\\n{code}\"\"\"} return data this is a hack! we'll hold the original inspect function and monkey patch it. locals().setdefault(\"_do_inspect\", shell.kernel.do_inspect) if I := (\"__file__\" not in locals()): load_ipython_extension(shell)","title":"customizing the IPython contextual help"},{"location":"xxiii/2023-01-16-inspector-hack.html#line-positions-and-tokenization","text":"it seemed important to have line and column numbers that align exactly with the code mirror line numbers and the line/column number in the bottom inspector ribbon. for extra confidence when writing pidgy we include the markdown block token the cursor is within. def lineno_at_cursor(cell, cursor_pos=0): offset = 0 for i, line in enumerate(cell.splitlines(True)): next_offset = offset + len(line) if not line.endswith('\\n'): next_offset += 1 if next_offset > cursor_pos: break offset = next_offset col = cursor_pos-offset return i, col def get_md_pointer(tokens, line, offset): where = [F\"L{line+1}:{offset+1}\"] for node in tokens: if node.type == \"root\": continue if node.map: if line < node.map[0]: break elif node.map[0] <= line < node.map[1]: where.append(node.type) return where","title":"line positions and tokenization"},{"location":"xxiii/2023-01-16-inspector-hack.html#the-bangs","text":"the bangs give extra interactivity. when inspecting code a group of exclamation !!! points with template the woven source, or with enough emphasism !!!!!! we'll actually run code. consider these easter eggs cause i want them. def get_bangs(cell, cursor_pos): if cell[cursor_pos-1] == \"!\": l, r = cell[:cursor_pos], cell[cursor_pos:] return len(l) - len(l.rstrip(\"!\")) + len(r) - len(r.strip(\"!\")) return 0 def post_bangs(cell, bangs): if bangs>=6: result = shell.run_cell(cell, store_history=False, silent=True) error = result.error_before_exec or result.error_in_exec if error: return {\"text/markdown\": F\"\"\"```pycon\\n{\"\".join( traceback.format_exception(type(error), error, error.__traceback__) )}\\n```\"\"\"} else: shell.weave.update() if bangs >=3: result = shell.environment.from_string(cell, None, pidgy.environment.IPythonTemplate) return {\"text/markdown\": result.render()}","title":"the bangs"},{"location":"xxiii/2023-01-16-inspector-hack.html#the-opportunity-in-the-misses","text":"the contextual help renders on keypress when a result is found. when we use the normal inspection. we find that there is a lot of time where the contextual help is not found. for example, when we process all the code inputs in this document the inspector finds information 36.0% of the time, meaning 64.0% opportunity!! def measure(x): yield from (shell.kernel.do_inspect(x, i) for i in range(len(x))) shell.kernel.do_inspect = _do_inspect inspection = pandas.Series(In).apply(measure).apply(list).explode().dropna().apply(pandas.Series) load_ipython_extension(shell) a use is the blank screen is to provide a help interface for people to learn our new tool. with links to good docs. help =\\ pidgy is markdown/python literate programming language. indented code and fenced code are executed","title":"the opportunity in the misses"},{"location":"xxiii/2023-01-16-inspector-hack.html#documentation","text":"jinja2 documentation markdown documentation python documentation","title":"documentation"},{"location":"xxiii/2023-01-16-inspector-hack.html#including-the-current-markdown-token","text":"sometimes when writing pidgy its possible to lose your you position. we include the line number and token the cursor is in an accidental outcome is this hack adds contextual help to markdown cells. its good. {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"including the current markdown token"},{"location":"xxiii/2023-01-18-voila-pidgy-integration.html","text":"cleaner integration of pidgy & voila \u00a4 pidgy works great with widgets, so it should rock with voila . the default display for pidgy does NOT use widgets by default, the author would have to opt into widgets in development. however, if we detect that voila is driving our notebook then we should use the pidgy.displays.IPyWidgetsHtml display. activate pidgy %reload_ext pidgy the cookies demo \u00a4 the cookies demo is inspired by the original tanglejs . run the demo var element = $('#390fe994-2817-43f5-82c2-9df179829e8d'); {\"model_id\": \"71a814a3ccf14578bf9f988b9a194bf0\", \"version_major\": 2, \"version_minor\": 0} when you eat 3 cookies you consume 225 calories. display(cookies:=IntSlider(3, description=\"cookies\")) state information \u00a4 the display class in this verison is because voila is not running. {\"state\": {\"71a814a3ccf14578bf9f988b9a194bf0\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"IntSliderModel\", \"state\": {\"behavior\": \"drag-tap\", \"description\": \"cookies\", \"layout\": \"IPY_MODEL_bd5ea24fcfbf4ff8bf25ccf5d94a4a5c\", \"style\": \"IPY_MODEL_e8fa9d41856d496d8a3f8178c2129e1e\", \"value\": 3}}, \"bd5ea24fcfbf4ff8bf25ccf5d94a4a5c\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"2.0.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"e8fa9d41856d496d8a3f8178c2129e1e\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"SliderStyleModel\", \"state\": {\"description_width\": \"\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"cleaner integration of <pre>pidgy</pre> &amp; <pre>voila</pre>"},{"location":"xxiii/2023-01-18-voila-pidgy-integration.html#cleaner-integration-of-pidgy-voila","text":"pidgy works great with widgets, so it should rock with voila . the default display for pidgy does NOT use widgets by default, the author would have to opt into widgets in development. however, if we detect that voila is driving our notebook then we should use the pidgy.displays.IPyWidgetsHtml display. activate pidgy %reload_ext pidgy","title":"cleaner integration of pidgy &amp; voila"},{"location":"xxiii/2023-01-18-voila-pidgy-integration.html#the-cookies-demo","text":"the cookies demo is inspired by the original tanglejs . run the demo var element = $('#390fe994-2817-43f5-82c2-9df179829e8d'); {\"model_id\": \"71a814a3ccf14578bf9f988b9a194bf0\", \"version_major\": 2, \"version_minor\": 0} when you eat 3 cookies you consume 225 calories. display(cookies:=IntSlider(3, description=\"cookies\"))","title":"the cookies demo"},{"location":"xxiii/2023-01-18-voila-pidgy-integration.html#state-information","text":"the display class in this verison is because voila is not running. {\"state\": {\"71a814a3ccf14578bf9f988b9a194bf0\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"IntSliderModel\", \"state\": {\"behavior\": \"drag-tap\", \"description\": \"cookies\", \"layout\": \"IPY_MODEL_bd5ea24fcfbf4ff8bf25ccf5d94a4a5c\", \"style\": \"IPY_MODEL_e8fa9d41856d496d8a3f8178c2129e1e\", \"value\": 3}}, \"bd5ea24fcfbf4ff8bf25ccf5d94a4a5c\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"2.0.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"e8fa9d41856d496d8a3f8178c2129e1e\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"2.0.0\", \"model_name\": \"SliderStyleModel\", \"state\": {\"description_width\": \"\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"state information"},{"location":"xxiii/2023-01-24-lite-build.html","text":"including development builds in jupyter lite \u00a4 this work augments our existing lite build by vendoring things that i'm actively working on for demonstration. https://jupyterlite.readthedocs.io/en/latest/howto/python/wheels.html#adding-pyolite-wheels if __name__ == \"__main__\" : if \"INIT\" not in locals (): % pushd ../.. INIT = True with __import__ ( \"importnb\" ) . imports ( \"ipynb\" ): import tonyfast.xxii.__lite_build as prior % reload_ext doit % reload_ext pidgy . extras from doit import task_params from pathlib import Path import os , doit TARGET = Path ( \"tonyfast\" ) SITE = Path ( \"site\" , \"run\" ) PYPI = SITE / \"pypi\" /home/tbone/Documents/tonyfast @task_params ([ dict ( name = \"pypi\" , type = list , default = [ \"https://github.com/deathbeds/midgy\" , \"https://github.com/deathbeds/pidgy\" , \"https://github.com/deathbeds/importnb\" ])]) def task_lite ( pypi ): with __import__ ( \"importnb\" ) . imports ( \"ipynb\" ): from tonyfast.xxii.__lite_build import set_files_imports env = os . environ . copy () env [ \"SETUPTOOLS_SCM_PRETEND_VERSION\" ] = \"6.6.6\" hatch = list ( filter ( lambda x : x . startswith ( \"HATCH\" ), env )) for x in hatch : env . pop ( x ) wheels = [] for repo in pypi : org , _ , name = repo . rpartition ( \"/\" ) target = TARGET / name yield dict ( name = F \"clone: { repo } \" , actions = [ F \"git clone --depth 1 { repo } { target } \" ], targets = [ HEAD := ( target / \".git\" / \"HEAD\" )], clean = [ F \"rm -rf { target } \" ], uptodate = [ target . exists ()] ) pypi = Path ( \"pypi\" ) . absolute () # this directory needs to relative to build directory wheel = pypi / F \" { name } -6.6.6-py3-none-any.whl\" wheels . append ( wheel ) yield dict ( name = F \"build: { name } \" , actions = [ ( doit . tools . create_folder , [ pypi ]), doit . tools . CmdAction ( F \"hatch -v build -t wheel { pypi } \" , cwd = target , env = env ) ], clean = True , targets = [ wheel ], uptodate = [ False ] ) yield dict ( name = \"build\" , file_dep = wheels , actions = [ \"rm -rf tonyfast/pidgy/lite tonyfast/pidgy/.binder\" , \"jupyter lite build --contents tonyfast --output-dir site/run\" , ( set_files_imports , ( Path ( \"site/run/files\" ),)) ], targets = [ \"site/run/index.html\" ], clean = [ \"rm -rf site/run\" ] ) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"including development builds in jupyter lite"},{"location":"xxiii/2023-01-24-lite-build.html#including-development-builds-in-jupyter-lite","text":"this work augments our existing lite build by vendoring things that i'm actively working on for demonstration. https://jupyterlite.readthedocs.io/en/latest/howto/python/wheels.html#adding-pyolite-wheels if __name__ == \"__main__\" : if \"INIT\" not in locals (): % pushd ../.. INIT = True with __import__ ( \"importnb\" ) . imports ( \"ipynb\" ): import tonyfast.xxii.__lite_build as prior % reload_ext doit % reload_ext pidgy . extras from doit import task_params from pathlib import Path import os , doit TARGET = Path ( \"tonyfast\" ) SITE = Path ( \"site\" , \"run\" ) PYPI = SITE / \"pypi\" /home/tbone/Documents/tonyfast @task_params ([ dict ( name = \"pypi\" , type = list , default = [ \"https://github.com/deathbeds/midgy\" , \"https://github.com/deathbeds/pidgy\" , \"https://github.com/deathbeds/importnb\" ])]) def task_lite ( pypi ): with __import__ ( \"importnb\" ) . imports ( \"ipynb\" ): from tonyfast.xxii.__lite_build import set_files_imports env = os . environ . copy () env [ \"SETUPTOOLS_SCM_PRETEND_VERSION\" ] = \"6.6.6\" hatch = list ( filter ( lambda x : x . startswith ( \"HATCH\" ), env )) for x in hatch : env . pop ( x ) wheels = [] for repo in pypi : org , _ , name = repo . rpartition ( \"/\" ) target = TARGET / name yield dict ( name = F \"clone: { repo } \" , actions = [ F \"git clone --depth 1 { repo } { target } \" ], targets = [ HEAD := ( target / \".git\" / \"HEAD\" )], clean = [ F \"rm -rf { target } \" ], uptodate = [ target . exists ()] ) pypi = Path ( \"pypi\" ) . absolute () # this directory needs to relative to build directory wheel = pypi / F \" { name } -6.6.6-py3-none-any.whl\" wheels . append ( wheel ) yield dict ( name = F \"build: { name } \" , actions = [ ( doit . tools . create_folder , [ pypi ]), doit . tools . CmdAction ( F \"hatch -v build -t wheel { pypi } \" , cwd = target , env = env ) ], clean = True , targets = [ wheel ], uptodate = [ False ] ) yield dict ( name = \"build\" , file_dep = wheels , actions = [ \"rm -rf tonyfast/pidgy/lite tonyfast/pidgy/.binder\" , \"jupyter lite build --contents tonyfast --output-dir site/run\" , ( set_files_imports , ( Path ( \"site/run/files\" ),)) ], targets = [ \"site/run/index.html\" ], clean = [ \"rm -rf site/run\" ] ) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"including development builds in jupyter lite"},{"location":"xxiii/2023-01-24-system-dependency-graphs.html","text":"extracting a dependency graph from importlib_metadata \u00a4 %reload_ext pidgy import importlib_metadata, pandas, networkx from toolz.curried import * @functools.lru_cache # cache this because the result will always be the same and parsing can be costly def get_tidy_dist() -> pandas.DataFrame: get_tidy_dist creates a tidy dataframe of the required distributions in this environment return get_dists().loc[\"Requires-Dist\"].apply( compose_left(pkg_resources.parse_requirements, first, vars, pandas.Series) ) def get_dists(): get_dists iterates through the importlib_metadata.distributions extracting the known metadata. return pandas.Series( dict((x.name, x.metadata._headers) for x in importlib_metadata.distributions()) ).rename_axis(index=[\"project\"]).explode().apply( pandas.Series, index=[\"key\", \"value\"] ).set_index(\"key\", append=True).reorder_levels((1, 0), 0)[\"value\"] applying the functionss \u00a4 generate the pandas.DataFrame of the dependency graph and metadata the metadata associated with my known python dependecies name url extras specifier marker unsafe_name project_name key specs hashCmp _Requirement__hash project retrolab jupyterlab None () ~=3.3.0 None jupyterlab jupyterlab jupyterlab [('~=', '3.3.0')] ('jupyterlab', None, , frozenset(), None) -7176474069730581504 retrolab jupyterlab-server None () ~=2.3 None jupyterlab-server jupyterlab-server jupyterlab-server [('~=', '2.3')] ('jupyterlab-server', None, , frozenset(), None) 4027949953799964160 retrolab jupyter-server None () ~=1.4 None jupyter-server jupyter-server jupyter-server [('~=', '1.4')] ('jupyter-server', None, , frozenset(), None) -3631246956991118336 retrolab nbclassic None () ~=0.2 None nbclassic nbclassic nbclassic [('~=', '0.2')] ('nbclassic', None, , frozenset(), None) 6600834436348797952 retrolab tornado None () >=6.1.0 None tornado tornado tornado [('>=', '6.1.0')] ('tornado', None, =6.1.0')>, frozenset(), None) 8619901927024904192 (df := get_tidy_dist()).head().style.set_caption(\"the metadata associated with my known python dependecies\") cast the tidy data as a networkx graph G =df.reset_index().pipe(networkx.from_pandas_edgelist, source=\"project\", target=\"name\") a table counting the frequency of specific distributions. pytest sphinx pytest-cov matplotlib flake8 numpy coverage requests pre-commit typing-extensions pandas importlib-metadata ipython six packaging black pyyaml jinja2 click ipywidgets count 147 78 70 55 51 48 45 41 36 34 33 32 31 30 30 29 28 27 25 25 df.name.value_counts().to_frame(\"count\").head(20).T.style.set_caption(\"a table counting the frequency of specific distributions.\") draw the graph n matplotlib matplotlib.pyplot.gcf().set_size_inches((20, 20)) networkx.draw_networkx(G) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"extracting a dependency graph from <pre>importlib_metadata</pre>"},{"location":"xxiii/2023-01-24-system-dependency-graphs.html#extracting-a-dependency-graph-from-importlib_metadata","text":"%reload_ext pidgy import importlib_metadata, pandas, networkx from toolz.curried import * @functools.lru_cache # cache this because the result will always be the same and parsing can be costly def get_tidy_dist() -> pandas.DataFrame: get_tidy_dist creates a tidy dataframe of the required distributions in this environment return get_dists().loc[\"Requires-Dist\"].apply( compose_left(pkg_resources.parse_requirements, first, vars, pandas.Series) ) def get_dists(): get_dists iterates through the importlib_metadata.distributions extracting the known metadata. return pandas.Series( dict((x.name, x.metadata._headers) for x in importlib_metadata.distributions()) ).rename_axis(index=[\"project\"]).explode().apply( pandas.Series, index=[\"key\", \"value\"] ).set_index(\"key\", append=True).reorder_levels((1, 0), 0)[\"value\"]","title":"extracting a dependency graph from importlib_metadata"},{"location":"xxiii/2023-01-24-system-dependency-graphs.html#applying-the-functionss","text":"generate the pandas.DataFrame of the dependency graph and metadata the metadata associated with my known python dependecies name url extras specifier marker unsafe_name project_name key specs hashCmp _Requirement__hash project retrolab jupyterlab None () ~=3.3.0 None jupyterlab jupyterlab jupyterlab [('~=', '3.3.0')] ('jupyterlab', None, , frozenset(), None) -7176474069730581504 retrolab jupyterlab-server None () ~=2.3 None jupyterlab-server jupyterlab-server jupyterlab-server [('~=', '2.3')] ('jupyterlab-server', None, , frozenset(), None) 4027949953799964160 retrolab jupyter-server None () ~=1.4 None jupyter-server jupyter-server jupyter-server [('~=', '1.4')] ('jupyter-server', None, , frozenset(), None) -3631246956991118336 retrolab nbclassic None () ~=0.2 None nbclassic nbclassic nbclassic [('~=', '0.2')] ('nbclassic', None, , frozenset(), None) 6600834436348797952 retrolab tornado None () >=6.1.0 None tornado tornado tornado [('>=', '6.1.0')] ('tornado', None, =6.1.0')>, frozenset(), None) 8619901927024904192 (df := get_tidy_dist()).head().style.set_caption(\"the metadata associated with my known python dependecies\") cast the tidy data as a networkx graph G =df.reset_index().pipe(networkx.from_pandas_edgelist, source=\"project\", target=\"name\") a table counting the frequency of specific distributions. pytest sphinx pytest-cov matplotlib flake8 numpy coverage requests pre-commit typing-extensions pandas importlib-metadata ipython six packaging black pyyaml jinja2 click ipywidgets count 147 78 70 55 51 48 45 41 36 34 33 32 31 30 30 29 28 27 25 25 df.name.value_counts().to_frame(\"count\").head(20).T.style.set_caption(\"a table counting the frequency of specific distributions.\") draw the graph n matplotlib matplotlib.pyplot.gcf().set_size_inches((20, 20)) networkx.draw_networkx(G) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"applying the functionss"},{"location":"xxiii/2023-01-24-whoosh-search.html","text":"use whoosh to search cells/articles on disk \u00a4 https://whoosh.readthedocs.io/en/latest/ !pip install whoosh import whoosh.fields , whoosh.index , whoosh.qparser , whoosh.writing import pathlib , shutil from tonyfast import nbframe __import__ ( \"nest_asyncio\" ) . apply () self = nbframe . Documents ( nbframe . Finder ( dir = \"..\" )) . load () initialize the search index \u00a4 INDEX = pathlib . Path ( \"search_index\" ) INDEX . mkdir ( exist_ok = True ) whoosh . index . create_in ( INDEX , schema := whoosh . fields . Schema ( source = whoosh . fields . TEXT , path = whoosh . fields . ID ( stored = True ))) index = whoosh . index . open_dir ( INDEX ) from tonyfast import nbframe self = nbframe . Documents ( nbframe . Finder ( dir = \"..\" )) . load () self.articles is a dataframe containing notebooks and files cast to the notebook schema. the dask and dataframes are shown below. display ( self . articles , self . articles . head ( 10 , 5 )) Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cell_type execution_count id metadata outputs source cell_ct attachments npartitions=85 ../2023-01-19-.ipynb object int64 object object object object int64 object ../2023-01-19-pidgy-afforndances.ipynb ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ../xxiii/vendor/tree-sitter-python/test/highlight/pattern_matching.py ... ... ... ... ... ... ... ... ../xxiii/what.md ... ... ... ... ... ... ... ... Dask Name: apply, 1 graph layer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cell_type execution_count id metadata outputs source cell_ct attachments path ../2023-01-19-.ipynb code None ad5f3630-daac-4b8b-95b0-22f27ea47af2 {} [] 0 None ../2023-01-19-pidgy-afforndances.ipynb code 1.0 409a2348-866f-4127-a25b-7fc0adcac5fc {} [{'ename': 'SyntaxError', 'evalue': 'invalid s... when i program in `pidgy`\\n\\n* `sys.modules` a... 0 None ../2023-01-19-pidgy-afforndances.ipynb code 1.0 45c4aa5d-e53d-4a02-82b2-8f872906ceba {} [{'data': {'text/markdown': ' %reload_ext p... %reload_ext pidgy\\n from toolz.curried ... 1 None ../2023-01-19-pidgy-afforndances.ipynb markdown NaN 87b3dd7c-3b9a-406d-b39c-547c69a938f7 {} None <iframe src=\"http://127.0.0.1:8787/status\"... 2 None ../2023-01-19-pidgy-afforndances.ipynb markdown NaN d3d234cd-7d46-44b2-b26f-a80e524764ec {} None # start the contents finder 3 None ../2023-01-19-pidgy-afforndances.ipynb code 1.0 3563e7c1-6f4f-4774-84c9-f2db766238cb {} [{'data': {'text/html': '<div> <div style=... \\n %reload_ext pidgy\\n import nbfram... 4 None ../2023-01-19-pidgy-afforndances.ipynb code 11.0 4b4f8e4c-d192-4dd8-818e-b728d4b6673e {} [{'data': {'text/html': '<div> <style scoped> ... result 5 None ../2023-01-19-pidgy-afforndances.ipynb code 21.0 1fba636a-14c1-4011-8af1-5d54452f7e36 {} [{'data': {'text/markdown': ' pretty neat that... {{asyncio.sleep(1) or \"\"}} pretty neat that we... 6 None ../2023-01-19-pidgy-afforndances.ipynb code 18.0 efdfc300-da33-40d5-b1ea-8636e8e1a001 {} [{'data': {'text/markdown': ' docs= 2', 'te... docs= 2 7 None ../2023-01-19-pidgy-afforndances.ipynb markdown NaN bad77dd2-294b-4f6a-a520-4ff25be27d41 {} None load and persist the data 8 None def get_article_path ( s ): return str ( s . name ) + \"#/cells/\" + str ( s . cell_ct ) self . articles [ \"path\" ] = self . articles . apply ( get_article_path , meta = ( \"path\" , \"O\" ), axis = 1 ) def write_documents ( df ): with whoosh . writing . AsyncWriter ( index ) as w : for _ , x in df . iterrows (): w . add_document ( ** x ) self . articles [[ \"source\" , \"path\" ]] . applymap ( \"\" . join ) . groupby ( self . articles . index ) . apply ( write_documents , meta = ( \"none\" , int )) . compute () querying the documents \u00a4 query = whoosh . qparser . QueryParser ( \"source\" , schema ) with index . searcher () as search : print ( search . search ( query . parse ( \"literate computing\" ))) <Top 0 Results for And([Term('source', 'literate'), Term('source', 'computing')]) runtime=9.207999937643763e-05> {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"use <pre>whoosh</pre> to search cells/articles on disk"},{"location":"xxiii/2023-01-24-whoosh-search.html#use-whoosh-to-search-cellsarticles-on-disk","text":"https://whoosh.readthedocs.io/en/latest/ !pip install whoosh import whoosh.fields , whoosh.index , whoosh.qparser , whoosh.writing import pathlib , shutil from tonyfast import nbframe __import__ ( \"nest_asyncio\" ) . apply () self = nbframe . Documents ( nbframe . Finder ( dir = \"..\" )) . load ()","title":"use whoosh to search cells/articles on disk"},{"location":"xxiii/2023-01-24-whoosh-search.html#initialize-the-search-index","text":"INDEX = pathlib . Path ( \"search_index\" ) INDEX . mkdir ( exist_ok = True ) whoosh . index . create_in ( INDEX , schema := whoosh . fields . Schema ( source = whoosh . fields . TEXT , path = whoosh . fields . ID ( stored = True ))) index = whoosh . index . open_dir ( INDEX ) from tonyfast import nbframe self = nbframe . Documents ( nbframe . Finder ( dir = \"..\" )) . load () self.articles is a dataframe containing notebooks and files cast to the notebook schema. the dask and dataframes are shown below. display ( self . articles , self . articles . head ( 10 , 5 )) Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cell_type execution_count id metadata outputs source cell_ct attachments npartitions=85 ../2023-01-19-.ipynb object int64 object object object object int64 object ../2023-01-19-pidgy-afforndances.ipynb ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ../xxiii/vendor/tree-sitter-python/test/highlight/pattern_matching.py ... ... ... ... ... ... ... ... ../xxiii/what.md ... ... ... ... ... ... ... ... Dask Name: apply, 1 graph layer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cell_type execution_count id metadata outputs source cell_ct attachments path ../2023-01-19-.ipynb code None ad5f3630-daac-4b8b-95b0-22f27ea47af2 {} [] 0 None ../2023-01-19-pidgy-afforndances.ipynb code 1.0 409a2348-866f-4127-a25b-7fc0adcac5fc {} [{'ename': 'SyntaxError', 'evalue': 'invalid s... when i program in `pidgy`\\n\\n* `sys.modules` a... 0 None ../2023-01-19-pidgy-afforndances.ipynb code 1.0 45c4aa5d-e53d-4a02-82b2-8f872906ceba {} [{'data': {'text/markdown': ' %reload_ext p... %reload_ext pidgy\\n from toolz.curried ... 1 None ../2023-01-19-pidgy-afforndances.ipynb markdown NaN 87b3dd7c-3b9a-406d-b39c-547c69a938f7 {} None <iframe src=\"http://127.0.0.1:8787/status\"... 2 None ../2023-01-19-pidgy-afforndances.ipynb markdown NaN d3d234cd-7d46-44b2-b26f-a80e524764ec {} None # start the contents finder 3 None ../2023-01-19-pidgy-afforndances.ipynb code 1.0 3563e7c1-6f4f-4774-84c9-f2db766238cb {} [{'data': {'text/html': '<div> <div style=... \\n %reload_ext pidgy\\n import nbfram... 4 None ../2023-01-19-pidgy-afforndances.ipynb code 11.0 4b4f8e4c-d192-4dd8-818e-b728d4b6673e {} [{'data': {'text/html': '<div> <style scoped> ... result 5 None ../2023-01-19-pidgy-afforndances.ipynb code 21.0 1fba636a-14c1-4011-8af1-5d54452f7e36 {} [{'data': {'text/markdown': ' pretty neat that... {{asyncio.sleep(1) or \"\"}} pretty neat that we... 6 None ../2023-01-19-pidgy-afforndances.ipynb code 18.0 efdfc300-da33-40d5-b1ea-8636e8e1a001 {} [{'data': {'text/markdown': ' docs= 2', 'te... docs= 2 7 None ../2023-01-19-pidgy-afforndances.ipynb markdown NaN bad77dd2-294b-4f6a-a520-4ff25be27d41 {} None load and persist the data 8 None def get_article_path ( s ): return str ( s . name ) + \"#/cells/\" + str ( s . cell_ct ) self . articles [ \"path\" ] = self . articles . apply ( get_article_path , meta = ( \"path\" , \"O\" ), axis = 1 ) def write_documents ( df ): with whoosh . writing . AsyncWriter ( index ) as w : for _ , x in df . iterrows (): w . add_document ( ** x ) self . articles [[ \"source\" , \"path\" ]] . applymap ( \"\" . join ) . groupby ( self . articles . index ) . apply ( write_documents , meta = ( \"none\" , int )) . compute ()","title":"initialize the search index"},{"location":"xxiii/2023-01-24-whoosh-search.html#querying-the-documents","text":"query = whoosh . qparser . QueryParser ( \"source\" , schema ) with index . searcher () as search : print ( search . search ( query . parse ( \"literate computing\" ))) <Top 0 Results for And([Term('source', 'literate'), Term('source', 'computing')]) runtime=9.207999937643763e-05> {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"querying the documents"},{"location":"xxiii/2023-02-19-semantic.html","text":"semantic notebook structure \u00a4 sketches of semantic notebook structures considering cells as form inputs. %reload_ext pidgy def jade(body): lines = body.splitlines(True) if body.startswith(\"```\"): lines.pop(0) if body.rstrip().endswith(\"```\"): lines.pop() return __import__(\"pyjade\").process(\"\".join(lines)) shape =\\ jade main head section(role=\"feed\") article(aria-posinset=1, aria-setsize=12) form(id, aria) fieldset(name=cell) legend input label(for=\"cell-input\") div.source(id=\"cell-input\", contenteditable=\"false\", role=\"textbox\"). some code that we will compute with fieldset(name=\"commands\", form) legend commands button \u23ea button \u23f9\ufe0f button \u25b6\ufe0f button \u23e9 fieldset(name=\"metadata\", form) legend information label in input(type=\"number\", disabled=\"\", value=2) label start input(type=\"time\", disabled=\"\", value=\"14:32\") label stop input(type=\"time\", disabled=\"\", value=\"14:33\") fieldset(name=\"tags\") legend tags label slide fieldset(name=\"outputs\", form) legend outputs fieldset.stderr label \u26a0 output.warning samp warning fieldset.stderr label \u2718 output.error samp error fieldset.stdout label \u2611 output.stdout samp \"hello world\" fieldset.display-data fieldset(name=\"metadata\") label out input(type=\"number\", disabled=\"\", value=2) output(markdown=\"true\"). # some markdown to remember ... < main > < head ></ head > < section role = \"feed\" > < article aria-posinset = \"1\" aria-setsize = \"12\" > < form id = \"id\" aria = \"aria\" > < fieldset > < legend > input </ legend > < label for = \"cell-input\" ></ label > < div id = \"cell-input\" contenteditable = \"false\" role = \"textbox\" class = \"source\" > some code that we will compute with </ div > < fieldset name = \"commands\" form = \"form\" > < legend > commands </ legend > < button > \u23ea </ button > < button > \u23f9\ufe0f </ button > < button > \u25b6\ufe0f </ button > < button > \u23e9 </ button > </ fieldset > < fieldset name = \"metadata\" form = \"form\" > < legend > information </ legend > < label > in </ label > < input type = \"number\" disabled = \"\" value = \"2\" /> < label > start </ label > < input type = \"time\" disabled = \"\" value = \"14:32\" /> < label > stop </ label > < input type = \"time\" disabled = \"\" value = \"14:33\" /> < fieldset name = \"tags\" > < legend > tags </ legend > < label > slide </ label > </ fieldset > </ fieldset > < fieldset name = \"outputs\" form = \"form\" > < legend > outputs </ legend > < fieldset class = \"stderr\" > < label > \u26a0 </ label > < output class = \"warning\" >< samp > warning </ samp > </ output > </ fieldset > < fieldset class = \"stderr\" > < label > \u2718 </ label > < output class = \"error\" >< samp > error </ samp > </ output > </ fieldset > < fieldset class = \"stdout\" > < label > \u2611 </ label > < output class = \"stdout\" >< samp > \"hello world\" </ samp > </ output > </ fieldset > < fieldset class = \"display-data\" > < fieldset name = \"metadata\" > < label > out </ label > < input type = \"number\" disabled = \"\" value = \"2\" /> </ fieldset > < output markdown = \"true\" > # some markdown to remember </ output > </ fieldset > </ fieldset > </ fieldset > </ form > </ article > </ section > </ main > input some code that we will compute with commands \u23ea \u23f9\ufe0f \u25b6\ufe0f \u23e9 information in start stop tags slide outputs \u26a0 warning \u2718 error \u2611 \"hello world\" out # some markdown to remember {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"semantic notebook structure"},{"location":"xxiii/2023-02-19-semantic.html#semantic-notebook-structure","text":"sketches of semantic notebook structures considering cells as form inputs. %reload_ext pidgy def jade(body): lines = body.splitlines(True) if body.startswith(\"```\"): lines.pop(0) if body.rstrip().endswith(\"```\"): lines.pop() return __import__(\"pyjade\").process(\"\".join(lines)) shape =\\ jade main head section(role=\"feed\") article(aria-posinset=1, aria-setsize=12) form(id, aria) fieldset(name=cell) legend input label(for=\"cell-input\") div.source(id=\"cell-input\", contenteditable=\"false\", role=\"textbox\"). some code that we will compute with fieldset(name=\"commands\", form) legend commands button \u23ea button \u23f9\ufe0f button \u25b6\ufe0f button \u23e9 fieldset(name=\"metadata\", form) legend information label in input(type=\"number\", disabled=\"\", value=2) label start input(type=\"time\", disabled=\"\", value=\"14:32\") label stop input(type=\"time\", disabled=\"\", value=\"14:33\") fieldset(name=\"tags\") legend tags label slide fieldset(name=\"outputs\", form) legend outputs fieldset.stderr label \u26a0 output.warning samp warning fieldset.stderr label \u2718 output.error samp error fieldset.stdout label \u2611 output.stdout samp \"hello world\" fieldset.display-data fieldset(name=\"metadata\") label out input(type=\"number\", disabled=\"\", value=2) output(markdown=\"true\"). # some markdown to remember ... < main > < head ></ head > < section role = \"feed\" > < article aria-posinset = \"1\" aria-setsize = \"12\" > < form id = \"id\" aria = \"aria\" > < fieldset > < legend > input </ legend > < label for = \"cell-input\" ></ label > < div id = \"cell-input\" contenteditable = \"false\" role = \"textbox\" class = \"source\" > some code that we will compute with </ div > < fieldset name = \"commands\" form = \"form\" > < legend > commands </ legend > < button > \u23ea </ button > < button > \u23f9\ufe0f </ button > < button > \u25b6\ufe0f </ button > < button > \u23e9 </ button > </ fieldset > < fieldset name = \"metadata\" form = \"form\" > < legend > information </ legend > < label > in </ label > < input type = \"number\" disabled = \"\" value = \"2\" /> < label > start </ label > < input type = \"time\" disabled = \"\" value = \"14:32\" /> < label > stop </ label > < input type = \"time\" disabled = \"\" value = \"14:33\" /> < fieldset name = \"tags\" > < legend > tags </ legend > < label > slide </ label > </ fieldset > </ fieldset > < fieldset name = \"outputs\" form = \"form\" > < legend > outputs </ legend > < fieldset class = \"stderr\" > < label > \u26a0 </ label > < output class = \"warning\" >< samp > warning </ samp > </ output > </ fieldset > < fieldset class = \"stderr\" > < label > \u2718 </ label > < output class = \"error\" >< samp > error </ samp > </ output > </ fieldset > < fieldset class = \"stdout\" > < label > \u2611 </ label > < output class = \"stdout\" >< samp > \"hello world\" </ samp > </ output > </ fieldset > < fieldset class = \"display-data\" > < fieldset name = \"metadata\" > < label > out </ label > < input type = \"number\" disabled = \"\" value = \"2\" /> </ fieldset > < output markdown = \"true\" > # some markdown to remember </ output > </ fieldset > </ fieldset > </ fieldset > </ form > </ article > </ section > </ main > input some code that we will compute with commands \u23ea \u23f9\ufe0f \u25b6\ufe0f \u23e9 information in start stop tags slide outputs \u26a0 warning \u2718 error \u2611 \"hello world\" out # some markdown to remember {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"semantic notebook structure"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html","text":"a sprint through history toward computional notebooks \u00a4 history helps us understand science and software. computational notebooks interfaces are the outcome of years of computing history. we'll follow history to better understand what shaped the technology millions use today. \ud83c\udfc3 run all \ud83d\uddb9 document mode \ud83d\udcfd presentation mode \u229f collapse all the code \u229e expand all the code jupyterlab-deck voila \ud83d\udd0d inspector \ud83d\udc6f side-by-side restart and run all or it didn't happen if __import__ ( \"sys\" ) . platform == \"emscripten\" : % pip install pidgy % reload_ext pidgy if __import__(\"sys\").platform == \"emscripten\": %pip install pidgy %reload_ext pidgy computational notebooks and the \ud83c\udfc6 award winning Project Jupyter \u00a4 Free software, open standards, and web services for interactive computing across all programming languages. A tagline from the official Project Jupyter home page indicating the values and reach of its technology. Jupyter is a critical software system for interactive computing that makes it possible read and write code in many languages. Most of we find Python and Markdown, but we are not limited to these languages. Follow me through the olds. ## Xerox Alto < figure markdown > < figcaption markdown > < blockquote cite = \"https://en.wikipedia.org/wiki/Xerox_Alto\" > The Xerox Alto is a computer designed from its inception to support an operating system based on a graphical user interface ( GUI ), later using the desktop metaphor . The first machines were introduced on 1 March 1973 , a decade before mass - market GUI machines became available . </ blockquote > ! [ xero alto ]( https : // upload . wikimedia . org / wikipedia / commons / thumb / 5 / 5 e / Xerox_Alto_mit_Rechner . JPG / 480 px - Xerox_Alto_mit_Rechner . JPG \"A Xerox Alto cabinet computer\" ) </ figcaption > </ figure > [ alto ]: https : // en . wikipedia . org / wiki / Xerox_Alto Xerox Alto \u00a4 The Xerox Alto is a computer designed from its inception to support an operating system based on a graphical user interface (GUI), later using the desktop metaphor. The first machines were introduced on 1 March 1973, a decade before mass-market GUI machines became available. The computer mouse and interactive computing \u00a4 at the dawn of post modernity, Doug Engelbart's X-Y Position Indicator for a Display System changed the way we interact with computers. ## [the mother of all demos] < figure markdown > < figcaption markdown > Doug Englebart 's Mother of all Demos presentation that changed computing forever. </ figcaption > < iframe width = \"629\" height = \"472\" src = \"https://www.youtube.com/embed/yJDv-zdhzMY\" title = \"The Mother of All Demos, presented by Douglas Engelbart (1968)\" frameborder = \"0\" allow = \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen ></ iframe > </ figure > > The live demonstration featured the introduction of a complete computer hardware and software system called the oN - Line System or , more commonly , NLS . The 90 - minute presentation demonstrated for the first time many of the fundamental elements of modern personal computing : windows , hypertext , graphics , efficient navigation and command input , video conferencing , the computer mouse , word processing , dynamic file linking , revision control , and a collaborative real - time editor . Engelbart 's presentation was the first to publicly demonstrate all of these elements in a single system. The demonstration was highly influential and spawned similar projects at Xerox PARC in the early 1970s. The underlying concepts and technologies influenced both the Apple Macintosh and Microsoft Windows graphical user interface operating systems in the 1980s and 1990s. [ the mother of all demos ]: https : // en . wikipedia . org / wiki / The_Mother_of_All_Demos \"the wiki page for the mother of all demos\" [ moad transcript ]: https : // github . com / atduskgreg / MoAD - Transcript \"a repository hosting the mother of all demos transcripts\" the mother of all demos \u00a4 Doug Englebart's Mother of all Demos presentation that changed computing forever. The live demonstration featured the introduction of a complete computer hardware and software system called the oN-Line System or, more commonly, NLS. The 90-minute presentation demonstrated for the first time many of the fundamental elements of modern personal computing: windows, hypertext, graphics, efficient navigation and command input, video conferencing, the computer mouse, word processing, dynamic file linking, revision control, and a collaborative real-time editor. Engelbart's presentation was the first to publicly demonstrate all of these elements in a single system. The demonstration was highly influential and spawned similar projects at Xerox PARC in the early 1970s. The underlying concepts and technologies influenced both the Apple Macintosh and Microsoft Windows graphical user interface operating systems in the 1980s and 1990s. 50 Years Later, We\u2019re Still Living in the Xerox Alto\u2019s World \u00a4 the impact of the Xerox Alto still ripples through the world as we're still build the systems in the mother of all demos. the Alto was a glimpse into the future of personal computing. ## Mathematica's computational essays < figure markdown > < figcaption markdown > mathematica is a successful closed source interface for mathematical computing using the proprietary wolfram language . mathematica author 's have been experimenting with notebook interfaces the designers of the interfaces are part of apple 's early history. relationship to applye </ figcaption > ! [ mathematica version 2 ]( https : // winworldpc . com / res / img / screenshots / Mathematica % 202.1 % 20 -% 20 About . png \"a screenshot of mathematica version 2\" ) </ figure > Mathematica's computational essays \u00a4 mathematica is a successful closed source interface for mathematical computing using the proprietary wolfram language. mathematica author's have been experimenting with notebook interfaces the designers of the interfaces are part of apple's early history. relationship to applye ### [what is a computational essay] < figure markdown > < figcaption markdown > < blockquote > There are basically three kinds of things here . First , ordinary text ( here in English ) . Second , computer input . And third , computer output . And the crucial point is that these all work together to express what \u2019 s being communicated .</ blockquote > </ figcaption > ! []( https : // content . wolfram . com / uploads / sites / 43 / 2017 / 11 / InOutImg1 . png ) </ figure > [ what is a computational essay ]: https : // writings . stephenwolfram . com / 2017 / 11 / what - is - a - computational - essay / < details markdown > < summary markdown > wolfram 's throughts on jupyter: an excerpt from [The Scientific Paper is Obsolete] </ summary > > \u201c I see these Jupyter guys , \u201d Wolfram said to me , \u201c they are about on a par with what we had in the early 1990 s . \u201d They \u2019 ve taken shortcuts , he said . \u201c We actually want to try and do it right . \u201d </ details > [ The Scientific Paper is Obsolete ]: https : // www . theatlantic . com / science / archive / 2018 / 04 / the - scientific - paper - is - obsolete / 556676 / what is a computational essay \u00a4 There are basically three kinds of things here. First, ordinary text (here in English). Second, computer input. And third, computer output. And the crucial point is that these all work together to express what\u2019s being communicated. wolfram's throughts on jupyter: an excerpt from The Scientific Paper is Obsolete \u201cI see these Jupyter guys,\u201d Wolfram said to me, \u201cthey are about on a par with what we had in the early 1990s.\u201d They\u2019ve taken shortcuts, he said. \u201cWe actually want to try and do it right.\u201d computer programning for everybody \u00a4 In the seventies, Xerox PARC asked: \"Can we have a computer on every desk?\" We now know this is possible, but those computers haven't necessarily empowered their users. Today's computers are often inflexible: the average computer user can typically only change a limited set of options configurable via a \"wizard\" (a lofty word for a canned dialog), and is dependent on expert programmers for everything else. We compare mass ability to read and write software with mass literacy, and predict equally pervasive changes to society. Hardware is now sufficiently fast and cheap to make mass computer education possible: the next big change will happen when most computer users have the knowledge and power to create and modify software. Interactive Python \u00a4 IPython grew to have a lot of features in a monolith an interactive shell a REPL protocol a notebook document fromat a notebook document conversion tool a web-based notebook authoring tool tools for building interactive UI (widgets) interactive parallel Python based on the above REPL protocol 2015 Project Jupyter The Big Split \u00a4 Jupyter is like IPython, but language agnostic ## 2017 ACM Software System Award < figure markdown > < figcaption > jupyter is recognized as a critical software software . </ figcaption > < blockquote cite = \"https://awards.acm.org/software-system\" > Jupyter has also gained wide industry adoption . Since 2015 , Jupyter - based products have been released by several companies including Google ( Cloud DataLab ), Microsoft ( AzureML , HDInsight ), Intel ( Trusted Analytics Platform ), and IBM ( IBM Watson Studio ) . Bloomberg and Anaconda Inc . have partnered with Project Jupyter to develop the next - generation web interface , JupyterLab . </ blockquote > </ figure > < blockquote cite = \"https://blog.jupyter.org/jupyter-receives-the-acm-software-system-award-d433b0dfe3a2\" > Similarly , there exist multiple client applications in addition to the Jupyter Notebook and JupyterLab to create and execute notebooks , each with its own use case and focus : the open source nteract project develops a lightweight desktop application to run notebooks ; CoCalc , a startup founded by William Stein , the creator of SageMath , offers a web - based client with real - time collaboration that includes Jupyter alongside SageMath , LaTeX , and tools focused on education ; and Google now provides Colaboratory , another web notebook frontend that runs alongside the rest of the Google Documents suite , with execution in the Google Cloud . </ blockquote > 2017 ACM Software System Award \u00a4 jupyter is recognized as a critical software software. Jupyter has also gained wide industry adoption. Since 2015, Jupyter-based products have been released by several companies including Google (Cloud DataLab), Microsoft (AzureML, HDInsight), Intel (Trusted Analytics Platform), and IBM (IBM Watson Studio). Bloomberg and Anaconda Inc. have partnered with Project Jupyter to develop the next-generation web interface, JupyterLab. Similarly, there exist multiple client applications in addition to the Jupyter Notebook and JupyterLab to create and execute notebooks, each with its own use case and focus: the open source nteract project develops a lightweight desktop application to run notebooks; CoCalc, a startup founded by William Stein, the creator of SageMath, offers a web-based client with real-time collaboration that includes Jupyter alongside SageMath, LaTeX, and tools focused on education; and Google now provides Colaboratory, another web notebook frontend that runs alongside the rest of the Google Documents suite, with execution in the Google Cloud. ## jupyter is in good company < figure markdown > < figcaption markdown > Project Jupyter wins the 2017 ACM Software System Award , sharing a similar prestige as the Xerox Alto 30 years later . </ figcaption > < iframe src = \"https://en.m.wikipedia.org/wiki/ACM_Software_System_Award#Recipients\" height = \"600\" width = \"100%\" ></ iframe > </ figure > jupyter is in good company \u00a4 Project Jupyter wins the 2017 ACM Software System Award, sharing a similar prestige as the Xerox Alto 30 years later. ### there many notebook implementations and user interfaces studies of notebook and literate computing interfaces : < figure markdown > < figcaption markdown > the notebook is a style of interface , collections of input forms and outputs repeated to weave a narrative and tangle code . there are open source and closed source options , along with language specific and agnostic implementations . </ figcaption > < iframe src = \"https://en.m.wikipedia.org/wiki/Notebook_interface#Free/open-source_notebooks\" height = \"600\" width = \"100%\" ></ iframe > </ figure > there many notebook implementations and user interfaces \u00a4 the notebook is a style of interface, collections of input forms and outputs repeated to weave a narrative and tangle code. there are open source and closed source options, along with language specific and agnostic implementations. ## notebooks and cells are increasingly more common forms < figure markdown > < figcaption > Observable 's description of notebooks and cells.</figcaption> < blockquote cite = \"https://observablehq.com/@observablehq/notebooks-cells\" > Observable is a platform for exploring data and code , visually , live in your browser . And < b > the central component of that platform is what we call a \"notebook\" : an interactive , editable document defined by chunks of code called \"cells\" .</ b > Observable notebooks help you explore live data , prototype visualizations , make interactive art , understand algorithms , collaborate on reports , and much more . Join Anjana Vakil , Observable Developer Advocate , in diving into Observable and understanding the basic mechanics of working with notebooks & cells . </ blockquote > </ figure > < iframe width = \"560\" height = \"315\" src = \"https://www.youtube.com/embed/M1xMRkb89oM\" title = \"Observable: Notebooks &amp; Cells\" frameborder = \"0\" allow = \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen ></ iframe > notebooks and cells are increasingly more common forms \u00a4 Observable's description of notebooks and cells. Observable is a platform for exploring data and code, visually, live in your browser. And the central component of that platform is what we call a \"notebook\": an interactive, editable document defined by chunks of code called \"cells\". Observable notebooks help you explore live data, prototype visualizations, make interactive art, understand algorithms, collaborate on reports, and much more. Join Anjana Vakil, Observable Developer Advocate, in diving into Observable and understanding the basic mechanics of working with notebooks & cells. ## <abbr title=\"web content accessibility guidelines\">WCAG</abbr>: we circumvented accessibility guidelines < blockquote cite = \"https://en.wikipedia.org/wiki/Web_Content_Accessibility_Guidelines\" > The ** Web Content Accessibility Guidelines ( WCAG ) ** are part of a series of web accessibility guidelines published by the Web Accessibility Initiative ( WAI ) of the World Wide Web Consortium ( W3C ), the main international standards organization for the Internet . They are a set of recommendations for making Web content more accessible , primarily for people with disabilities \u2014 but also for all user agents , including highly limited devices , such as mobile phones . </ blockquote > < blockquote cite = \"https://www.boia.org/blog/history-of-the-web-content-accessibility-guidelines-wcag\" markdown > ### wcag drafts * May 5 , 1999 : WCAG 1.0 is born . It included 14 guidelines , ranging from the need to provide text equivalents to considering clarity and simplicity on the web . Each guideline had between one and 10 supporting checkpoints . * December 11 , 2008 : WCAG 2.0 broadens scope and offers the four principles . The early 2000 s were years of unbelievable changes in technology , so WCAG evolved to keep up . WCAG 2.0 was an incredible follow - up to its predecessor and was intended to be applied to almost all things digital ( including documents and apps ) . WCAG 2.0 also introduced the four guiding principles of accessibility , stating content must be perceivable , operable , understandable , and robust , supported by success criteria for meeting those principles . WCAG 2.0 reigned as the gold standard for a long time . * June 5 , 2018 : WCAG 2.1 builds on but does not replace WCAG 2.0 . The latest version of WCAG is backwards - compatible with the previous , which means if you comply with WCAG 2.1 , you automatically comply with WCAG 2.0 \u2014 great news for website and app creators . WCAG 2.1 came as a highly - welcomed update , after the decade - old WCAG 2.0 could no longer completely account for advancements in technology and web use . The new WCAG 2.1 standards include several success criteria for improving web accessibility on mobile devices , as well as for people with low vision and cognitive disabilities . </ blockquote > WCAG : we circumvented accessibility guidelines \u00a4 The **Web Content Accessibility Guidelines (WCAG)** are part of a series of web accessibility guidelines published by the Web Accessibility Initiative (WAI) of the World Wide Web Consortium (W3C), the main international standards organization for the Internet. They are a set of recommendations for making Web content more accessible, primarily for people with disabilities\u2014but also for all user agents, including highly limited devices, such as mobile phones. wcag drafts \u00a4 May 5, 1999: WCAG 1.0 is born. It included 14 guidelines, ranging from the need to provide text equivalents to considering clarity and simplicity on the web. Each guideline had between one and 10 supporting checkpoints. December 11, 2008: WCAG 2.0 broadens scope and offers the four principles. The early 2000s were years of unbelievable changes in technology, so WCAG evolved to keep up. WCAG 2.0 was an incredible follow-up to its predecessor and was intended to be applied to almost all things digital (including documents and apps). WCAG 2.0 also introduced the four guiding principles of accessibility, stating content must be perceivable, operable, understandable, and robust, supported by success criteria for meeting those principles. WCAG 2.0 reigned as the gold standard for a long time. June 5, 2018: WCAG 2.1 builds on but does not replace WCAG 2.0. The latest version of WCAG is backwards-compatible with the previous, which means if you comply with WCAG 2.1, you automatically comply with WCAG 2.0 \u2014 great news for website and app creators. WCAG 2.1 came as a highly-welcomed update, after the decade-old WCAG 2.0 could no longer completely account for advancements in technology and web use. The new WCAG 2.1 standards include several success criteria for improving web accessibility on mobile devices, as well as for people with low vision and cognitive disabilities. accessible notebook interfaces \u00a4 recently, Jupyter developers were able to remove accessibility violations caught be axe. read the blog post . ### automated testing < figure markdown > < figcaption markdown > [ axe core ][ axe ] 's is the open source industry standard for accessibility testing. caveat emptor ... </ figcaption > > With [ axe - core ][ axe ], you can find ** on average 57 % of WCAG issues automatically **. Additionally , axe - core will return elements as \"incomplete\" where axe - core could not be certain , and manual review is needed . </ figure > [ axe ]: https : // github . com / dequelabs / axe - core #### using axe * https : // github . com / dequelabs / axe - core * [ firefox axe extension ] * [ chrome axe extension ] [ firefox axe extension ]: https : // addons . mozilla . org / en - US / firefox / addon / axe - devtools / [ chrome axe extension ]: https : // chrome . google . com / webstore / detail / axe - devtools - web - accessib / lhdoppojpmngadmnindnejefpokejbdd automated testing \u00a4 axe core 's is the open source industry standard for accessibility testing. caveat emptor... With axe-core , you can find on average 57% of WCAG issues automatically . Additionally, axe-core will return elements as \"incomplete\" where axe-core could not be certain, and manual review is needed. using axe \u00a4 https://github.com/dequelabs/axe-core firefox axe extension chrome axe extension manual testing \u00a4 Nothing about us without us pull requests to the tests a primary function of this project has been testing notebooks with screen reader users to design a better experience. ## we want to design notebook interfaces that empower assistive technology users things we can control things we cant control we need you help ! venn diagram of notebook accessibility and output accessibility . write accessibile notebooks . * advocacy , action * join our meetings . we want to design notebook interfaces that empower assistive technology users \u00a4 things we can control things we cant control we need you help! venn diagram of notebook accessibility and output accessibility. write accessibile notebooks. advocacy, action join our meetings. < style > article > . highlight { display : none ; } </ style > article > .highlight { display: none; } {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"a sprint through history toward computional notebooks"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#a-sprint-through-history-toward-computional-notebooks","text":"history helps us understand science and software. computational notebooks interfaces are the outcome of years of computing history. we'll follow history to better understand what shaped the technology millions use today. \ud83c\udfc3 run all \ud83d\uddb9 document mode \ud83d\udcfd presentation mode \u229f collapse all the code \u229e expand all the code jupyterlab-deck voila \ud83d\udd0d inspector \ud83d\udc6f side-by-side restart and run all or it didn't happen if __import__ ( \"sys\" ) . platform == \"emscripten\" : % pip install pidgy % reload_ext pidgy if __import__(\"sys\").platform == \"emscripten\": %pip install pidgy %reload_ext pidgy","title":"a sprint through history toward computional notebooks"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#computational-notebooks-and-the-award-winning-project-jupyter","text":"Free software, open standards, and web services for interactive computing across all programming languages. A tagline from the official Project Jupyter home page indicating the values and reach of its technology. Jupyter is a critical software system for interactive computing that makes it possible read and write code in many languages. Most of we find Python and Markdown, but we are not limited to these languages. Follow me through the olds. ## Xerox Alto < figure markdown > < figcaption markdown > < blockquote cite = \"https://en.wikipedia.org/wiki/Xerox_Alto\" > The Xerox Alto is a computer designed from its inception to support an operating system based on a graphical user interface ( GUI ), later using the desktop metaphor . The first machines were introduced on 1 March 1973 , a decade before mass - market GUI machines became available . </ blockquote > ! [ xero alto ]( https : // upload . wikimedia . org / wikipedia / commons / thumb / 5 / 5 e / Xerox_Alto_mit_Rechner . JPG / 480 px - Xerox_Alto_mit_Rechner . JPG \"A Xerox Alto cabinet computer\" ) </ figcaption > </ figure > [ alto ]: https : // en . wikipedia . org / wiki / Xerox_Alto","title":"computational notebooks  and the \ud83c\udfc6 award winning Project Jupyter"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#xerox-alto","text":"The Xerox Alto is a computer designed from its inception to support an operating system based on a graphical user interface (GUI), later using the desktop metaphor. The first machines were introduced on 1 March 1973, a decade before mass-market GUI machines became available.","title":"Xerox Alto"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#the-computer-mouse-and-interactive-computing","text":"at the dawn of post modernity, Doug Engelbart's X-Y Position Indicator for a Display System changed the way we interact with computers. ## [the mother of all demos] < figure markdown > < figcaption markdown > Doug Englebart 's Mother of all Demos presentation that changed computing forever. </ figcaption > < iframe width = \"629\" height = \"472\" src = \"https://www.youtube.com/embed/yJDv-zdhzMY\" title = \"The Mother of All Demos, presented by Douglas Engelbart (1968)\" frameborder = \"0\" allow = \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen ></ iframe > </ figure > > The live demonstration featured the introduction of a complete computer hardware and software system called the oN - Line System or , more commonly , NLS . The 90 - minute presentation demonstrated for the first time many of the fundamental elements of modern personal computing : windows , hypertext , graphics , efficient navigation and command input , video conferencing , the computer mouse , word processing , dynamic file linking , revision control , and a collaborative real - time editor . Engelbart 's presentation was the first to publicly demonstrate all of these elements in a single system. The demonstration was highly influential and spawned similar projects at Xerox PARC in the early 1970s. The underlying concepts and technologies influenced both the Apple Macintosh and Microsoft Windows graphical user interface operating systems in the 1980s and 1990s. [ the mother of all demos ]: https : // en . wikipedia . org / wiki / The_Mother_of_All_Demos \"the wiki page for the mother of all demos\" [ moad transcript ]: https : // github . com / atduskgreg / MoAD - Transcript \"a repository hosting the mother of all demos transcripts\"","title":"The computer mouse and interactive computing"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#the-mother-of-all-demos","text":"Doug Englebart's Mother of all Demos presentation that changed computing forever. The live demonstration featured the introduction of a complete computer hardware and software system called the oN-Line System or, more commonly, NLS. The 90-minute presentation demonstrated for the first time many of the fundamental elements of modern personal computing: windows, hypertext, graphics, efficient navigation and command input, video conferencing, the computer mouse, word processing, dynamic file linking, revision control, and a collaborative real-time editor. Engelbart's presentation was the first to publicly demonstrate all of these elements in a single system. The demonstration was highly influential and spawned similar projects at Xerox PARC in the early 1970s. The underlying concepts and technologies influenced both the Apple Macintosh and Microsoft Windows graphical user interface operating systems in the 1980s and 1990s.","title":"the mother of all demos"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#50-years-later-were-still-living-in-the-xerox-altos-world","text":"the impact of the Xerox Alto still ripples through the world as we're still build the systems in the mother of all demos. the Alto was a glimpse into the future of personal computing. ## Mathematica's computational essays < figure markdown > < figcaption markdown > mathematica is a successful closed source interface for mathematical computing using the proprietary wolfram language . mathematica author 's have been experimenting with notebook interfaces the designers of the interfaces are part of apple 's early history. relationship to applye </ figcaption > ! [ mathematica version 2 ]( https : // winworldpc . com / res / img / screenshots / Mathematica % 202.1 % 20 -% 20 About . png \"a screenshot of mathematica version 2\" ) </ figure >","title":"50 Years Later, We\u2019re Still Living in the Xerox Alto\u2019s World"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#mathematicas-computational-essays","text":"mathematica is a successful closed source interface for mathematical computing using the proprietary wolfram language. mathematica author's have been experimenting with notebook interfaces the designers of the interfaces are part of apple's early history. relationship to applye ### [what is a computational essay] < figure markdown > < figcaption markdown > < blockquote > There are basically three kinds of things here . First , ordinary text ( here in English ) . Second , computer input . And third , computer output . And the crucial point is that these all work together to express what \u2019 s being communicated .</ blockquote > </ figcaption > ! []( https : // content . wolfram . com / uploads / sites / 43 / 2017 / 11 / InOutImg1 . png ) </ figure > [ what is a computational essay ]: https : // writings . stephenwolfram . com / 2017 / 11 / what - is - a - computational - essay / < details markdown > < summary markdown > wolfram 's throughts on jupyter: an excerpt from [The Scientific Paper is Obsolete] </ summary > > \u201c I see these Jupyter guys , \u201d Wolfram said to me , \u201c they are about on a par with what we had in the early 1990 s . \u201d They \u2019 ve taken shortcuts , he said . \u201c We actually want to try and do it right . \u201d </ details > [ The Scientific Paper is Obsolete ]: https : // www . theatlantic . com / science / archive / 2018 / 04 / the - scientific - paper - is - obsolete / 556676 /","title":"Mathematica's computational essays"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#what-is-a-computational-essay","text":"There are basically three kinds of things here. First, ordinary text (here in English). Second, computer input. And third, computer output. And the crucial point is that these all work together to express what\u2019s being communicated. wolfram's throughts on jupyter: an excerpt from The Scientific Paper is Obsolete \u201cI see these Jupyter guys,\u201d Wolfram said to me, \u201cthey are about on a par with what we had in the early 1990s.\u201d They\u2019ve taken shortcuts, he said. \u201cWe actually want to try and do it right.\u201d","title":"what is a computational essay"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#computer-programning-for-everybody","text":"In the seventies, Xerox PARC asked: \"Can we have a computer on every desk?\" We now know this is possible, but those computers haven't necessarily empowered their users. Today's computers are often inflexible: the average computer user can typically only change a limited set of options configurable via a \"wizard\" (a lofty word for a canned dialog), and is dependent on expert programmers for everything else. We compare mass ability to read and write software with mass literacy, and predict equally pervasive changes to society. Hardware is now sufficiently fast and cheap to make mass computer education possible: the next big change will happen when most computer users have the knowledge and power to create and modify software.","title":"computer programning for everybody"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#interactive-python","text":"IPython grew to have a lot of features in a monolith an interactive shell a REPL protocol a notebook document fromat a notebook document conversion tool a web-based notebook authoring tool tools for building interactive UI (widgets) interactive parallel Python based on the above REPL protocol","title":"Interactive Python"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#2015-project-jupyter-the-big-split","text":"Jupyter is like IPython, but language agnostic ## 2017 ACM Software System Award < figure markdown > < figcaption > jupyter is recognized as a critical software software . </ figcaption > < blockquote cite = \"https://awards.acm.org/software-system\" > Jupyter has also gained wide industry adoption . Since 2015 , Jupyter - based products have been released by several companies including Google ( Cloud DataLab ), Microsoft ( AzureML , HDInsight ), Intel ( Trusted Analytics Platform ), and IBM ( IBM Watson Studio ) . Bloomberg and Anaconda Inc . have partnered with Project Jupyter to develop the next - generation web interface , JupyterLab . </ blockquote > </ figure > < blockquote cite = \"https://blog.jupyter.org/jupyter-receives-the-acm-software-system-award-d433b0dfe3a2\" > Similarly , there exist multiple client applications in addition to the Jupyter Notebook and JupyterLab to create and execute notebooks , each with its own use case and focus : the open source nteract project develops a lightweight desktop application to run notebooks ; CoCalc , a startup founded by William Stein , the creator of SageMath , offers a web - based client with real - time collaboration that includes Jupyter alongside SageMath , LaTeX , and tools focused on education ; and Google now provides Colaboratory , another web notebook frontend that runs alongside the rest of the Google Documents suite , with execution in the Google Cloud . </ blockquote >","title":"2015 Project Jupyter The Big Split"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#2017-acm-software-system-award","text":"jupyter is recognized as a critical software software. Jupyter has also gained wide industry adoption. Since 2015, Jupyter-based products have been released by several companies including Google (Cloud DataLab), Microsoft (AzureML, HDInsight), Intel (Trusted Analytics Platform), and IBM (IBM Watson Studio). Bloomberg and Anaconda Inc. have partnered with Project Jupyter to develop the next-generation web interface, JupyterLab. Similarly, there exist multiple client applications in addition to the Jupyter Notebook and JupyterLab to create and execute notebooks, each with its own use case and focus: the open source nteract project develops a lightweight desktop application to run notebooks; CoCalc, a startup founded by William Stein, the creator of SageMath, offers a web-based client with real-time collaboration that includes Jupyter alongside SageMath, LaTeX, and tools focused on education; and Google now provides Colaboratory, another web notebook frontend that runs alongside the rest of the Google Documents suite, with execution in the Google Cloud. ## jupyter is in good company < figure markdown > < figcaption markdown > Project Jupyter wins the 2017 ACM Software System Award , sharing a similar prestige as the Xerox Alto 30 years later . </ figcaption > < iframe src = \"https://en.m.wikipedia.org/wiki/ACM_Software_System_Award#Recipients\" height = \"600\" width = \"100%\" ></ iframe > </ figure >","title":"2017 ACM Software System Award"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#jupyter-is-in-good-company","text":"Project Jupyter wins the 2017 ACM Software System Award, sharing a similar prestige as the Xerox Alto 30 years later. ### there many notebook implementations and user interfaces studies of notebook and literate computing interfaces : < figure markdown > < figcaption markdown > the notebook is a style of interface , collections of input forms and outputs repeated to weave a narrative and tangle code . there are open source and closed source options , along with language specific and agnostic implementations . </ figcaption > < iframe src = \"https://en.m.wikipedia.org/wiki/Notebook_interface#Free/open-source_notebooks\" height = \"600\" width = \"100%\" ></ iframe > </ figure >","title":"jupyter is in good company"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#there-many-notebook-implementations-and-user-interfaces","text":"the notebook is a style of interface, collections of input forms and outputs repeated to weave a narrative and tangle code. there are open source and closed source options, along with language specific and agnostic implementations. ## notebooks and cells are increasingly more common forms < figure markdown > < figcaption > Observable 's description of notebooks and cells.</figcaption> < blockquote cite = \"https://observablehq.com/@observablehq/notebooks-cells\" > Observable is a platform for exploring data and code , visually , live in your browser . And < b > the central component of that platform is what we call a \"notebook\" : an interactive , editable document defined by chunks of code called \"cells\" .</ b > Observable notebooks help you explore live data , prototype visualizations , make interactive art , understand algorithms , collaborate on reports , and much more . Join Anjana Vakil , Observable Developer Advocate , in diving into Observable and understanding the basic mechanics of working with notebooks & cells . </ blockquote > </ figure > < iframe width = \"560\" height = \"315\" src = \"https://www.youtube.com/embed/M1xMRkb89oM\" title = \"Observable: Notebooks &amp; Cells\" frameborder = \"0\" allow = \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen ></ iframe >","title":"there many notebook implementations and user interfaces"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#notebooks-and-cells-are-increasingly-more-common-forms","text":"Observable's description of notebooks and cells. Observable is a platform for exploring data and code, visually, live in your browser. And the central component of that platform is what we call a \"notebook\": an interactive, editable document defined by chunks of code called \"cells\". Observable notebooks help you explore live data, prototype visualizations, make interactive art, understand algorithms, collaborate on reports, and much more. Join Anjana Vakil, Observable Developer Advocate, in diving into Observable and understanding the basic mechanics of working with notebooks & cells. ## <abbr title=\"web content accessibility guidelines\">WCAG</abbr>: we circumvented accessibility guidelines < blockquote cite = \"https://en.wikipedia.org/wiki/Web_Content_Accessibility_Guidelines\" > The ** Web Content Accessibility Guidelines ( WCAG ) ** are part of a series of web accessibility guidelines published by the Web Accessibility Initiative ( WAI ) of the World Wide Web Consortium ( W3C ), the main international standards organization for the Internet . They are a set of recommendations for making Web content more accessible , primarily for people with disabilities \u2014 but also for all user agents , including highly limited devices , such as mobile phones . </ blockquote > < blockquote cite = \"https://www.boia.org/blog/history-of-the-web-content-accessibility-guidelines-wcag\" markdown > ### wcag drafts * May 5 , 1999 : WCAG 1.0 is born . It included 14 guidelines , ranging from the need to provide text equivalents to considering clarity and simplicity on the web . Each guideline had between one and 10 supporting checkpoints . * December 11 , 2008 : WCAG 2.0 broadens scope and offers the four principles . The early 2000 s were years of unbelievable changes in technology , so WCAG evolved to keep up . WCAG 2.0 was an incredible follow - up to its predecessor and was intended to be applied to almost all things digital ( including documents and apps ) . WCAG 2.0 also introduced the four guiding principles of accessibility , stating content must be perceivable , operable , understandable , and robust , supported by success criteria for meeting those principles . WCAG 2.0 reigned as the gold standard for a long time . * June 5 , 2018 : WCAG 2.1 builds on but does not replace WCAG 2.0 . The latest version of WCAG is backwards - compatible with the previous , which means if you comply with WCAG 2.1 , you automatically comply with WCAG 2.0 \u2014 great news for website and app creators . WCAG 2.1 came as a highly - welcomed update , after the decade - old WCAG 2.0 could no longer completely account for advancements in technology and web use . The new WCAG 2.1 standards include several success criteria for improving web accessibility on mobile devices , as well as for people with low vision and cognitive disabilities . </ blockquote >","title":"notebooks and cells are increasingly more common forms"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#wcag-we-circumvented-accessibility-guidelines","text":"The **Web Content Accessibility Guidelines (WCAG)** are part of a series of web accessibility guidelines published by the Web Accessibility Initiative (WAI) of the World Wide Web Consortium (W3C), the main international standards organization for the Internet. They are a set of recommendations for making Web content more accessible, primarily for people with disabilities\u2014but also for all user agents, including highly limited devices, such as mobile phones.","title":"WCAG: we circumvented accessibility guidelines"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#wcag-drafts","text":"May 5, 1999: WCAG 1.0 is born. It included 14 guidelines, ranging from the need to provide text equivalents to considering clarity and simplicity on the web. Each guideline had between one and 10 supporting checkpoints. December 11, 2008: WCAG 2.0 broadens scope and offers the four principles. The early 2000s were years of unbelievable changes in technology, so WCAG evolved to keep up. WCAG 2.0 was an incredible follow-up to its predecessor and was intended to be applied to almost all things digital (including documents and apps). WCAG 2.0 also introduced the four guiding principles of accessibility, stating content must be perceivable, operable, understandable, and robust, supported by success criteria for meeting those principles. WCAG 2.0 reigned as the gold standard for a long time. June 5, 2018: WCAG 2.1 builds on but does not replace WCAG 2.0. The latest version of WCAG is backwards-compatible with the previous, which means if you comply with WCAG 2.1, you automatically comply with WCAG 2.0 \u2014 great news for website and app creators. WCAG 2.1 came as a highly-welcomed update, after the decade-old WCAG 2.0 could no longer completely account for advancements in technology and web use. The new WCAG 2.1 standards include several success criteria for improving web accessibility on mobile devices, as well as for people with low vision and cognitive disabilities.","title":"wcag drafts"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#accessible-notebook-interfaces","text":"recently, Jupyter developers were able to remove accessibility violations caught be axe. read the blog post . ### automated testing < figure markdown > < figcaption markdown > [ axe core ][ axe ] 's is the open source industry standard for accessibility testing. caveat emptor ... </ figcaption > > With [ axe - core ][ axe ], you can find ** on average 57 % of WCAG issues automatically **. Additionally , axe - core will return elements as \"incomplete\" where axe - core could not be certain , and manual review is needed . </ figure > [ axe ]: https : // github . com / dequelabs / axe - core #### using axe * https : // github . com / dequelabs / axe - core * [ firefox axe extension ] * [ chrome axe extension ] [ firefox axe extension ]: https : // addons . mozilla . org / en - US / firefox / addon / axe - devtools / [ chrome axe extension ]: https : // chrome . google . com / webstore / detail / axe - devtools - web - accessib / lhdoppojpmngadmnindnejefpokejbdd","title":"accessible notebook interfaces"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#automated-testing","text":"axe core 's is the open source industry standard for accessibility testing. caveat emptor... With axe-core , you can find on average 57% of WCAG issues automatically . Additionally, axe-core will return elements as \"incomplete\" where axe-core could not be certain, and manual review is needed.","title":"automated testing"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#using-axe","text":"https://github.com/dequelabs/axe-core firefox axe extension chrome axe extension","title":"using axe"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#manual-testing","text":"Nothing about us without us pull requests to the tests a primary function of this project has been testing notebooks with screen reader users to design a better experience. ## we want to design notebook interfaces that empower assistive technology users things we can control things we cant control we need you help ! venn diagram of notebook accessibility and output accessibility . write accessibile notebooks . * advocacy , action * join our meetings .","title":"manual testing"},{"location":"xxiii/2023-03-04-computational-notebooks-history.html#we-want-to-design-notebook-interfaces-that-empower-assistive-technology-users","text":"things we can control things we cant control we need you help! venn diagram of notebook accessibility and output accessibility. write accessibile notebooks. advocacy, action join our meetings. < style > article > . highlight { display : none ; } </ style > article > .highlight { display: none; } {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"we want to design notebook interfaces that empower assistive technology users"}]}